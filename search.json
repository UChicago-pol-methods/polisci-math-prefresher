[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "University of Chicago Political Science Math Prefresher",
    "section": "",
    "text": "Special thanks to Shiro Kuriwaki for developing the original version of this tutorial↩︎\nSpecial thanks to Shiro Kuriwaki and Yon Soo Park for developing the original module↩︎\nModule originally written by Shiro Kuriwaki, Connor Jerzak, and Yon Soo Park↩︎\nModule originally written by Connor Jerzak and Shiro Kuriwaki↩︎"
  },
  {
    "objectID": "02_sets_and_functions.html",
    "href": "02_sets_and_functions.html",
    "title": "2  Sets, Operations, and Functions",
    "section": "",
    "text": "Sets are the fundamental building blocks of mathematics. Events are not inherently numerical: the onset of war or the stock market crashing is not inherently a number. Sets can define such events, and we wrap math around so that we have a transparent language to communicate about those events. Combining sets with operations, relations, metrics, measures, etc… allows us to define useful mathematical structures. For example, the set of real numbers (\\(\\mathbb{R}\\)) has a notion of order as well as defined operations of addition and multiplication.\nSet : A set is any well defined collection of elements. If \\(x\\) is an element of \\(S\\), \\(x \\in S\\).\nExamples:\n\nThe set of choices available to a player in Rock-Paper-Scissors \\(\\{\\text{Rock}, \\text{Paper}, \\text{Scissors}\\}\\)\nThe set of possible outcomes of a roll of a six-sided die \\(\\{1, 2, 3, 4, 5, 6\\}\\)\nThe set of all natural numbers \\(\\mathbb{N}\\)\nThe set of all real numbers \\(\\mathbb{R}\\)\n\nCommon mathematical notation relevant to sets:\n\n\\(\\in\\) = “is an element of”; \\(\\notin\\) = “is not an element of”\n\\(\\forall\\) = “for all” (univeral quantifier)\n\\(\\exists\\) = “there exists” (existential quantifier)\n\\(:\\) = “such that”\n\nSubset: If every element of set \\(A\\) is also in set \\(B\\), then \\(A\\) is a subset of \\(B\\). \\(A \\subseteq B\\). If, in addition to being a subset of \\(B\\), \\(A\\) is not equal to \\(B\\), \\(A\\) is a proper subset \\(A \\subset B\\).\nEmpty Set: a set with no elements. \\(S = \\{\\}\\). It is denoted by the symbol \\(\\emptyset\\).\nCardinality: The cardinality of a set \\(S\\), typically written \\(|S|\\) is the number of members of \\(S\\).\nMany sets are infinite. For example, \\(\\mathbb{N}\\) the set of natural numbers \\(\\mathbb{N} = \\{0, 1, 2, 3, 4, \\dotsc\\}\\) - Sets with cardinality less than \\(|\\mathbb{N}|\\) are countable - Sets with the same cardinality as \\(\\\\mathbb{N}|\\) are countably infinite - Sets with greater cardinality than \\(|\\mathbb{N}|\\) are uncountably infinite (e.g. the real numbers).\nSet operations:\n\nUnion: The union of two sets \\(A\\) and \\(B\\), \\(A \\cup B\\), is the set containing all of the elements in \\(A\\) or \\(B\\). \\(A_1 \\cup A_2 \\cup \\cdots \\cup A_n = \\bigcup_{i=1}^n A_i\\)\nIntersection: The intersection of sets \\(A\\) and \\(B\\), \\(A \\cap B\\), is the set containing all of the elements in both \\(A\\) and \\(B\\). \\(A_1 \\cap A_2 \\cap \\cdots \\cap A_n = \\bigcap_{i=1}^n A_i\\)\nComplement: If set \\(A\\) is a subset of \\(S\\), then the complement of \\(A\\), denoted \\(A^C\\), is the set containing all of the elements in \\(S\\) that are not in \\(A\\).\n\nProperties of set operations:\n\nCommutative: \\(A \\cup B = B \\cup A\\); \\(A \\cap B = B \\cap A\\)\nAssociative: \\(A \\cup (B \\cup C) = (A \\cup B) \\cup C\\); \\(A \\cap (B \\cap C) = (A \\cap B) \\cap C\\)\nDistributive: \\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\); \\(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\)\nde Morgan’s laws: \\((A \\cup B)^C = A^C \\cap B^C\\); \\((A \\cap B)^C = A^C \\cup B^C\\)\nDisjointness: Sets are disjoint when they do not intersect, such that \\(A \\cap B = \\emptyset\\). A collection of sets is pairwise disjoint (mutually exclusive) if, for all \\(i \\neq j\\), \\(A_i \\cap A_j = \\emptyset\\). A collection of sets form a partition of set \\(S\\) if they are pairwise disjoint and they cover set \\(S\\), such that \\(\\bigcup_{i = 1}^k A_i = S\\).\n\n\nExample 2.1 (Sets) Let set \\(A\\) be \\(\\{1, 2, 3, 4\\}\\), \\(B\\) be \\(\\{3, 4, 5, 6\\}\\), and \\(C\\) be \\(\\{5, 6, 7, 8\\}\\). Sets \\(A\\), \\(B\\), and \\(C\\) are all subsets of the \\(S\\) which is \\(\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\}\\)\nWrite out the following sets:\n\n\\(A \\cup B\\)\n\\(C \\cap B\\)\n\\(B^c\\)\n\\(A \\cap (B \\cup C)\\)\n\n\n\nExercise 2.1 (Sets) Suppose you had a pair of four-sided dice. You sum the results from a single toss.\nWhat is the set of possible outcomes?\nConsider subsets \\(A=\\{2, 8\\}\\) and \\(B=\\{2,3,7\\}\\) of the sample space you found. What is\n\n\\(A^c\\)\n\\((A \\cup B)^c\\)"
  },
  {
    "objectID": "02_sets_and_functions.html#metric-spaces",
    "href": "02_sets_and_functions.html#metric-spaces",
    "title": "2  Sets, Operations, and Functions",
    "section": "2.2 Metric spaces",
    "text": "2.2 Metric spaces\nA metric space is a set that has a notion of distance - called a “metric” - defined between any two elements (sometimes referred to as “points”).\nThe distance function \\(d(x,y)\\) defines the distance between element \\(x\\) and element \\(y\\)\n\nThe real numbers \\(\\mathbb{R}\\) have a single distance function: \\(d(x,y) = |x - y|\\)\nIn higher-dimensional real space (e.g. \\(\\mathbb{R}^2)\\), we can define multiple distance metrics between \\(x=(x_1, x_2)\\) and \\(y=(y_1, y_2)\\)\n\n“Euclidean” distance: \\(d(x, y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}\\)\n“Taxicab” distance: \\(d(x, y) = |x_1 - y_1| + |x_2 - y_2|\\)\nChebyshev distance: \\(d(x, y) = \\text{max}\\{|x_1 - y_1| + |x_2 - y_2|\\}\\)\n\nAll of these generalize to \\(\\mathbb{R}^n\\)\n\nA metric is a function that satisfies the following axioms\n\nA distance between a point and itself is zero \\(d(x,x) = 0\\)\nThe distance between two points is strictly positive \\(d(x,y) > 0 \\forall x \\neq y\\)\nDistance from \\(x\\) to \\(y\\) is the same as the distance from \\(y\\) to \\(x\\) (\\(d(x,y) = d(y,x)\\))\nThe “triangle inequality” holds: \\(d(x,z) \\le d(x,y) + d(y,z)\\)\n\nOnce we have a metric space, we can define some additional useful concepts\nBall: A ball of radius \\(r\\) centered at \\(x_0\\) is a set that contains all points with a distance less than \\(r\\) from \\(x_0\\).\nSphere: A sphere of radius \\(r\\) centered at \\(x_0\\) is the set that contains all points with a distance exactly \\(r\\) from \\(x_0\\).\nInterior Point: The point \\(x\\) is an interior point of the set \\(S\\) if \\(x\\) is in \\(S\\) and if there is some \\(\\epsilon\\)-ball around \\(x\\) that contains only points in \\(S\\). The interior of \\(S\\) is the collection of all interior points in \\(S\\). The interior can also be defined as the union of all open sets in \\(S\\).\n\nIf the set \\(S\\) is circular, the interior points are everything inside of the circle, but not on the circle’s rim.\nExample: The interior of the set \\(\\{ (x,y) : x^2+y^2\\le 4 \\}\\) is \\(\\{ (x,y) : x^2+y^2< 4 \\}\\) .\n\nBoundary Point: The point \\(\\mathbf x\\) is a boundary point of the set \\(S\\) if every \\(\\epsilon\\)-ball around \\(\\mathbf x\\) contains both points that are in \\(S\\) and points that are outside \\(S\\). The boundary is the collection of all boundary points.\n\nIf the set \\(S\\) is circular, the boundary points are everything on the circle’s rim.\nExample: The boundary of \\(\\{ (x,y) : x^2+y^2\\le 4 \\}\\) is \\(\\{ (x,y) : x^2+y^2 = 4 \\}\\).\n\nOpen: A set \\(S\\) is open if for each point \\(\\mathbf x\\) in \\(S\\), there exists an open \\(\\epsilon\\)-ball around \\(\\mathbf x\\) completely contained in \\(S\\).\n\nIf the set \\(S\\) is circular and open, the points contained within the set get infinitely close to the circle’s rim, but do not touch it.\nExample: \\(\\{ (x,y) : x^2+y^2<4 \\}\\)\n\nClosed: A set \\(S\\) is closed if it contains all of its boundary points.\n\nAlternatively: A set is closed if its complement is open.\nIf the set \\(S\\) is circular and closed, the set contains all points within the rim as well as the rim itself.\nExample: \\(\\{ (x,y) : x^2+y^2\\le 4 \\}\\)\nNote: a set may be neither open nor closed. Example: \\(\\{ (x,y) : 2 < x^2+y^2\\le 4 \\}\\)"
  },
  {
    "objectID": "02_sets_and_functions.html#operators-sum-and-product-notation",
    "href": "02_sets_and_functions.html#operators-sum-and-product-notation",
    "title": "2  Sets, Operations, and Functions",
    "section": "2.3 Operators; Sum and Product notation",
    "text": "2.3 Operators; Sum and Product notation\nAddition (+), Subtraction (-), multiplication and division are basic operations of arithmetic. In statistics or calculus, we will often want to add a sequence of numbers that can be expressed as a pattern without needing to write down all its components. For example, how would we express the sum of all numbers from 1 to 100 without writing a hundred numbers?\nFor this we use the summation operator \\(\\sum\\) and the product operator \\(\\prod\\).\nSummation:\n\\[\\sum\\limits_{i=1}^{100} x_i = x_1+x_2+x_3+\\cdots+x_{100}\\]\nThe bottom of the \\(\\sum\\) symbol indicates an index (here, \\(i\\)), and its start value \\(1\\). At the top is where the index ends. The notion of “addition” is part of the \\(\\sum\\) symbol. The content to the right of the summation is the meat of what we add. While you can pick your favorite index, start, and end values, the content must also have the index.\nA few important features of sums:\n\n\\(\\sum\\limits_{i=1}^n c x_i = c \\sum\\limits_{i=1}^n x_i\\)\n\\(\\sum\\limits_{i=1}^n (x_i + y_i) = \\sum\\limits_{i=1}^n x_i + \\sum\\limits_{i=1}^n y_i\\)\n\\(\\sum\\limits_{i=1}^n c = n c\\)\n\nProduct:\n\\[\\prod\\limits_{i=1}^n x_i = x_1 x_2 x_3 \\cdots x_n\\]\nProperties:\n\n\\(\\prod\\limits_{i=1}^n c x_i = c^n \\prod\\limits_{i=1}^n x_i\\)\n\\(\\prod\\limits_{i=k}^n c x_i = c^{n-k+1} \\prod\\limits_{i=k}^n x_i\\)\n\\(\\prod\\limits_{i=1}^n (x_i + y_i) =\\) a total mess\n\\(\\prod\\limits_{i=1}^n c = c^n\\)\n\nOther Useful Operations\nFactorials!:\n\\[x! = x\\cdot (x-1) \\cdot (x-2) \\cdots (1)\\]\nModulo: Tells you the remainder when you divide the first number by the second.\n\n\\(17 \\mod 3 = 2\\)\n\\(100 \\ \\% \\ 30 = 10\\)\n\n\n\nExample 2.2 (Operators) \n\\(\\sum\\limits_{i=1}^{5} i =\\)\n\\(\\prod\\limits_{i=1}^{5} i =\\)\n\\(14 \\mod 4 =\\)\n\\(4! =\\)\n\n\n\nExercise 2.2 (Operators) Let \\(x_1 = 4, x_2 = 3, x_3 = 7, x_4 = 11, x_5 = 2\\)\n\n\\(\\sum\\limits_{i=1}^{3} (7)x_i\\)\n\\(\\sum\\limits_{i=1}^{5} 2\\)\n\\(\\prod\\limits_{i=3}^{5} (2)x_i\\)"
  },
  {
    "objectID": "02_sets_and_functions.html#introduction-to-functions",
    "href": "02_sets_and_functions.html#introduction-to-functions",
    "title": "2  Sets, Operations, and Functions",
    "section": "2.4 Introduction to Functions",
    "text": "2.4 Introduction to Functions\nA function is a mapping, or transformation, that relates members of one set to members of another set. For instance, if you have two sets: set \\(A\\) and set \\(B\\), a function from \\(A\\) to \\(B\\) maps every value \\(a\\) in set \\(A\\) such that \\(f(a) \\in B\\). Functions can be “many-to-one”, where many values or combinations of values from set \\(A\\) produce a single output in set \\(B\\), or they can be “one-to-one”, where each value in set \\(A\\) corresponds to a single value in set \\(B\\). A function by definition has a single function value for each element of its domain. This means, there cannot be “one-to-many” mapping.\nDimensionality: \\({\\mathbf R}^1\\) is the set of all real numbers extending from \\(-\\infty\\) to \\(+\\infty\\) — i.e., the real number line. \\({\\mathbf R}^n\\) is an \\(n\\)-dimensional space, where each of the \\(n\\) axes extends from \\(-\\infty\\) to \\(+\\infty\\).\n\n\\({\\mathbf R}^1\\) is a one dimensional line.\n\\({\\mathbf R}^2\\) is a two dimensional plane.\n\\({\\mathbf R}^3\\) is a three dimensional space.\n\nPoints in \\({\\mathbf R}^n\\) are ordered \\(n\\)-tuples (just means an combination of \\(n\\) elements where order matters), where each element of the \\(n\\)-tuple represents the coordinate along that dimension.\nFor example:\n\n\\({\\mathbf R}^1\\): (3)\n\\({\\mathbf R}^2\\): (-15, 5)\n\\({\\mathbf R}^3\\): (86, 4, 0)\n\nExamples of mapping notation:\nFunction of one variable: \\(f:{\\mathbf R}^1\\to{\\mathbf R}^1\\)\n\n\\(f(x)=x+1\\). For each \\(x\\) in \\({\\mathbf R}^1\\), \\(f(x)\\) assigns the number \\(x+1\\).\n\nFunction of two variables: \\(f: {\\mathbf R}^2\\to{\\mathbf R}^1\\).\n\n\\(f(x,y)=x^2+y^2\\). For each ordered pair \\((x,y)\\) in \\({\\mathbf R}^2\\), \\(f(x,y)\\) assigns the number \\(x^2+y^2\\).\n\nWe often use variable \\(x\\) as input and another \\(y\\) as output, e.g. \\(y=x+1\\)\n\nExample 2.3 (Functions) For each of the following, state whether they are one-to-one or many-to-one functions.\n\nFor \\(x \\in [0,\\infty]\\), \\(f : x \\rightarrow x^2\\) (this could also be written as \\(f(x) = x^2\\)).\nFor \\(x \\in [-\\infty, \\infty]\\), \\(f: x \\rightarrow x^2\\).\n\n\n\nExercise 2.3 (Functions) For each of the following, state whether they are one-to-one or many-to-one functions.\n\nFor \\(x \\in [-3, \\infty]\\), \\(f: x \\rightarrow x^2\\).\nFor \\(x \\in [0, \\infty]\\), \\(f: x \\rightarrow \\sqrt{x}\\)\n\n\nSome functions are defined only on proper subsets of \\({\\mathbf R}^n\\).\n\nDomain: the set of numbers in \\(X\\) at which \\(f(x)\\) is defined.\nRange: elements of \\(Y\\) assigned by \\(f(x)\\) to elements of \\(X\\), or \\(f(X)=\\{ y : y=f(x), x\\in X\\}\\) Most often used when talking about a function \\(f:{\\mathbf R}^1\\to{\\mathbf R}^1\\).\nImage: same as range, but more often used when talking about a function \\(f:{\\mathbf R}^n\\to{\\mathbf R}^1\\).\n\nSome General Types of Functions\nMonomials: \\(f(x)=a x^k\\)\n\\(a\\) is the coefficient. \\(k\\) is the degree.\nExamples: \\(y=x^2\\), \\(y=-\\frac{1}{2}x^3\\)\nPolynomials: sum of monomials.\nExamples: \\(y=-\\frac{1}{2}x^3+x^2\\), \\(y=3x+5\\)\nThe degree of a polynomial is the highest degree of its monomial terms. Also, it’s often a good idea to write polynomials with terms in decreasing degree."
  },
  {
    "objectID": "02_sets_and_functions.html#logexponents",
    "href": "02_sets_and_functions.html#logexponents",
    "title": "2  Sets, Operations, and Functions",
    "section": "2.5 Logarithms and Exponents",
    "text": "2.5 Logarithms and Exponents\nExponential Functions: Example: \\(y=2^x\\)\nRelationship of logarithmic and exponential functions: \\[y=\\log_a(x) \\iff a^y=x\\]\nThe log function can be thought of as an inverse for exponential functions. \\(a\\) is referred to as the “base” of the logarithm.\nCommon Bases: The two most common logarithms are base 10 and base \\(e\\).\n\nBase 10: \\(\\quad y=\\log_{10}(x) \\iff 10^y=x\\). The base 10 logarithm is often simply written as “\\(\\log(x)\\)” with no base denoted.\nBase \\(e\\): \\(\\quad y=\\log_e(x) \\iff e^y=x\\). The base \\(e\\) logarithm is referred to as the “natural” logarithm and is written as ``\\(\\ln(x)\\)“.\n\nProperties of exponential functions:\n\n\\(a^x a^y = a^{x+y}\\)\n\\(a^{-x} = 1/a^x\\)\n\\(a^x/a^y = a^{x-y}\\)\n\\((a^x)^y = a^{x y}\\)\n\\(a^0 = 1\\)\n\nProperties of logarithmic functions (any base):\nGenerally, when statisticians or social scientists write \\(\\log(x)\\) they mean \\(\\log_e(x)\\). In other words: \\(\\log_e(x) \\equiv \\ln(x) \\equiv \\log(x)\\)\n\\[\\log_a(a^x)=x\\] and \\[a^{\\log_a(x)}=x\\]\n\n\\(\\log(x y)=\\log(x)+\\log(y)\\)\n\\(\\log(x^y)=y\\log(x)\\)\n\\(\\log(1/x)=\\log(x^{-1})=-\\log(x)\\)\n\\(\\log(x/y)=\\log(x\\cdot y^{-1})=\\log(x)+\\log(y^{-1})=\\log(x)-\\log(y)\\)\n\\(\\log(1)=\\log(e^0)=0\\)\n\nChange of Base Formula: Use the change of base formula to switch bases as necessary: \\[\\log_b(x) = \\frac{\\log_a(x)}{\\log_a(b)}\\]\nExample: \\[\\log_{10}(x) = \\frac{\\ln(x)}{\\ln(10)}\\]\nYou can use logs to go between sum and product notation. This will be particularly important when you’re learning how to optimize likelihood functions.\n\\[\\begin{eqnarray*}\n            \\log \\bigg(\\prod\\limits_{i=1}^n x_i \\bigg) &=& \\log(x_1 \\cdot x_2 \\cdot x_3 \\cdots \\cdot x_n)\\\\\n            &=& \\log(x_1) + \\log(x_2) + \\log(x_3) + \\cdots + \\log(x_n)\\\\\n            &=& \\sum\\limits_{i=1}^n \\log (x_i)\n\\end{eqnarray*}\\]\nTherefore, you can see that the log of a product is equal to the sum of the logs. We can write this more generally by adding in a constant, \\(c\\):\n\\[\\begin{eqnarray*}\n            \\log \\bigg(\\prod\\limits_{i=1}^n c x_i\\bigg) &=& \\log(cx_1 \\cdot cx_2 \\cdots cx_n)\\\\\n            &=& \\log(c^n \\cdot x_1 \\cdot x_2 \\cdots x_n)\\\\\n            &=& \\log(c^n) + \\log(x_1) + \\log(x_2) + \\cdots + \\log(x_n)\\\\\\\\\n            &=& n \\log(c) +  \\sum\\limits_{i=1}^n \\log (x_i)\\\\\n\\end{eqnarray*}\\]\n\nExample 2.4 (Logarithms) Evaluate each of the following logarithms\n\n\\(\\log_4(16)\\)\n\\(\\log_2(16)\\)\n\nSimplify the following logarithm. By “simplify”, we actually really mean - use as many of the logarithmic properties as you can.\n\n\\(\\log_4(x^3y^5)\\)\n\n\n\nExercise 2.4 Evaluate each of the following logarithms\n\n\\(\\log_\\frac{3}{2}(\\frac{27}{8})\\)\n\nSimplify each of the following logarithms. By “simplify”, we actually really mean - use as many of the logarithmic properties as you can.\n\n\\(\\log(\\frac{x^9y^5}{z^3})\\)\n\\(\\ln{\\sqrt{xy}}\\)"
  },
  {
    "objectID": "02_sets_and_functions.html#graphing-functions",
    "href": "02_sets_and_functions.html#graphing-functions",
    "title": "2  Sets, Operations, and Functions",
    "section": "2.6 Graphing Functions",
    "text": "2.6 Graphing Functions\nWhat can a graph tell you about a function?\n\nIs the function increasing or decreasing? Over what part of the domain?\nHow ``fast” does it increase or decrease?\nAre there global or local maxima and minima? Where?\nAre there inflection points?\nIs the function continuous?\nIs the function differentiable?\nDoes the function tend to some limit?\nOther questions related to the substance of the problem at hand."
  },
  {
    "objectID": "02_sets_and_functions.html#solving-for-variables-and-finding-roots",
    "href": "02_sets_and_functions.html#solving-for-variables-and-finding-roots",
    "title": "2  Sets, Operations, and Functions",
    "section": "2.7 Solving for Variables and Finding Roots",
    "text": "2.7 Solving for Variables and Finding Roots\nSometimes we’re given a function \\(y=f(x)\\) and we want to find how \\(x\\) varies as a function of \\(y\\). Use algebra to move \\(x\\) to the left hand side (LHS) of the equation and so that the right hand side (RHS) is only a function of \\(y\\).\n\nExample 2.5 (Solving) Solve for x:\n\n\\(y=3x+2\\)\n\\(y=e^x\\)\n\n\nSolving for variables is especially important when we want to find the roots of an equation: those values of variables that cause an equation to equal zero. Especially important in finding equilibria and in doing maximum likelihood estimation.\nProcedure: Given \\(y=f(x)\\), set \\(f(x)=0\\). Solve for \\(x\\).\nMultiple Roots: \\[f(x)=x^2 - 9 \\quad\\Longrightarrow\\quad 0=x^2 - 9 \\quad\\Longrightarrow\\quad 9=x^2 \\quad\\Longrightarrow\\quad \\pm \\sqrt{9}=\\sqrt{x^2} \\quad\\Longrightarrow\\quad \\pm 3=x\\]\nQuadratic Formula: For quadratic equations \\(ax^2+bx+c=0\\), use the quadratic formula: \\[x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\]\n\nExercise 2.5 (Roots) Solve for x:\n\n\\(f(x)=3x+2 = 0\\)\n\\(f(x)=x^2+3x-4=0\\)\n\\(f(x)=e^{-x}-10 = 0\\)"
  },
  {
    "objectID": "03_limits.html",
    "href": "03_limits.html",
    "title": "3  Limits",
    "section": "",
    "text": "Solving limits, i.e. finding out the value of functions as its input moves closer to some value, is important for the social scientist’s mathematical toolkit for two related tasks. The first is for the study of calculus, which will be in turn useful to show where certain functions are maximized or minimized. The second is for the study of statistical inference, which is the study of inferring things about things you cannot see by using things you can see."
  },
  {
    "objectID": "03_limits.html#example-the-central-limit-theorem",
    "href": "03_limits.html#example-the-central-limit-theorem",
    "title": "3  Limits",
    "section": "Example: The Central Limit Theorem",
    "text": "Example: The Central Limit Theorem\nPerhaps the most important theorem in statistics is the Central Limit Theorem,\n\nTheorem 3.1 (Central Limit Theorem) For any series of independent and identically distributed random variables \\(X_1, X_2, \\cdots\\), we know the distribution of its sum even if we do not know the distribution of \\(X\\). The distribution of the sum is a Normal distribution.\n\\[\\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} \\text{Normal}(0, 1)\\]\nwhere \\(\\mu\\) is the mean of \\(X\\) and \\(\\sigma\\) is the standard deviation of \\(X\\). The arrow is read as “converges in distribution to”. \\(\\text{Normal}(0, 1)\\) indicates a Normal Distribution with mean 0 and variance 1.\nThat is, the limit of the distribution of the lefthand side is the distribution of the righthand side.\n\nThe sign of a limit is the arrow “\\(\\rightarrow\\)”. Although we have not yet covered probability so we have not described what distributions and random variables are, it is worth foreshadowing the Central Limit Theorem. The Central Limit Theorem is powerful because it gives us a guarantee of what would happen if \\(n \\rightarrow \\infty\\), which in this case means we collected more data."
  },
  {
    "objectID": "03_limits.html#example-the-law-of-large-numbers",
    "href": "03_limits.html#example-the-law-of-large-numbers",
    "title": "3  Limits",
    "section": "Example: The Law of Large Numbers",
    "text": "Example: The Law of Large Numbers\nA finding that perhaps rivals the Central Limit Theorem is the (Weak) Law of Large Numbers:\n\nTheorem 3.2 ((Weak) Law of Large Numbers) For any draw of identically distributed independent variables with mean \\(\\mu\\), the sample average after \\(n\\) draws, \\(\\bar{X}_n\\), converges in probability to the true mean as \\(n \\rightarrow \\infty\\):\n\\[\\lim\\limits_{n\\to \\infty} P(|\\bar{X}_n - \\mu | > \\varepsilon) = 0\\]\nA shorthand of which is \\(\\bar{X}_n \\xrightarrow{p} \\mu\\), where the arrow is read as “converges in probability to”.\n\nIntuitively, the more data, the more accurate is your guess. For example, Figure 3.1 shows how the sample average from many coin tosses converges to the true value : 0.5.\n\n\n\n\n\nFigure 3.1: As the number of coin tosses goes to infinity, the average probabiity of heads converges to 0.5"
  },
  {
    "objectID": "03_limits.html#sequences",
    "href": "03_limits.html#sequences",
    "title": "3  Limits",
    "section": "3.1 Sequences",
    "text": "3.1 Sequences\nWe need a couple of steps until we get to limit theorems in probability. First we will introduce a “sequence”, then we will think about the limit of a sequence, then we will think about the limit of a function.\nA sequence \\(\\{x_n\\}=\\{x_1, x_2, x_3, \\ldots, x_n\\}\\) is an ordered set of real numbers, where \\(x_1\\) is the first term in the sequence and \\(y_n\\) is the \\(n\\)th term. Generally, a sequence is infinite, that is it extends to \\(n=\\infty\\). We can also write the sequence as \\(\\{x_n\\}^\\infty_{n=1}\\)\nwhere the subscript and superscript are read together as “from 1 to infinity.”\n\nExample 3.1 (Sequences) How do these sequences behave?\n\n\\(\\{A_n\\}=\\left\\{ 2-\\frac{1}{n^2} \\right\\}\\)\n\\(\\{B_n\\}=\\left\\{\\frac{n^2+1}{n} \\right\\}\\)\n\\(\\{C_n\\}=\\left\\{(-1)^n \\left(1-\\frac{1}{n}\\right) \\right\\}\\)\n\n\nWe find the sequence by simply “plugging in” the integers into each \\(n\\). The important thing is to get a sense of how these numbers are going to change.\nGraphing helps you make this point more clearly. See the sequence of \\(n = 1, ...20\\) for each of the three examples in Figure 3.2.\n\n\n\n\n\nFigure 3.2: Behavior of Some Sequences"
  },
  {
    "objectID": "03_limits.html#the-limit-of-a-sequence",
    "href": "03_limits.html#the-limit-of-a-sequence",
    "title": "3  Limits",
    "section": "3.2 The Limit of a Sequence",
    "text": "3.2 The Limit of a Sequence\nThe notion of “converging to a limit” is the behavior of the points in Example -@#exm-seqbehav. In some sense, that’s the counterfactual we want to know. What happens as \\(n\\rightarrow \\infty\\)?\n\nSequences like 1 above that converge to a limit.\nSequences like 2 above that increase without bound.\nSequences like 3 above that neither converge nor increase without bound — alternating over the number line.\n\nDefinition: Limit The sequence \\(\\{y_n\\}\\) has the limit \\(L\\), which we write as \\(\\lim\\limits_{n \\to \\infty} y_n =L\\), if for any \\(\\epsilon>0\\) there is an integer \\(N\\) (which depends on \\(\\epsilon\\)) with the property that \\(|y_n -L|<\\epsilon\\) for each \\(n>N\\). \\(\\{y_n\\}\\) is said to converge to \\(L\\). If the above does not hold, then \\(\\{y_n\\}\\) diverges.\nWe can also express the behavior of a sequence as bounded or not:\n\nBounded: if \\(|y_n|\\le K\\) for all \\(n\\)\nMonotonically Increasing: \\(y_{n+1}>y_n\\) for all \\(n\\)\nMonotonically Decreasing: \\(y_{n+1}<y_n\\) for all \\(n\\)\n\nA limit is unique: If \\(\\{y_n\\}\\) converges, then the limit \\(L\\) is unique.\nIf a sequence converges, then the sum of such sequences also converges. Let \\(\\lim\\limits_{n \\to \\infty} y_n = y\\) and \\(\\lim\\limits_{n \\to \\infty} z_n =z\\). Then\n\n\\(\\lim\\limits_{n \\to \\infty} [k y_n + \\ell z_n]= k y + \\ell z\\)\n\\(\\lim\\limits_{n \\to \\infty} y_n z_n = yz\\)\n\\(\\lim\\limits_{n \\to \\infty} \\frac{y_n}{z_n} = \\frac{y}{z}\\), provided \\(z\\neq 0\\)\n\nThis looks reasonable enough. The harder question, obviously is when the parts of the fraction don’t converge. If \\(\\lim_{n\\to\\infty} y_n = \\infty\\) and \\(\\lim_{n\\to\\infty} z_n = \\infty\\), What is \\(\\lim_{n\\to\\infty} y_n - z_n\\)? What is \\(\\lim_{n\\to\\infty} \\frac{y_n}{z_n}\\)?\nIt is nice for a sequence to converge in limit. We want to know if complex-looking sequences converge or not. The name of the game here is to break that complex sequence up into sums of simple fractions where \\(n\\) only appears in the denominator: \\(\\frac{1}{n}, \\frac{1}{n^2}\\), and so on. Each of these will converge to 0, because the denominator gets larger and larger. Then, because of the properties above, we can then find the final sequence.\n\nExample 3.2 (Ratios) Find the limit of \\(\\lim_{n\\to \\infty} \\frac{n + 3}{n}\\)\n\n\nSolution. At first glance, \\(n + 3\\) and \\(n\\) both grow to \\(\\infty\\), so it looks like we need to divide infinity by infinity. However, we can express this fraction as a sum, then the limits apply separately:\n\\[\\lim_{n\\to \\infty} \\frac{n + 3}{n} = \\lim_{n\\to \\infty} \\left(1 + \\frac{3}{n}\\right) =  \\underbrace{\\lim_{n\\to \\infty}1}_{1} +  \\underbrace{\\lim_{n\\to \\infty}\\left(\\frac{3}{n}\\right)}_{0}\\]\nso, the limit is actually 1.\n\nAfter some practice, the key to intuition is whether one part of the fraction grows “faster” than another. If the denominator grows faster to infinity than the numerator, then the fraction will converge to 0, even if the numerator will also increase to infinity. In a sense, limits show how not all infinities are the same.\n\nExercise 3.1 (Limits) Find the following limits of sequences, then explain in English the intuition for why that is the case.\n\n\\(\\lim\\limits_{n\\to\\infty} \\frac{2n}{n^2 + 1}\\)\n\\(\\lim\\limits_{n\\to\\infty} (n^3 - 100n^2)\\)"
  },
  {
    "objectID": "03_limits.html#limitsfun",
    "href": "03_limits.html#limitsfun",
    "title": "3  Limits",
    "section": "3.3 Limits of a Function",
    "text": "3.3 Limits of a Function\nWe’ve now covered functions and just covered limits of sequences, so now is the time to combine the two.\nA function \\(f\\) is a compact representation of some behavior we care about. Like for sequences, we often want to know if \\(f(x)\\) approaches some number \\(L\\) as its independent variable \\(x\\) moves to some number \\(c\\) (which is usually 0 or \\(\\pm\\infty\\)). If it does, we say that the limit of \\(f(x)\\), as \\(x\\) approaches \\(c\\), is \\(L\\): \\(\\lim\\limits_{x \\to c} f(x)=L\\). Unlike a sequence, \\(x\\) is a continuous number, and we can move in decreasing order as well as increasing.\nFor a limit \\(L\\) to exist, the function \\(f(x)\\) must approach \\(L\\) from both the left (increasing) and the right (decreasing).\n\nDefinition 3.1 (Limits of a function) Let \\(f(x)\\) be defined at each point in some open interval containing the point \\(c\\). Then \\(L\\) equals \\(\\lim\\limits_{x \\to c} f(x)\\) if for any (small positive) number \\(\\epsilon\\), there exists a corresponding number \\(\\delta>0\\) such that if \\(0<|x-c|<\\delta\\), then \\(|f(x)-L|<\\epsilon\\).\n\nA neat, if subtle result is that \\(f(x)\\) does not necessarily have to be defined at \\(c\\) for \\(\\lim\\limits_{x \\to c}\\) to exist.\nProperties: Let \\(f\\) and \\(g\\) be functions with \\(\\lim\\limits_{x \\to c} f(x)=k\\) and \\(\\lim\\limits_{x \\to c} g(x)=\\ell\\).\n\n\\(\\lim\\limits_{x \\to c}[f(x)+g(x)]=\\lim\\limits_{x \\to c} f(x)+ \\lim\\limits_{x \\to c} g(x)\\)\n\\(\\lim\\limits_{x \\to c} kf(x) = k\\lim\\limits_{x \\to c} f(x)\\)\n\\(\\lim\\limits_{x \\to c} f(x) g(x) = \\left[\\lim\\limits_{x \\to c} f(x)\\right]\\cdot \\left[\\lim\\limits_{x \\to c} g(x)\\right]\\)\n\\(\\lim\\limits_{x \\to c} \\frac{f(x)}{g(x)} = \\frac{\\lim\\limits_{x \\to c} f(x)}{\\lim\\limits_{x \\to c} g(x)}\\), provided \\(\\lim\\limits_{x \\to c} g(x)\\ne 0\\).\n\nSimple limits of functions can be solved as we did limits of sequences. Just be careful which part of the function is changing.\n\nExample 3.3 (Limits of a function) Find the limit of the following functions.\n\n\\(\\lim_{x \\to c} k\\)\n\\(\\lim_{x \\to c} x\\)\n\\(\\lim_{x\\to 2} (2x-3)\\)\n\\(\\lim_{x \\to c} x^n\\)\n\n\nLimits can get more complex in roughly two ways. First, the functions may become large polynomials with many moving pieces. Second, the functions may become discontinuous.\nThe function can be thought of as a more general or “smooth” version of sequences. For example,\n\nExample 3.4 (Limits of ratios) Find the limit of\n\\[\\lim_{x\\to\\infty} \\frac{(x^4 +3x−99)(2−x^5)}{(18x^7 +9x^6 −3x^2 −1)(x+1)}\\]\n\nNow, the functions will become a bit more complex:\n\nExercise 3.2 (Limits of a function) Solve the following limits of functions\n\n\\(\\lim\\limits_{x\\to 0} |x|\\)\n\\(\\lim\\limits_{x\\to 0} \\left(1+\\frac{1}{x^2}\\right)\\)\n\n\nSo there are a few more alternatives about what a limit of a function could be:\n\nRight-hand limit: The value approached by \\(f(x)\\) when you move from right to left.\nLeft-hand limit: The value approached by \\(f(x)\\) when you move from left to right.\nInfinity: The value approached by \\(f(x)\\) as x grows infinitely large. Sometimes this may be a number; sometimes it might be \\(\\infty\\) or \\(-\\infty\\).\nNegative infinity: The value approached by \\(f(x)\\) as x grows infinitely negative. Sometimes this may be a number; sometimes it might be \\(\\infty\\) or \\(-\\infty\\).\n\nThe distinction between left and right becomes important when the function is not determined for some values of \\(x\\). What are those cases in the examples below?\n\n\n\n\n\nFunctions which are not defined in some areas"
  },
  {
    "objectID": "03_limits.html#continuity",
    "href": "03_limits.html#continuity",
    "title": "3  Limits",
    "section": "3.4 Continuity",
    "text": "3.4 Continuity\nTo repeat a finding from the limits of functions: \\(f(x)\\) does not necessarily have to be defined at \\(c\\) for \\(\\lim\\limits_{x \\to c}\\) to exist. Functions that have breaks in their lines are called discontinuous. Functions that have no breaks are called continuous. Continuity is a concept that is more fundamental to, but related to that of “differentiability”, which we will cover next in calculus.\n\nDefinition 3.2 (Continuity) Suppose that the domain of the function \\(f\\) includes an open interval containing the point \\(c\\). Then \\(f\\) is continuous at \\(c\\) if \\(\\lim\\limits_{x \\to c} f(x)\\) exists and if \\(\\lim\\limits_{x \\to c} f(x)=f(c)\\). Further, \\(f\\) is continuous on an open interval \\((a,b)\\) if it is continuous at each point in the interval.\n\nTo prove that a function is continuous for all points is beyond this practical introduction to math, but the general intuition can be grasped by graphing.\n\nExample 3.5 (Continuity) For each function, determine if it is continuous or discontinuous.\n\n\\(f(x) = \\sqrt{x}\\)\n\\(f(x) = e^x\\)\n\\(f(x) = 1 + \\frac{1}{x^2}\\)\n\\(f(x) = \\text{floor}(x)\\).\n\nThe floor is the smaller of the two integers bounding a number. So \\(\\text{floor}(x = 2.999) = 2\\), \\(\\text{floor}(x = 2.0001) = 2\\), and \\(\\text{floor}(x = 2) = 2.\\)\n\n\nSolution. In Figure 3.3, we can see that the first two functions are continuous, and the next two are discontinuous. \\(f(x) = 1 + \\frac{1}{x^2}\\) is discontinuous at \\(x= 0\\), and \\(f(x) = \\text{floor}(x)\\) is discontinuous at each whole number.\n\n\n\n\n\nFigure 3.3: Continuous and Discontinuous Functions\n\n\n\n\n\nSome properties of continuous functions:\n\nIf \\(f\\) and \\(g\\) are continuous at point \\(c\\), then \\(f+g\\), \\(f-g\\), \\(f \\cdot g\\), \\(|f|\\), and \\(\\alpha f\\) are continuous at point \\(c\\) also. \\(f/g\\) is continuous, provided \\(g(c)\\ne 0\\).\nBoundedness: If \\(f\\) is continuous on the closed bounded interval \\([a,b]\\), then there is a number \\(K\\) such that \\(|f(x)|\\le K\\) for each \\(x\\) in \\([a,b]\\).\nMax/Min: If \\(f\\) is continuous on the closed bounded interval \\([a,b]\\), then \\(f\\) has a maximum and a minimum on \\([a,b]\\). They may be located at the end points.\n\nExercise\nLet \\(f(x) = \\frac{x^2 + 2x}{x}.\\)\n\nGraph the function. Is it defined everywhere?\nWhat is the functions limit at \\(x \\rightarrow 0\\)?"
  },
  {
    "objectID": "04_calculus.html",
    "href": "04_calculus.html",
    "title": "4  Calculus",
    "section": "",
    "text": "Calculus is a fundamental part of any type of statistics exercise. Although you may not be taking derivatives and integral in your daily work as an analyst, calculus undergirds many concepts we use: maximization, expectation, and cumulative probability."
  },
  {
    "objectID": "04_calculus.html#example-the-mean-is-a-type-of-integral",
    "href": "04_calculus.html#example-the-mean-is-a-type-of-integral",
    "title": "4  Calculus",
    "section": "Example: The Mean is a Type of Integral",
    "text": "Example: The Mean is a Type of Integral\nThe average of a quantity is a type of weighted mean, where the potential values are weighted by their likelihood, loosely speaking. The integral is actually a general way to describe this weighted average when there are conceptually an infinite number of potential values.\nIf \\(X\\) is a continuous random variable, its expected value \\(E(X)\\) – the center of mass – is given by\n\\[E(X) = \\int^{\\infty}_{-\\infty}x f(x) dx\\]\nwhere \\(f(x)\\) is the probability density function of \\(X\\).\nThis is a continuous version of the case where \\(X\\) is discrete, in which case\n\\[E(X) = \\sum^\\infty_{j=1} x_j P(X = x_j)\\]\neven more concretely, if the potential values of \\(X\\) are finite, then we can write out the expected value as a weighted mean, where the weights is the probability that the value occurs.\n\\[E(X) = \\large \\sum_{x} \\quad\\left( \\underbrace{x}_{\\text{value}}\\cdot \\underbrace{P(X = x)}_{\\text{weight, or PMF}}\\right)\\]"
  },
  {
    "objectID": "04_calculus.html#sec-derivintro",
    "href": "04_calculus.html#sec-derivintro",
    "title": "4  Calculus",
    "section": "4.1 Derivatives",
    "text": "4.1 Derivatives\nThe derivative of \\(f\\) at \\(x\\) is its rate of change at \\(x\\): how much \\(f(x)\\) changes with a change in \\(x\\). The rate of change is a fraction — rise over run — but because not all lines are straight and the rise over run formula will give us different values depending on the range we examine, we need to take a limit (Section -Chapter 3).\n\nDefinition 4.1 (Derivative) Let \\(f\\) be a function whose domain includes an open interval containing the point \\(x\\). The derivative of \\(f\\) at \\(x\\) is given by\n\\[\\frac{d}{dx}f(x) =\\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{(x+h)-x} = \\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{h}\\]\nThere are a two main ways to denote a derivate:\n\nLeibniz Notation: \\(\\frac{d}{dx}(f(x))\\)\nPrime or Lagrange Notation: \\(f'(x)\\)\n\n\nIf \\(f(x)\\) is a straight line, the derivative is the slope. For a curve, the slope changes by the values of \\(x\\), so the derivative is the slope of the line tangent to the curve at \\(x\\). See, For example, Figure -Figure 4.1\n\n\n\n\n\nFigure 4.1: The Derivative as a Slope\n\n\n\n\nIf \\(f'(x)\\) exists at a point \\(x_0\\), then \\(f\\) is said to be differentiable at \\(x_0\\). That also implies that \\(f(x)\\) is continuous at \\(x_0\\).\n\nProperties of derivatives\nSuppose that \\(f\\) and \\(g\\) are differentiable at \\(x\\) and that \\(\\alpha\\) is a constant. Then the functions \\(f\\pm g\\), \\(\\alpha f\\), \\(f g\\), and \\(f/g\\) (provided \\(g(x)\\ne 0\\)) are also differentiable at \\(x\\). Additionally,\nConstant rule: \\[\\left[k f(x)\\right]' = k f'(x)\\]\nSum rule: \\[\\left[f(x)\\pm g(x)\\right]' = f'(x)\\pm g'(x)\\]\nWith a bit more algebra, we can apply the definition of derivatives to get a formula for of the derivative of a product and a derivative of a quotient.\nProduct rule: \\[\\left[f(x)g(x)\\right]^\\prime = f^\\prime(x)g(x)+f(x)g^\\prime(x)\\]\nQuotient rule: \\[\\left[f(x)/g(x)\\right]^\\prime = \\frac{f^\\prime(x)g(x) - f(x)g^\\prime(x)}{[g(x)]^2}, ~g(x)\\neq 0\\]\nFinally, one way to think of the power of derivatives is that it takes a function a notch down in complexity. The power rule applies to any higher-order function:\nPower rule: \\[\\left[x^k\\right]^\\prime = k x^{k-1}\\]\nFor any real number \\(k\\) (that is, both whole numbers and fractions). The power rule is proved by induction, a neat method of proof used in many fundamental applications to prove that a general statement holds for every possible case, even if there are countably infinite cases. We’ll show a simple case where \\(k\\) is an integer here.\n\nProposition 4.1 (Power Rule) \\[\\left[x^k\\right]^\\prime = k x^{k-1}\\] for any integer \\(k\\).\n\n\nProof. First, consider the first case (the base case) of \\(k = 1\\). We can show by the definition of derivatives (setting \\(f(x) = x^1 = 1\\)) that\n\\[[x^1]^\\prime = \\lim_{h \\rightarrow 0}\\frac{(x + h) - x}{(x + h) - x}= 1.\\]\nBecause \\(1\\) is also expressed as \\(1 x^{1- 1}\\), the statement we want to prove holds for the case \\(k =1\\).\nNow, that the statement holds for some integer \\(m\\). That is, assume \\[\\left[x^m\\right]^\\prime = m x^{m-1}\\]\nThen, for the case \\(m + 1\\), using the product rule above, we can simplify\n\\[\\begin{align*}\n\\left[x^{m + 1}\\right]^\\prime &= [x^{m}\\cdot x]^\\prime\\\\\n&= (x^m)^\\prime\\cdot x + (x^m)\\cdot (x)^\\prime\\\\\n&= m x^{m - 1}\\cdot x + x^m ~~\\because \\text{by previous assumption}\\\\\n&= mx^m + x^m\\\\\n&= (m + 1)x^m\\\\\n&= (m + 1)x^{(m + 1) - 1}\n\\end{align*}\\]\nTherefore, the rule holds for the case \\(k = m + 1\\) once we have assumed it holds for \\(k = m\\). Combined with the first case, this completes proof by induction – we have now proved that the statement holds for all integers \\(k = 1, 2, 3, \\cdots\\).\nTo show that it holds for real fractions as well, we can prove expressing that exponent by a fraction of two integers.\n\nThese “rules” become apparent by applying the definition of the derivative above to each of the things to be “derived”, but these come up so frequently that it is best to repeat until it is muscle memory.\n\nExercise 4.1 (Derivatives) For each of the following functions, find the first-order derivative \\(f^\\prime(x)\\).\n\n\\(f(x)=c\\)\n\\(f(x)=x\\)\n\\(f(x)=x^2\\)\n\\(f(x)=x^3\\)\n\\(f(x)=\\frac{1}{x^2}\\)\n\\(f(x)=(x^3)(2x^4)\\)\n\\(f(x) = x^4 - x^3 + x^2 - x + 1\\)\n\\(f(x) = (x^2 + 1)(x^3 - 1)\\)\n\\(f(x) = 3x^2 + 2x^{1/3}\\)\n\\(f(x)=\\frac{x^2+1}{x^2-1}\\)"
  },
  {
    "objectID": "04_calculus.html#derivpoly",
    "href": "04_calculus.html#derivpoly",
    "title": "4  Calculus",
    "section": "4.2 Higher-Order Derivatives (Derivatives of Derivatives of Derivatives)",
    "text": "4.2 Higher-Order Derivatives (Derivatives of Derivatives of Derivatives)\nThe first derivative is applying the definition of derivatives on the function, and it can be expressed as\n\\[f'(x),  ~~ y',  ~~ \\frac{d}{dx}f(x), ~~ \\frac{dy}{dx}\\]\nWe can keep applying the differentiation process to functions that are themselves derivatives. The derivative of \\(f'(x)\\) with respect to \\(x\\), would then be \\[f''(x)=\\lim\\limits_{h\\to 0}\\frac{f'(x+h)-f'(x)}{h}\\] and we can therefore call it the Second derivative:\n\\[f''(x), ~~ y'', ~~ \\frac{d^2}{dx^2}f(x), ~~ \\frac{d^2y}{dx^2}\\]\nSimilarly, the derivative of \\(f''(x)\\) would be called the third derivative and is denoted \\(f'''(x)\\). And by extension, the nth derivative is expressed as \\(\\frac{d^n}{dx^n}f(x)\\), \\(\\frac{d^ny}{dx^n}\\).\n\nExample 4.1 (Succession of derivatives) \\[\\begin{align*}\nf(x) &=x^3\\\\\nf^{\\prime}(x) &=3x^2\\\\\nf^{\\prime\\prime}(x) &=6x \\\\\nf^{\\prime\\prime\\prime}(x) &=6\\\\\nf^{\\prime\\prime\\prime\\prime}(x) &=0\\\\\n\\end{align*}\\]\n\nEarlier, in Section -Section 4.1, we said that if a function differentiable at a given point, then it must be continuous. Further, if \\(f'(x)\\) is itself continuous, then \\(f(x)\\) is called continuously differentiable. All of this matters because many of our findings about optimization (Section @ref(optim)) rely on differentiation, and so we want our function to be differentiable in as many layers. A function that is continuously differentiable infinitly is called “smooth”. Some examples: \\(f(x) = x^2\\), \\(f(x) = e^x\\)."
  },
  {
    "objectID": "04_calculus.html#composite-functions-and-the-chain-rule",
    "href": "04_calculus.html#composite-functions-and-the-chain-rule",
    "title": "4  Calculus",
    "section": "4.3 Composite Functions and the Chain Rule",
    "text": "4.3 Composite Functions and the Chain Rule\nAs useful as the above rules are, many functions you’ll see won’t fit neatly in each case immediately. Instead, they will be functions of functions. For example, the difference between \\(x^2 + 1^2\\) and \\((x^2 + 1)^2\\) may look trivial, but the sum rule can be easily applied to the former, while it’s actually not obvious what do with the latter.\nComposite functions are formed by substituting one function into another and are denoted by \\[(f\\circ g)(x)=f[g(x)].\\] To form \\(f[g(x)]\\), the range of \\(g\\) must be contained (at least in part) within the domain of \\(f\\). The domain of \\(f\\circ g\\) consists of all the points in the domain of \\(g\\) for which \\(g(x)\\) is in the domain of \\(f\\).\n\nExample 4.2 (Composite functions) Let \\(f(x)=\\log x\\) for \\(0<x<\\infty\\) and \\(g(x)=x^2\\) for \\(-\\infty<x<\\infty\\).\nThen \\[(f\\circ g)(x)=\\log x^2, -\\infty<x<\\infty - \\{0\\}\\]\nAlso \\[(g\\circ f)(x)=[\\log x]^2, 0<x<\\infty\\]\nNotice that \\(f\\circ g\\) and \\(g\\circ f\\) are not the same functions.\n\nWith the notation of composite functions in place, now we can introduce a helpful additional rule that will deal with a derivative of composite functions as a chain of concentric derivatives.\nChain Rule:\nLet \\(y=(f\\circ g)(x)= f[g(x)]\\). The derivative of \\(y\\) with respect to \\(x\\) is \\[\\frac{d}{dx} \\{ f[g(x)] \\} = f'[g(x)] g'(x)\\]\nWe can read this as: “the derivative of the composite function \\(y\\) is the derivative of \\(f\\) evaluated at \\(g(x)\\), times the derivative of \\(g\\).”\nThe chain rule can be thought of as the derivative of the “outside” times the derivative of the “inside”, remembering that the derivative of the outside function is evaluated at the value of the inside function.\n\nThe chain rule can also be written as \\[\\frac{dy}{dx}=\\frac{dy}{dg(x)} \\frac{dg(x)}{dx}\\] This expression does not imply that the \\(dg(x)\\)’s cancel out, as in fractions. They are part of the derivative notation and you can’t separate them out or cancel them.)\n\n\nExample 4.3 (Composite Exponent) Find \\(f^\\prime(x)\\) for \\(f(x) = (3x^2+5x-7)^6\\).\n\nThe direct use of a chain rule is when the exponent of is itself a function, so the power rule could not have applied generaly:\nGeneralized Power Rule:\nIf \\(f(x)=[g(x)]^p\\) for any rational number \\(p\\), \\[f^\\prime(x) =p[g(x)]^{p-1}g^\\prime(x)\\]"
  },
  {
    "objectID": "04_calculus.html#derivatives-of-natural-logs-and-the-exponent",
    "href": "04_calculus.html#derivatives-of-natural-logs-and-the-exponent",
    "title": "4  Calculus",
    "section": "4.4 Derivatives of natural logs and the exponent",
    "text": "4.4 Derivatives of natural logs and the exponent\nNatural logs and exponents (they are inverses of each other; see Section @ref(logexponents)) crop up everywhere in statistics. Their derivative is a special case from the above, but quite elegant.\n\nTheorem 4.1 (Derivative of Exponents/Logs) The functions \\(e^x\\) and the natural logarithm \\(\\log(x)\\) are continuous and differentiable in their domains, and their first derivative is \\[(e^x)^\\prime = e^x\\] \\[\\log(x)^\\prime = \\frac{1}{x}\\]\nAlso, when these are composite functions, it follows by the generalized power rule that\n\\[\\left(e^{g(x)}\\right)^\\prime = e^{g(x)} \\cdot g^\\prime(x)\\] \\[\\left(\\log g(x)\\right)^\\prime = \\frac{g^\\prime(x)}{g(x)}, ~~\\text{if}~~ g(x) > 0\\]\n\n\nDerivatives of natural exponential function (\\(e\\))\nTo repeat the main rule in Theorem @ref(thm:derivexplog), the intuition is that\n\nDerivative of \\(e^x\\) is itself: \\(\\frac{d}{dx}e^x = e^x\\) (See Figure 4.2)\nSame thing if there were a constant in front: \\(\\frac{d}{dx}\\alpha e^x = \\alpha e^x\\)\nSame thing no matter how many derivatives there are in front: \\(\\frac{d^n}{dx^n} \\alpha e^x = \\alpha e^x\\)\nChain Rule: When the exponent is a function of \\(x\\), remember to take derivative of that function and add to product. \\(\\frac{d}{dx}e^{g(x)}= e^{g(x)} g^\\prime(x)\\)\n\n\n\n\n\n\nFigure 4.2: Derivative of the Exponential Function\n\n\n\n\n\nExample 4.4 (Derivatives of exponents) Find the derivative for the following.\n\n\\(f(x)=e^{-3x}\\)\n\\(f(x)=e^{x^2}\\)\n\\(f(x)=(x-1)e^x\\)\n\n\n\n\nDerivatives of logarithms\nThe natural log is the mirror image of the natural exponent and has mirroring properties, again, to repeat the theorem,\n\nlog prime x is one over x: \\(\\frac{d}{dx} \\log x = \\frac{1}{x}\\) (Figure 4.3)\nExponents become multiplicative constants: \\(\\frac{d}{dx} \\log x^k = \\frac{d}{dx} k \\log x = \\frac{k}{x}\\)\nChain rule again: \\(\\frac{d}{dx} \\log u(x) = \\frac{u'(x)}{u(x)}\\quad\\)\nFor any positive base \\(b\\), \\(\\frac{d}{dx} b^x = (\\log b)\\left(b^x\\right)\\).\n\n\n\n\n\n\nFigure 4.3: Derivative of the Natural Log\n\n\n\n\n\nExample 4.5 (Derivatives of logs) Find \\(dy/dx\\) for the following.\n\n\\(f(x)=\\log(x^2+9)\\)\n\\(f(x)=\\log(\\log x)\\)\n\\(f(x)=(\\log x)^2\\)\n\\(f(x)=\\log e^x\\)\n\n\n\n\nOutline of Proof\nWe actually show the derivative of the log first, and then the derivative of the exponential naturally follows.\nThe general derivative of the log at any base \\(a\\) is solvable by the definition of derivatives.\n\\[\\begin{align*}\n(\\log_a x)^\\prime = \\lim\\limits_{h\\to 0} \\frac{1}{h}\\log_{a}\\left(1 + \\frac{h}{x}\\right)\n\\end{align*}\\]\nRe-express \\(g = \\frac{h}{x}\\) and get \\[\\begin{align*}\n(\\log_a x)^\\prime &= \\frac{1}{x}\\lim_{g\\to 0}\\log_{a} (1 + g)^{\\frac{1}{g}}\\\\\n&= \\frac{1}{x}\\log_a e\n\\end{align*}\\]\nBy definition of \\(e\\). As a special case, when \\(a = e\\), then \\((\\log x)^\\prime = \\frac{1}{x}\\).\nNow let’s think about the inverse, taking the derivative of \\(y = a^x\\).\n\\[\\begin{align*}\ny &= a^x \\\\\n\\Rightarrow \\log y &= x \\log a\\\\\n\\Rightarrow \\frac{y^\\prime}{y} &= \\log a\\\\\n\\Rightarrow  y^\\prime = y \\log a\\\\\n\\end{align*}\\]\nThen in the special case where \\(a = e\\),\n\\[(e^x)^\\prime = (e^x)\\]"
  },
  {
    "objectID": "04_calculus.html#partial-derivatives",
    "href": "04_calculus.html#partial-derivatives",
    "title": "4  Calculus",
    "section": "4.5 Partial Derivatives",
    "text": "4.5 Partial Derivatives\nWhat happens when there’s more than variable that is changing?\n\nIf you can do ordinary derivatives, you can do partial derivatives: just hold all the other input variables constant except for the one you’re differentiating with respect to. (Joe Blitzstein’s Math Notes)\n\nSuppose we have a function \\(f\\) now of two (or more) variables and we want to determine the rate of change relative to one of the variables. To do so, we would find its partial derivative, which is defined similar to the derivative of a function of one variable.\nPartial Derivative: Let \\(f\\) be a function of the variables \\((x_1,\\ldots,x_n)\\). The partial derivative of \\(f\\) with respect to \\(x_i\\) is\n\\[\\frac{\\partial f}{\\partial x_i} (x_1,\\ldots,x_n) = \\lim\\limits_{h\\to 0} \\frac{f(x_1,\\ldots,x_i+h,\\ldots,x_n)-f(x_1,\\ldots,x_i,\\ldots,x_n)}{h}\\]\nOnly the \\(i\\)th variable changes — the others are treated as constants.\nWe can take higher-order partial derivatives, like we did with functions of a single variable, except now the higher-order partials can be with respect to multiple variables.\n\nExample 4.6 (Partial derivatives) Notice that you can take partials with regard to different variables.\nSuppose \\(f(x,y)=x^2+y^2\\). Then\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}(x,y) &=\\\\\n\\frac{\\partial f}{\\partial y}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &=\n\\end{align*}\\]\n\n\nExercise 4.2 (Partial derivatives) Let \\(f(x,y)=x^3 y^4 +e^x -\\log y\\). What are the following partial derivatives?\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}(x,y) &=\\\\\n\\frac{\\partial f}{\\partial y}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) &=\\\\\n\\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &=\n\\end{align*}\\]"
  },
  {
    "objectID": "04_calculus.html#taylorapprox",
    "href": "04_calculus.html#taylorapprox",
    "title": "4  Calculus",
    "section": "4.6 Taylor Series Approximation",
    "text": "4.6 Taylor Series Approximation\nA common form of approximation used in statistics involves derivatives. A Taylor series is a way to represent common functions as infinite series (a sum of infinite elements) of the function’s derivatives at some point \\(a\\).\nFor example, Taylor series are very helpful in representing nonlinear (read: difficult) functions as linear (read: manageable) functions. One can thus approximate functions by using lower-order, finite series known as Taylor polynomials. If \\(a=0\\), the series is called a Maclaurin series.\nSpecifically, a Taylor series of a real or complex function \\(f(x)\\) that is infinitely differentiable in the neighborhood of point \\(a\\) is:\n\\[\\begin{align*}\n    f(x) &= f(a) + \\frac{f'(a)}{1!} (x-a) +  \\frac{f''(a)}{2!} (x-a)^2 + \\cdots\\\\\n     &= \\sum_{n=0}^\\infty \\frac{f^{(n)} (a)}{n!} (x-a)^n\n\\end{align*}\\]\nTaylor Approximation: We can often approximate the curvature of a function \\(f(x)\\) at point \\(a\\) using a 2nd order Taylor polynomial around point \\(a\\):\n\\[f(x) = f(a) + \\frac{f'(a)}{1!} (x-a) +  \\frac{f''(a)}{2!} (x-a)^2\n+ R_2\\]\n\\(R_2\\) is the remainder (R for remainder, 2 for the fact that we took two derivatives) and often treated as negligible, giving us:\n\\[f(x) \\approx f(a) + f'(a)(x-a) +  \\dfrac{f''(a)}{2} (x-a)^2\\]\nThe more derivatives that are added, the smaller the remainder \\(R\\) and the more accurate the approximation. Proofs involving limits guarantee that the remainder converges to 0 as the order of derivation increases."
  },
  {
    "objectID": "04_calculus.html#the-indefinite-integration",
    "href": "04_calculus.html#the-indefinite-integration",
    "title": "4  Calculus",
    "section": "4.7 The Indefinite Integration",
    "text": "4.7 The Indefinite Integration\nSo far, we’ve been interested in finding the derivative \\(f=F'\\) of a function \\(F\\). However, sometimes we’re interested in exactly the reverse: finding the function \\(F\\) for which \\(f\\) is its derivative. We refer to \\(F\\) as the antiderivative of \\(f\\).\n\nDefinition 4.2 (Antiderivative) The antiverivative of a function \\(f(x)\\) is a differentiable function \\(F\\) whose derivative is \\(f\\).\n\\[F^\\prime = f.\\]\n\nAnother way to describe is through the inverse formula. Let \\(DF\\) be the derivative of \\(F\\). And let \\(DF(x)\\) be the derivative of \\(F\\) evaluated at \\(x\\). Then the antiderivative is denoted by \\(D^{-1}\\) (i.e., the inverse derivative). If \\(DF=f\\), then \\(F=D^{-1}f\\).\nThis definition bolsters the main takeaway about integrals and derivatives: They are inverses of each other.\n\nExercise 4.3 (Antiderivative) Find the antiderivative of the following:\n\n\\(f(x) = \\frac{1}{x^2}\\)\n\\(f(x) = 3e^{3x}\\)\n\n\nWe know from derivatives how to manipulate \\(F\\) to get \\(f\\). But how do you express the procedure to manipulate \\(f\\) to get \\(F\\)? For that, we need a new symbol, which we will call indefinite integration.\n:::{#def-indefint}"
  },
  {
    "objectID": "04_calculus.html#indefinite-integral",
    "href": "04_calculus.html#indefinite-integral",
    "title": "4  Calculus",
    "section": "4.8 Indefinite Integral",
    "text": "4.8 Indefinite Integral\nThe indefinite integral of \\(f(x)\\) is written\n\\[\\int f(x) dx \\]\nand is equal to the antiderivative of \\(f\\).\n\nExample 4.7 (Graphing) Draw the function \\(f(x)\\) and its indefinite integral, \\(\\int\\limits f(x) dx\\)\n\\[f(x) = (x^2-4)\\]\n\n\nSolution. The Indefinite Integral of the function \\(f(x) = (x^2-4)\\) can, for example, be \\(F(x) = \\frac{1}{3}x^3 - 4x.\\) But it can also be \\(F(x) = \\frac{1}{3}x^3 - 4x + 1\\), because the constant 1 disappears when taking the derivative.\n\nSome of these functions are plotted in the bottom panel of Figure 4.4 as dotted lines.\n\n\n\n\n\nFigure 4.4: The Many Indefinite Integrals of a Function\n\n\n\n\nNotice from these examples that while there is only a single derivative for any function, there are multiple antiderivatives: one for any arbitrary constant \\(c\\). \\(c\\) just shifts the curve up or down on the \\(y\\)-axis. If more information is present about the antiderivative — e.g., that it passes through a particular point — then we can solve for a specific value of \\(c\\).\n\nCommon Rules of Integration\nSome common rules of integrals follow by virtue of being the inverse of a derivative.\n\nConstants are allowed to slip out: \\(\\int a f(x)dx = a\\int f(x)dx\\)\nIntegration of the sum is sum of integrations: \\(\\int [f(x)+g(x)]dx=\\int f(x)dx + \\int g(x)dx\\)\nReverse Power-rule: \\(\\int x^n dx = \\frac{1}{n+1} x^{n+1} + c\\)\nExponents are still exponents: \\(\\int e^x dx = e^x +c\\)\nRecall the derivative of \\(\\log(x)\\) is one over \\(x\\), and so: \\(\\int \\frac{1}{x} dx = \\log x + c\\)\nReverse chain-rule: \\(\\int e^{f(x)}f^\\prime(x)dx = e^{f(x)}+c\\)\nMore generally: \\(\\int [f(x)]^n f'(x)dx = \\frac{1}{n+1}[f(x)]^{n+1}+c\\)\nRemember the derivative of a log of a function: \\(\\int \\frac{f^\\prime(x)}{f(x)}dx=\\log f(x) + c\\)\n\n\nExample 4.8 (Common Integration) Simplify the following indefinite integrals:\n\n(3x^2 dx)\n((2x+1)dx)\n(e^x e{ex} dx)"
  },
  {
    "objectID": "04_calculus.html#the-definite-integral-the-area-under-the-curve",
    "href": "04_calculus.html#the-definite-integral-the-area-under-the-curve",
    "title": "4  Calculus",
    "section": "4.9 The Definite Integral: The Area under the Curve",
    "text": "4.9 The Definite Integral: The Area under the Curve\nIf there is a indefinite integral, there must be a definite integral. Indeed there is, but the notion of definite integrals comes from a different objective: finding the are a under a function. We will find, perhaps remarkably, that the formula we find to get the sum turns out to be expressible by the anti-derivative.\nSuppose we want to determine the area \\(A(R)\\) of a region \\(R\\) defined by a curve \\(f(x)\\) and some interval \\(a\\le x \\le b\\).\n\n\n\n\n\nFigure 4.5: The Riemann Integral as a Sum of Evaluations\n\n\n\n\nOne way to calculate the area would be to divide the interval \\(a\\le x\\le b\\) into \\(n\\) subintervals of length \\(\\Delta x\\) and then approximate the region with a series of rectangles, where the base of each rectangle is \\(\\Delta x\\) and the height is \\(f(x)\\) at the midpoint of that interval. \\(A(R)\\) would then be approximated by the area of the union of the rectangles, which is given by \\[S(f,\\Delta x)=\\sum\\limits_{i=1}^n f(x_i)\\Delta x\\] and is called a Riemann sum.\nAs we decrease the size of the subintervals \\(\\Delta x\\), making the rectangles “thinner,” we would expect our approximation of the area of the region to become closer to the true area. This allows us to express the area as a limit of a series: \\[A(R)=\\lim\\limits_{\\Delta x\\to 0}\\sum\\limits_{i=1}^n f(x_i)\\Delta x\\]\nFigure 4.5 shows that illustration. The curve depicted is \\(f(x) = -15(x - 5) + (x - 5)^3 + 50.\\) We want approximate the area under the curve between the \\(x\\) values of 0 and 10. We can do this in blocks of arbitrary width, where the sum of rectangles (the area of which is width times \\(f(x)\\) evaluated at the midpoint of the bar) shows the Riemann Sum. As the width of the bars \\(\\Delta x\\) becomes smaller, the better the estimate of \\(A(R)\\).\nThis is how we define the “Definite” Integral:\n\nDefinition 4.3 (The Definite Integral (Riemann)) If for a given function \\(f\\) the Riemann sum approaches a limit as \\(\\Delta x \\to 0\\), then that limit is called the Riemann integral of \\(f\\) from \\(a\\) to \\(b\\). We express this with the \\(\\int\\), symbol, and write \\[\\int\\limits_a^b f(x) dx= \\lim\\limits_{\\Delta x\\to 0} \\sum\\limits_{i=1}^n f(x_i)\\Delta x\\]\nThe most straightforward of a definite integral is the definite integral. That is, we read \\[\\int\\limits_a^b f(x) dx\\] as the definite integral of \\(f\\) from \\(a\\) to \\(b\\) and we defined as the area under the “curve” \\(f(x)\\) from point \\(x=a\\) to \\(x=b\\).\n\nThe fundamental theorem of calculus shows us that this sum is, in fact, the antiderivative.\n\nTheorem 4.2 (First Fundamental Theorem of Calculus) Let the function \\(f\\) be bounded on \\([a,b]\\) and continuous on \\((a,b)\\). Then, suggestively, use the symbol \\(F(x)\\) to denote the definite integral from \\(a\\) to \\(x\\): \\[F(x)=\\int\\limits_a^x f(t)dt, \\quad a\\le x\\le b\\]\nThen \\(F(x)\\) has a derivative at each point in \\((a,b)\\) and \\[F^\\prime(x)=f(x), \\quad a<x<b\\] That is, the definite integral function of \\(f\\) is the one of the antiderivatives of some \\(f\\).\n\nThis is again a long way of saying that that differentiation is the inverse of integration. But now, we’ve covered definite integrals.\nThe second theorem gives us a simple way of computing a definite integral as a function of indefinite integrals.\n\nTheorem 4.3 (Second Fundamental Theorem of Calculus) Let the function \\(f\\) be bounded on \\([a,b]\\) and continuous on \\((a,b)\\). Let \\(F\\) be any function that is continuous on \\([a,b]\\) such that \\(F^\\prime(x)=f(x)\\) on \\((a,b)\\). Then \\[\\int\\limits_a^bf(x)dx = F(b)-F(a)\\]\n\nSo the procedure to calculate a simple definite integral \\(\\int\\limits_a^b f(x)dx\\) is then\n\nFind the indefinite integral \\(F(x)\\).\nEvaluate \\(F(b)-F(a)\\).\n\n\nExample 4.9 (Definite Integral of a monomial) Solve \\(\\int\\limits_1^3 3x^2 dx.\\)\nLet \\(f(x) = 3x^2\\).\n\n\nExercise 4.4 (Indefinite integrals) What is the value of \\(\\int\\limits_{-2}^2 e^x e^{e^x} dx\\)?\n\n\nCommon Rules for Definite Integrals\nThe area-interpretation of the definite integral provides some rules for simplification.\n\nThere is no area below a point: \\[\\int\\limits_a^a f(x)dx=0\\]\nReversing the limits changes the sign of the integral: \\[\\int\\limits_a^b f(x)dx=-\\int\\limits_b^a f(x)dx\\]\nSums can be separated into their own integrals: \\[\\int\\limits_a^b [\\alpha f(x)+\\beta g(x)]dx = \\alpha \\int\\limits_a^b f(x)dx + \\beta \\int\\limits_a^b g(x)dx\\]\nAreas can be combined as long as limits are linked: \\[\\int\\limits_a^b f(x) dx +\\int\\limits_b^c f(x)dx = \\int\\limits_a^c f(x)dx\\]\n\n\nExercise 4.5 (Definite integrals) Simplify the following definite intergrals.\n\n\\(\\int\\limits_1^1 3x^2 dx =\\)\n\\(\\int\\limits_0^4 (2x+1)dx=\\)\n\\(\\int\\limits_{-2}^0 e^x e^{e^x} dx + \\int\\limits_0^2 e^x e^{e^x} dx =\\)"
  },
  {
    "objectID": "04_calculus.html#integration-by-substitution",
    "href": "04_calculus.html#integration-by-substitution",
    "title": "4  Calculus",
    "section": "4.10 Integration by Substitution",
    "text": "4.10 Integration by Substitution\nFrom the second fundamental theorem of calculus, we now that a quick way to get a definite integral is to first find the indefinite integral, and then just plug in the bounds.\nSometimes the integrand (the thing that we are trying to take an integral of) doesn’t appear integrable using common rules and antiderivatives. A method one might try is integration by substitution, which is related to the Chain Rule.\nSuppose we want to find the indefinite integral \\[\\int g(x)dx\\] but \\(g(x)\\) is complex and none of the formulas we have seen so far seem to apply immediately. The trick is to come up with a new function \\(u(x)\\) such that \\[g(x)=f[u(x)]u'(x).\\]\nWhy does an introduction of yet another function end of simplifying things? Let’s refer to the antiderivative of \\(f\\) as \\(F\\). Then the chain rule tells us that \\[\\frac{d}{dx} F[u(x)]=f[u(x)]u'(x)\\]. So, \\(F[u(x)]\\) is the antiderivative of \\(g\\). We can then write \\[\\int g(x) dx= \\int f[u(x)]u'(x)dx = \\int \\frac{d}{dx} F[u(x)]dx = F[u(x)]+c\\]\nTo summarize, the procedure to determine the indefinite integral \\(\\int g(x)dx\\) by the method of substitution:\n\nIdentify some part of \\(g(x)\\) that might be simplified by substituting in a single variable \\(u\\) (which will then be a function of \\(x\\)).\nDetermine if \\(g(x)dx\\) can be reformulated in terms of \\(u\\) and \\(du\\).\nSolve the indefinite integral.\nSubstitute back in for \\(x\\)\n\nSubstitution can also be used to calculate a definite integral. Using the same procedure as above, \\[\\int\\limits_a^b g(x)dx=\\int\\limits_c^d f(u)du = F(d)-F(c)\\] where \\(c=u(a)\\) and \\(d=u(b)\\).\n\nExample 4.10 Integration by Substitution I\nSolve the indefinite integral \\[\\int x^2 \\sqrt{x+1}dx.\\]\n\nFor the above problem, we could have also used the substitution \\(u=\\sqrt{x+1}\\). Then \\(x=u^2-1\\) and \\(dx=2u du\\). Substituting these in, we get \\[\\int x^2\\sqrt{x+1}dx=\\int (u^2-1)^2 u 2u du\\] which when expanded is again a polynomial and gives the same result as above.\nAnother case in which integration by substitution is is useful is with a fraction.\n\nExample 4.11 (Integration by Substitutiton II) Simplify \\[\\int\\limits_0^1 \\frac{5e^{2x}}{(1+e^{2x})^{1/3}}dx.\\]"
  },
  {
    "objectID": "04_calculus.html#integration-by-parts",
    "href": "04_calculus.html#integration-by-parts",
    "title": "4  Calculus",
    "section": "4.11 Integration by Parts",
    "text": "4.11 Integration by Parts\nAnother useful integration technique is integration by parts, which is related to the Product Rule of differentiation. The product rule states that \\[\\frac{d}{dx}(uv)=u\\frac{dv}{dx}+v\\frac{du}{dx}\\] Integrating this and rearranging, we get \\[\\int u\\frac{dv}{dx}dx= u v - \\int v \\frac{du}{dx}dx\\] or \\[\\int u(x) v'(x)dx=u(x)v(x) - \\int v(x)u'(x)dx\\]\nMore easily remembered with the mnemonic “Ultraviolet Voodoo”: \\[\\int u dv = u v - \\int v du\\] where \\(du=u'(x)dx\\) and \\(dv=v'(x)dx\\).\nFor definite integrals, this is simply \\[\\int\\limits_a^b u\\frac{dv}{dx}dx = \\left. u v \\right|_a^b - \\int\\limits_a^b v \\frac{du}{dx}dx\\]\nOur goal here is to find expressions for \\(u\\) and \\(dv\\) that, when substituted into the above equation, yield an expression that’s more easily evaluated.\n\nExample 4.12 (Integration by parts) Simplify the following integrals. These seemingly obscure forms of integrals come up often when integrating distributions.\n\\[\\int x e^{ax} dx\\]\n\n\nSolution. Let \\(u=x\\) and \\(\\frac{dv}{dx} = e^{ax}\\). Then \\(du=dx\\) and \\(v=(1/a)e^{ax}\\). Substituting this into the integration by parts formula, we obtain\n\\[\\begin{eqnarray}\n\\int x e^{ax} dx &=& u v - \\int v du\\nonumber\\\\\n                &=&x\\left( \\frac{1}{a}e^{ax}\\right) -\\int\\frac{1}{a}e^{ax}dx\\nonumber\\\\\n                &=&\\frac{1}{a}xe^{ax}-\\frac{1}{a^2}e^{ax}+c\\nonumber\n\\end{eqnarray}\\]\n\n\n\nExercise 4.6 (Integration by parts) \nIntegrate \\[\\int x^n e^{ax} dx\\]\nIntegrate \\[\\int x^3 e^{-x^2} dx\\]"
  },
  {
    "objectID": "05_optimization.html",
    "href": "05_optimization.html",
    "title": "5  Optimization",
    "section": "",
    "text": "To optimize, we use derivatives and calculus. Optimization is to find the maximum or minimum of a functon, and to find what value of an input gives that extremum. This has obvious uses in engineering. Many tools in the statistical toolkit use optimization. One of the most common ways of estimating a model is through “Maximum Likelihood Estimation”, done via optimizing a function (the likelihood).\nOptimization also comes up in Economics, Formal Theory, and Political Economy all the time. A go-to model of human behavior is that they optimize a certain utility function. Humans are not pure utility maximizers, of course, but nuanced models of optimization – for example, adding constraints and adding uncertainty – will prove to be quite useful."
  },
  {
    "objectID": "05_optimization.html#example-meltzer-richard",
    "href": "05_optimization.html#example-meltzer-richard",
    "title": "5  Optimization",
    "section": "Example: Meltzer-Richard",
    "text": "Example: Meltzer-Richard\nA standard backdrop in comparative political economy, the Meltzer-Richard (1981) model states that redistribution of wealth should be higher in societies where the median income is much smaller than the average income. More to the point, typically income distributions where the median is very different from the average is one of high inequality. In other words, the Meltzer-Richard model says that highly unequal economies will have more re-distribution of wealth. Why is that the case? Here is a simplified example that is not the exact model by Meltzer and Richard1, but adapted from Persson and Tabellini2\nWe will set the following things about our model human and model democracy.\n\nIndividuals are indexed by \\(i\\), and the total population is normalized to unity (“1”) without loss of generality.\n\\(U(\\cdot)\\), u for “utility”, is a function that is concave and increasing, and expresses the utility gained from public goods. This tells us that its first derivative is positive, and its second derivative is negative.\n\\(y_i\\) is the income of person \\(i\\)\n\\(W_i\\), w for “welfare”, is the welfare of person \\(i\\)\n\\(c_i\\), c for “consumption”, is the consumption utility of person \\(i\\)\n\nAlso, the government is democratically elected and sets the following redistribution output:\n\n\\(\\tau\\), t for “tax”, is a flat tax rate between 0 and 1 that is applied to everyone’s income.\n\\(g\\), “g” for “goods”, is the amount of public goods that the government provides.\n\nSuppose an individual’s welfare is given by: \\[W_i = c_i + U(g)\\]\nThe consumption good is the person’s post-tax income.\n\\[c_i = (1 - \\tau) y_i\\]\nIncome varies by person (In the next section we will cover probability, by then we will know that we can express this by saying that \\(y\\) is a random variable with the cumulative distribution function \\(F\\), i.e. \\(y \\sim F\\).). Every distribution has a mean and an median.\n\n\\(E(y)\\) is the average income of the society.\n\\(\\text{med}(y)\\) is the median income of the society.\n\nWhat will happen in this economy? What will the tax rate be set too? How much public goods will be provided?\nWe’ve skipped ahead of some formal theory results of democracy, but hopefully these are conceptually intuitive. First, if a democracy is competitive, there is no slack in the government’s goods, and all tax revenue becomes a public good. So we can go ahead and set the constraint:\n\\[g = \\sum_{i} \\tau y_i P(y_i) = \\tau E(y)\\]\nWe can do this trick because of the “normalizes to unity” setting, but this is a general property of the average.\nNow given this constraint we can re-write an individual’s welfare as\n\\[\\begin{align*}\nW_i &= \\left(1 - \\frac{g}{E(y)}\\right)y_i + U(g)\\\\\n&= \\left(E(y) - g\\right) \\frac{1}{E(y)} y_i + U(g)\\\\\n&= \\left(E(y) - g\\right) \\frac{y_i}{E(y)} + U(g)\\\\\n\\end{align*}\\]\nWhen is the individual’s welfare maximized, as a function of the public good? \\[\\begin{align*}\n\\frac{d}{dg}W_i &=  - \\frac{y_i}{E(y)} + \\frac{d}{dg}U(g)\\\\\n\\end{align*}\\]\n\\(\\frac{d}{dg}W_i = 0\\) when \\(\\frac{d}{dg}U(g) = \\frac{y_i}{E(y)}\\), and so after expressing the derivative as \\(U_g = \\frac{d}{dg}U(g)\\) for simplicity,\n\\[g_i^\\star = {U_g}^{-1}\\left(\\frac{y_i}{E(y)}\\right)\\]\nNow recall that because we assumed concavity, \\(U_g\\) is a negative sloping function whose value is positive. It can be shown that the inverse of such a function is also decreasing. Thus an individual’s preferred level of government is determined by a single continuum, the person’s income divided by the average income, and the function is decreasing in \\(y_i\\). This is consistent with our intuition that richer people prefer less redistribution.\nThat was the amount for any given person. The government has to set one value of \\(g\\), however. So what will that be? Now we will use another result, the median voter theorem. This says that under certain general electoral conditions (single-peaked preferences, two parties, majority rule), the policy winner will be that preferred by the median person in the population. Because the only thing that determines a person’s preferred level of government is \\(y_i / E(y)\\), we can presume that the median voter, whose income is \\(\\text{med}(y)\\) will prevail in their preferred choice of government. Therefore, we wil see\n\\[g^\\star = {U_g}^{-1}\\left(\\frac{\\text{med}(y)}{E(y)}\\right)\\]\nWhat does this say about the level of redistribution we observe in an economy? The higher the average income is than the median income, which often (but not always) means more inequality, there should be more redistribution."
  },
  {
    "objectID": "05_optimization.html#maxima-and-minima",
    "href": "05_optimization.html#maxima-and-minima",
    "title": "5  Optimization",
    "section": "5.1 Maxima and Minima",
    "text": "5.1 Maxima and Minima\nThe first derivative, \\(f'(x)\\), quantifies the slope of a function. Therefore, it can be used to check whether the function \\(f(x)\\) at the point \\(x\\) is increasing or decreasing at \\(x\\).\n\nIncreasing: \\(f'(x)>0\\)\nDecreasing: \\(f'(x)<0\\)\nNeither increasing nor decreasing: \\(f'(x)=0\\) i.e. a maximum, minimum, or saddle point\n\nSo for example, \\(f(x) = x^2 + 2\\) and \\(f^\\prime(x) = 2x\\)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nMaxima and Minima\n\n\n\n\n\nExercise 5.1 (Plotting a maximum and minimum) Plot \\(f(x)=x^3+ x^2 + 2\\), plot its derivative, and identifiy where the derivative is zero. Is there a maximum or minimum?\n\n\n\n\nThe second derivative \\(f''(x)\\) identifies whether the function \\(f(x)\\) at the point \\(x\\) is\n\nConcave down: \\(f''(x)<0\\)\nConcave up (convex): \\(f''(x)>0\\)\n\nMaximum (Minimum): \\(x_0\\) is a local maximum (minimum) if \\(f(x_0)>f(x)\\) (\\(f(x_0)<f(x))\\) for all \\(x\\) within some open interval containing \\(x_0\\). \\(x_0\\) is a global maximum (minimum) if \\(f(x_0)>f(x)\\) (\\(f(x_0)<f(x))\\) for all \\(x\\) in the domain of \\(f\\).\nGiven the function \\(f\\) defined over domain \\(D\\), all of the following are defined as critical points:\n\nAny interior point of \\(D\\) where \\(f'(x)=0\\).\nAny interior point of \\(D\\) where \\(f'(x)\\) does not exist.\nAny endpoint that is in \\(D\\).\n\nThe maxima and minima will be a subset of the critical points.\nSecond Derivative Test of Maxima/Minima: We can use the second derivative to tell us whether a point is a maximum or minimum of \\(f(x)\\).\n\nLocal Maximum: \\(f'(x)=0\\) and \\(f''(x)<0\\)\nLocal Minimum: \\(f'(x)=0\\) and \\(f''(x)>0\\)\nNeed more info: \\(f'(x)=0\\) and \\(f''(x)=0\\)\n\nGlobal Maxima and Minima Sometimes no global max or min exists — e.g., \\(f(x)\\) not bounded above or below. However, there are three situations where we can fairly easily identify global max or min.\n\nFunctions with only one critical point. If \\(x_0\\) is a local max or min of \\(f\\) and it is the only critical point, then it is the global max or min.\nGlobally concave up or concave down functions. If \\(f''(x)\\) is never zero, then there is at most one critical point. That critical point is a global maximum if \\(f''<0\\) and a global minimum if \\(f''>0\\).\nFunctions over closed and bounded intervals must have both a global maximum and a global minimum.\n\n\nExample 5.1 (Maxima and Minima by drawing) Find any critical points and identify whether they are a max, min, or saddle point:\n\n\\(f(x)=x^2+2\\)\n\\(f(x)=x^3+2\\)\n\\(f(x)=|x^2-1|\\), \\(x\\in [-2,2]\\)"
  },
  {
    "objectID": "05_optimization.html#concavity-of-a-function",
    "href": "05_optimization.html#concavity-of-a-function",
    "title": "5  Optimization",
    "section": "5.2 Concavity of a Function",
    "text": "5.2 Concavity of a Function\nConcavity helps identify the curvature of a function, \\(f(x)\\), in 2 dimensional space.\n\nDefinition 5.1 (Concave Function) A function \\(f\\) is strictly concave over the set S \\(\\forall x_1,x_2 \\in S\\) and \\(\\forall a \\in (0,1)\\), \\[f(ax_1 + (1-a)x_2) > af(x_1) + (1-a)f(x_2)\\] line connecting two points on a concave function will lie the function.\n\n\n\n\n\n\n\nDefinition 5.2 (Convex Function) Convex: A function f is strictly convex over the set S \\(\\forall x_1,x_2 \\in S\\) and \\(\\forall a \\in (0,1)\\), \\[f(ax_1 + (1-a)x_2) < af(x_1) + (1-a)f(x_2)\\]\nAny line connecting two points on a convex function will lie above the function.\n\nSometimes, concavity and convexity are strict of a requirement. For most purposes of getting solutions, what we call quasi-concavity is enough.\n\nDefinition 5.3 (Quasiconcave Function) A function f is quasiconcave over the set S if \\(\\forall x_1,x_2 \\in S\\) and \\(\\forall a \\in (0,1)\\), \\[f(ax_1 + (1-a)x_2) \\ge \\min(f(x_1),f(x_2))\\]\nNo matter what two points you select, the valued point will always be an end point.\n\n\nDefinition 5.4 (Quasiconvex Function) A function f is quasiconvex over the set \\(S\\) if \\(\\forall x_1,x_2 \\in S\\) and \\(\\forall a \\in (0,1)\\), \\[f(ax_1 + (1-a)x_2) \\le \\max(f(x_1),f(x_2))\\] No matter what two points you select, the valued point will always be an end point.\n\nSecond Derivative Test of Concavity: The second derivative can be used to understand concavity.\nIf \\[\\begin{array}{lll}\nf''(x) < 0 & \\Rightarrow & \\text{Concave}\\\\\nf''(x) > 0 & \\Rightarrow & \\text{Convex}\n\\end{array}\\]\n\nQuadratic Forms\nQuadratic forms is shorthand for a way to summarize a function. This is important for finding concavity because\n\nApproximates local curvature around a point — e.g., used to identify max vs min vs saddle point.\nThey are simple to express even in \\(n\\) dimensions:\nHave a matrix representation.\n\nQuadratic Form: A polynomial where each term is a monomial of degree 2 in any number of variables:\n\\[\\begin{align*}\n\\text{One variable: }& Q(x_1) = a_{11}x_1^2\\\\\n\\text{Two variables: }& Q(x_1,x_2) = a_{11}x_1^2 + a_{12}x_1x_2 + a_{22}x_2^2\\\\\n\\text{N variables: }& Q(x_1,\\cdots,x_n)=\\sum\\limits_{i\\le j} a_{ij}x_i x_j\n\\end{align*}\\]\nwhich can be written in matrix terms:\nOne variable\n\\[Q(\\mathbf{x}) = x_1^\\top a_{11} x_1\\]\nN variables: \\[\\begin{align*}\nQ(\\mathbf{x}) &=\\begin{pmatrix} x_1 & x_2 & \\cdots & x_n \\end{pmatrix}\\begin{pmatrix}\na_{11}&\\frac{1}{2}a_{12}&\\cdots&\\frac{1}{2}a_{1n}\\\\\n\\frac{1}{2}a_{12}&a_{22}&\\cdots&\\frac{1}{2}a_{2n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\frac{1}{2}a_{1n}&\\frac{1}{2}a_{2n}&\\cdots&a_{nn}\n\\end{pmatrix}\n\\begin{pmatrix} x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{pmatrix}\\\\\n&= \\mathbf{x}^\\top\\mathbf{Ax}\n\\end{align*}\\]\nFor example, the Quadratic on \\(\\mathbf{R}^2\\): \\[\\begin{align*}\n  Q(x_1,x_2)&=\\begin{pmatrix} x_1& x_2 \\end{pmatrix} \\begin{pmatrix} a_{11}&\\frac{1}{2} a_{12}\\\\\n  \\frac{1}{2}a_{12}&a_{22}\\end{pmatrix} \\begin{pmatrix} x_1\\\\x_2 \\end{pmatrix} \\\\\n  &= a_{11}x_1^2 + a_{12}x_1x_2 + a_{22}x_2^2\n\\end{align*}\\]\n\n\nDefiniteness of Quadratic Forms\nWhen the function \\(f(\\mathbf{x})\\) has more than two inputs, determining whether it has a maxima and minima (remember, functions may have many inputs but they have only one output) is a bit more tedious. Definiteness helps identify the curvature of a function, \\(Q(\\textbf{x})\\), in n dimensional space.\nDefiniteness: By definition, a quadratic form always takes on the value of zero when \\(x = 0\\), \\(Q(\\textbf{x})=0\\) at \\(\\textbf{x}=0\\). The definiteness of the matrix \\(\\textbf{A}\\) is determined by whether the quadratic form \\(Q(\\textbf{x})=\\textbf{x}^\\top\\textbf{A}\\textbf{x}\\) is greater than zero, less than zero, or sometimes both over all \\(\\mathbf{x}\\ne 0\\)."
  },
  {
    "objectID": "05_optimization.html#foc-and-soc",
    "href": "05_optimization.html#foc-and-soc",
    "title": "5  Optimization",
    "section": "5.3 FOC and SOC",
    "text": "5.3 FOC and SOC\nWe can see from a graphical representation that if a point is a local maxima or minima, it must meet certain conditions regarding its derivative. These are so commonly used that we refer these to “First Order Conditions” (FOCs) and “Second Order Conditions” (SOCs) in the economic tradition.\n\nFirst Order Conditions\nWhen we examined functions of one variable \\(x\\), we found critical points by taking the first derivative, setting it to zero, and solving for \\(x\\). For functions of \\(n\\) variables, the critical points are found in much the same way, except now we set the partial derivatives equal to zero. Note: We will only consider critical points on the interior of a function’s domain.\nIn a derivative, we only took the derivative with respect to one variable at a time. When we take the derivative separately with respect to all variables in the elements of \\(\\mathbf{x}\\) and then express the result as a vector, we use the term Gradient and Hessian.\n\nDefinition 5.5 (Gradient) Given a function \\(f(\\textbf{x})\\) in \\(n\\) variables, the gradient \\(\\nabla f(\\mathbf{x})\\) (the greek letter nabla ) is a column vector, where the \\(i\\)th element is the partial derivative of \\(f(\\textbf{x})\\) with respect to \\(x_i\\):\n\\[\\nabla f(\\mathbf{x}) = \\begin{pmatrix}\n\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}\\\\ \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}\\\\\n  \\vdots \\\\ \\frac{\\partial f(\\mathbf{x})}{\\partial x_n} \\end{pmatrix}\\]\n\nBefore we know whether a point is a maxima or minima, if it meets the FOC it is a “Critical Point”.\n\nDefinition 5.6 (Critical Point) \\(\\mathbf{x}^*\\) is a critical point if and only if \\(\\nabla f(\\mathbf{x}^*)=0\\). If the partial derivative of f(x) with respect to \\(x^*\\) is 0, then \\(\\mathbf{x}^*\\) is a critical point. To solve for \\(\\mathbf{x}^*\\), find the gradient, set each element equal to 0, and solve the system of equations. \\[\\mathbf{x}^* = \\begin{pmatrix} x_1^*\\\\x_2^*\\\\ \\vdots \\\\ x_n^*\\end{pmatrix}\\]\n\n\nExample 5.2 Example: Given a function \\(f(\\mathbf{x})=(x_1-1)^2+x_2^2+1\\), find the (1) Gradient and (2) Critical point of \\(f(\\mathbf{x})\\).\n\n\nSolution. Gradient\n\\[\\begin{align*}\n\\nabla f(\\mathbf{x}) &= \\begin{pmatrix}\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}\\\\ \\frac{\\partial f(\\mathbf{x})}{\\partial x_2} \\end{pmatrix}\\\\\n&= \\begin{pmatrix} 2(x_1-1)\\\\ 2x_2 \\end{pmatrix}\n\\end{align*}\\]\nCritical Point \\(\\mathbf{x}^* =\\)\n\\[\\begin{align*}\n&\\frac{\\partial f(\\mathbf{x})}{\\partial x_1} = 2(x_1-1) = 0\\\\\n&\\Rightarrow x_1^* = 1\\\\\n&\\frac{\\partial f(\\mathbf{x})}{\\partial x_2} = 2x_2 = 0\\\\\n&\\Rightarrow   x_2^* = 0\\\\\n\\end{align*}\\]\nSo \\[\\mathbf{x}^* = (1,0)\\]\n\n\n\nSecond Order Conditions\nWhen we found a critical point for a function of one variable, we used the second derivative as a indicator of the curvature at the point in order to determine whether the point was a min, max, or saddle (second derivative test of concavity). For functions of \\(n\\) variables, we use second order partial derivatives as an indicator of curvature.\n\nDefinition 5.7 (Hessian) Given a function \\(f(\\mathbf{x})\\) in \\(n\\) variables, the hessian \\(\\mathbf{H(x)}\\) is an \\(n\\times n\\) matrix, where the \\((i,j)\\)th element is the second order partial derivative of \\(f(\\mathbf{x})\\) with respect to \\(x_i\\) and \\(x_j\\):\n\\[\\mathbf{H(x)}=\\begin{pmatrix}\n\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1^2}&\\frac{\\partial^2f(\\mathbf{x})}{\\partial x_1 \\partial x_2}&\n\\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 \\partial x_n}\\\\\n\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_2 \\partial x_1}&\\frac{\\partial^2f(\\mathbf{x})}{\\partial x_2^2}&\n\\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_2 \\partial x_n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_n \\partial x_1}&\\frac{\\partial^2f(\\mathbf{x})}{\\partial x_n \\partial x_2}&\n\\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_n^2}\\end{pmatrix}\\]\n\nNote that the hessian will be a symmetric matrix because \\(\\frac{\\partial f(\\mathbf{x})}{\\partial x_1\\partial x_2} = \\frac{\\partial f(\\mathbf{x})}{\\partial x_2\\partial x_1}\\).\nAlso note that given that \\(f(\\mathbf{x})\\) is of quadratic form, each element of the hessian will be a constant.\nThese definitions will be employed when we determine the Second Order Conditions of a function:\nGiven a function \\(f(\\mathbf{x})\\) and a point \\(\\mathbf{x}^*\\) such that \\(\\nabla f(\\mathbf{x}^*)=0\\),\n\nHessian is Positive Definite \\(\\quad \\Longrightarrow \\quad\\) Strict Local Min\nHessian is Positive Semidefinite \\(\\forall \\mathbf{x}\\in B(\\mathbf{x}^*,\\epsilon)\\)} \\(\\quad \\Longrightarrow \\quad\\) Local Min\nHessian is Negative Definite \\(\\quad \\Longrightarrow \\quad\\) Strict Local Max\nHessian is Negative Semidefinite \\(\\forall \\mathbf{x}\\in B(\\mathbf{x}^*,\\epsilon)\\)} \\(\\quad \\Longrightarrow \\quad\\) Local Max\nHessian is Indefinite \\(\\quad \\Longrightarrow \\quad\\) Saddle Point\n\n\nExample 5.3 (Max and min with two dimensions) We found that the only critical point of \\(f(\\mathbf{x})=(x_1-1)^2+x_2^2+1\\) is at \\(\\mathbf{x}^*=(1,0)\\). Is it a min, max, or saddle point?\n\n\nSolution. The Hessian is \\[\\begin{align*}\n\\mathbf{H(x)} &= \\begin{pmatrix} 2&0\\\\0&2 \\end{pmatrix}\n\\end{align*}\\]\nThe Leading principal minors of the Hessian are \\(M_1=2; M_2=4\\). Now we consider Definiteness. Since both leading principal minors are positive, the Hessian is positive definite.\nMaxima, Minima, or Saddle Point? Since the Hessian is positive definite and the gradient equals 0, \\(x^\\star = (1,0)\\) is a strict local minimum.\nNote: Alternate check of definiteness. Is \\(\\mathbf{H(x^*)} \\geq \\leq 0 \\quad \\forall \\quad \\mathbf{x}\\ne 0\\)\n\\[\\begin{align*}\n\\mathbf{x}^\\top H(\\mathbf{x}^*) \\mathbf{x} &= \\begin{pmatrix} x_1 & x_2 \\end{pmatrix}\\\\\n&= \\begin{pmatrix} 2&0\\\\0&2 \\end{pmatrix}\\\\\n\\begin{pmatrix} x_1\\\\x_2\\end{pmatrix} &= 2x_1^2+2x_2^2\n\\end{align*}\\]\nFor any \\(\\mathbf{x}\\ne 0\\), \\(2(x_1^2+x_2^2)>0\\), so the Hessian is positive definite and \\(\\mathbf{x}^*\\) is a strict local minimum.\n\n\n\nDefiniteness and Concavity\nAlthough definiteness helps us to understand the curvature of an n-dimensional function, it does not necessarily tell us whether the function is globally concave or convex.\nWe need to know whether a function is globally concave or convex to determine whether a critical point is a global min or max. We can use the definiteness of the Hessian to determine whether a function is globally concave or convex:\n\nHessian is Positive Semidefinite \\(\\forall \\mathbf{x}\\)} \\(\\quad \\Longrightarrow \\quad\\) Globally Convex\nHessian is Negative Semidefinite \\(\\forall \\mathbf{x}\\)} \\(\\quad \\Longrightarrow \\quad\\) Globally Concave\n\nNotice that the definiteness conditions must be satisfied over the entire domain."
  },
  {
    "objectID": "05_optimization.html#global-maxima-and-minima",
    "href": "05_optimization.html#global-maxima-and-minima",
    "title": "5  Optimization",
    "section": "5.4 Global Maxima and Minima",
    "text": "5.4 Global Maxima and Minima\nGlobal Max/Min Conditions: Given a function \\(f(\\mathbf{x})\\) and a point \\(\\mathbf{x}^*\\) such that \\(\\nabla f(\\mathbf{x}^*)=0\\),\nNote that showing that \\(\\mathbf{H(x^*)}\\) is negative semidefinite is not enough to guarantee \\(\\mathbf{x}^*\\) is a local max. However, showing that \\(\\mathbf{H(x)}\\) is negative semidefinite for all \\(\\mathbf{x}\\) guarantees that \\(x^*\\) is a global max. (The same goes for positive semidefinite and minima.)\\\nExample: Take \\(f_1(x)=x^4\\) and \\(f_2(x)=-x^4\\). Both have \\(x=0\\) as a critical point. Unfortunately, \\(f''_1(0)=0\\) and \\(f''_2(0)=0\\), so we can’t tell whether \\(x=0\\) is a min or max for either. However, \\(f''_1(x)=12x^2\\) and \\(f''_2(x)=-12x^2\\). For all \\(x\\), \\(f''_1(x)\\ge 0\\) and \\(f''_2(x)\\le 0\\) — i.e., \\(f_1(x)\\) is globally convex and \\(f_2(x)\\) is globally concave. So \\(x=0\\) is a global min of \\(f_1(x)\\) and a global max of \\(f_2(x)\\).\n\nExercise 5.2 (Optimization) Given \\(f(\\mathbf{x})=x_1^3-x_2^3+9x_1x_2\\), find any maxima or minima."
  },
  {
    "objectID": "05_optimization.html#constrained-optimization",
    "href": "05_optimization.html#constrained-optimization",
    "title": "5  Optimization",
    "section": "5.5 Constrained Optimization",
    "text": "5.5 Constrained Optimization\nWe have already looked at optimizing a function in one or more dimensions over the whole domain of the function. Often, however, we want to find the maximum or minimum of a function over some restricted part of its domain.\nex: Maximizing utility subject to a budget constraint\n\n\n\nA typical Utility Function with a Budget Constraint\n\n\nTypes of Constraints: For a function \\(f(x_1, \\dots, x_n)\\), there are two types of constraints that can be imposed:\nIn any constrained optimization problem, the constrained maximum will always be less than or equal to the unconstrained maximum. If the constrained maximum is less than the unconstrained maximum, then the constraint is binding. Essentially, this means that you can treat your constraint as an equality constraint rather than an inequality constraint.\nFor example, the budget constraint binds when you spend your entire budget. This generally happens because we believe that utility is strictly increasing in consumption, i.e. you always want more so you spend everything you have.\nAny number of constraints can be placed on an optimization problem. When working with multiple constraints, always make sure that the set of constraints are not pathological; it must be possible for all of the constraints to be satisfied simultaneously.\n \\[\\max_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2)\\] \\[\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2)\\] This tells us to maximize/minimize our function, \\(f(x_1,x_2)\\), with respect to the choice variables, \\(x_1,x_2\\), subject to the constraint.\nExample: \\[\\max_{x_1,x_2} f(x_1, x_2) = -(x_1^2 + 2x_2^2) \\text{ s.t. }x_1 + x_2 = 4\\] It is easy to see that the maximum occurs at \\((x_1, x_2) = (0,0)\\), but that does not satisfy the constraint. How should we proceed?\n\nEquality Constraints\nEquality constraints are the easiest to deal with because we know that the maximum or minimum has to lie on the (intersection of the) constraint(s).\nThe trick is to change the problem from a constrained optimization problem in \\(n\\) variables to an unconstrained optimization problem in \\(n + k\\) variables, adding one variable for each equality constraint. We do this using a lagrangian multiplier.\nLagrangian function: The Lagrangian function allows us to combine the function we want to optimize and the constraint function into a single function. Once we have this single function, we can proceed as if this were an unconstrained optimization problem.\nFor each constraint, we must include a Lagrange multiplier (\\(\\lambda_i\\)) as an additional variable in the analysis. These terms are the link between the constraint and the Lagrangian function.\nGiven a two dimensional set-up: \\[\\max_{x_1,x_2}/\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2) = a\\]\nWe define the Lagrangian function \\(L(x_1,x_2,\\lambda_1)\\) as follows: \\[L(x_1,x_2,\\lambda_1) = f(x_1,x_2) - \\lambda_1 (c(x_1,x_2) - a)\\]\nMore generally, in n dimensions: \\[ L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k) = f(x_1, \\dots, x_n) - \\sum_{i=1}^k\\lambda_i(c_i(x_1,\\dots, x_n) - r_i)\\]\nGetting the sign right: Note that above we subtract the lagrangian term and we subtract the constraint constant from the constraint function. Occasionally, you may see the following alternative form of the Lagrangian, which is equivalent: \\[ L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k) = f(x_1, \\dots, x_n) + \\sum_{i=1}^k\\lambda_i(r_i - c_i(x_1,\\dots, x_n))\\] Here we add the lagrangian term and we subtract the constraining function from the constraint constant.\nUsing the Lagrangian to Find the Critical Points: To find the critical points, we take the partial derivatives of lagrangian function, \\(L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k)\\), with respect to each of its variables (all choice variables \\(\\mathbf{x}\\) and all lagrangian multipliers \\(\\mathbf{\\lambda}\\)). At a critical point, each of these partial derivatives must be equal to zero, so we obtain a system of \\(n + k\\) equations in \\(n + k\\) unknowns:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial x_1} &= \\frac{\\partial f}{\\partial x_1} - \\sum_{i = 1}^k\\lambda_i\\frac{\\partial c_i}{\\partial x_1} = 0\\\\\n\\vdots &= \\vdots \\nonumber \\\\\n\\frac{\\partial L}{\\partial x_n}  &= \\frac{\\partial f}{\\partial x_n} - \\sum_{i = 1}^k\\lambda_i\\frac{\\partial c_i}{\\partial x_n} = 0\\\\\n\\frac{\\partial L}{\\partial \\lambda_1} &= c_1(x_i, \\dots, x_n) - r_1 =  0\\\\\n\\vdots &= \\vdots \\nonumber \\\\\n\\frac{\\partial L}{\\partial \\lambda_k} &= c_k(x_i, \\dots, x_n) - r_k = 0\n\\end{align*}\\]\nWe can then solve this system of equations, because there are \\(n+k\\) equations and \\(n+k\\) unknowns, to calculate the critical point \\((x_1^*,\\dots,x_n^*,\\lambda_1^*,\\dots,\\lambda_k^*)\\).\nSecond-order Conditions and Unconstrained Optimization: There may be more than one critical point, i.e. we need to verify that the critical point we find is a maximum/minimum. Similar to unconstrained optimization, we can do this by checking the second-order conditions.\n\nExample 5.4 (Constrained optimization with two goods and a budget constraint) Find the constrained optimization of \\[\\max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2) \\text{ s.t. } x_1 + x_2 = 4\\]\n\n\n\nSolution. \nBegin by writing the Lagrangian: \\[L(x_1, x_2, \\lambda) =  -(x_1^2 + 2x_2^2) - \\lambda(x_1 + x_2 - 4)\\]\nTake the partial derivatives and set equal to zero:\n\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial x_1} = -2x_1 - \\lambda \\quad \\quad \\quad &= 0\\\\\n\\frac{\\partial L}{\\partial x_2}  = -4x_2 - \\lambda \\quad \\quad \\quad &= 0\\\\\n\\frac{\\partial L}{\\partial \\lambda} = -(x_1 + x_2 - 4) \\quad & = & 0\\\\\n\\end{align*}\\]\n\nSolve the system of equations: Using the first two partials, we see that \\(\\lambda = -2x_1\\) and \\(\\lambda = -4x_2\\) Set these equal to see that \\(x_1 = 2x_2\\). Using the third partial and the above equality, \\(4 = 2x_2 + x_2\\) from which we get \\[x_2^* = 4/3, x_1^* = 8/3, \\lambda = -16/3\\]\nTherefore, the only critical point is \\(x_1^* = \\frac{8}{3}\\) and \\(x_2^* = \\frac{4}{3}\\)\nThis gives \\(f(\\frac{8}{3}, \\frac{4}{3}) = -\\frac{96}{9}\\), which is less than the unconstrained optimum \\(f(0,0) = 0\\)\n\n\nNotice that when we take the partial derivative of L with respect to the Lagrangian multiplier and set it equal to 0, we return exactly our constraint! This is why signs matter."
  },
  {
    "objectID": "05_optimization.html#inequality-constraints",
    "href": "05_optimization.html#inequality-constraints",
    "title": "5  Optimization",
    "section": "5.6 Inequality Constraints",
    "text": "5.6 Inequality Constraints\nInequality constraints define the boundary of a region over which we seek to optimize the function. This makes inequality constraints more challenging because we do not know if the maximum/minimum lies along one of the constraints (the constraint binds) or in the interior of the region.\nWe must introduce more variables in order to turn the problem into an unconstrained optimization.\nSlack: For each inequality constraint \\(c_i(x_1, \\dots, x_n) \\leq a_i\\), we define a slack variable \\(s_i^2\\) for which the expression \\(c_i(x_1, \\dots, x_n) \\leq a_i - s_i^2\\) would hold with equality. These slack variables capture how close the constraint comes to binding. We use \\(s^2\\) rather than \\(s\\) to ensure that the slack is positive.\nSlack is just a way to transform our constraints.\nGiven a two-dimensional set-up and these edited constraints: \\[\\max_{x_1,x_2}/\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2) \\le a_1\\]\nAdding in Slack: \\[\\max_{x_1,x_2}/\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2) \\le a_1 - s_1^2\\]\nWe define the Lagrangian function \\(L(x_1,x_2,\\lambda_1,s_1)\\) as follows: \\[L(x_1,x_2,\\lambda_1,s_1) = f(x_1,x_2) - \\lambda_1 ( c(x_1,x_2) + s_1^2 - a_1)\\]\nMore generally, in n dimensions: \\[ L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k, s_1, \\dots, s_k) = f(x_1, \\dots, x_n) - \\sum_{i = 1}^k \\lambda_i(c_i(x_1,\\dots, x_n) + s_i^2 - a_i)\\]\nFinding the Critical Points: To find the critical points, we take the partial derivatives of the lagrangian function, \\(L(x_1,\\dots,x_n,\\lambda_1,\\dots,\\lambda_k,s_1,\\dots,s_k)\\), with respect to each of its variables (all choice variables \\(x\\), all lagrangian multipliers \\(\\lambda\\), and all slack variables \\(s\\)). At a critical point, each of these partial derivatives must be equal to zero, so we obtain a system of \\(n + 2k\\) equations in \\(n + 2k\\) unknowns:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial x_1} &= \\frac{\\partial f}{\\partial x_1} - \\sum_{i = 1}^k\\lambda_i\\frac{\\partial c_i}{\\partial x_1} = 0\\\\\n\\vdots & =  \\vdots  \\\\\n\\frac{\\partial L}{\\partial x_n}  &= \\frac{\\partial f}{\\partial x_n} - \\sum_{i = 1}^k\\lambda_i\\frac{\\partial c_i}{\\partial x_n} = 0\\\\\n\\frac{\\partial L}{\\partial \\lambda_1} &= c_1(x_i, \\dots, x_n) + s_1^2 - b_1 = 0\\\\\n\\vdots & = \\vdots \\\\\n\\frac{\\partial L}{\\partial \\lambda_k} &= c_k(x_i, \\dots, x_n) + s_k^2 - b_k = 0\\\\\n\\frac{\\partial L}{\\partial s_1} &= 2s_1\\lambda_1 = 0\\\\\n\\vdots =\\vdots \\\\\n\\frac{\\partial L}{\\partial s_k} &= 2s_k\\lambda_k = 0\n\\end{align*}\\]\nComplementary slackness conditions: The last set of first order conditions of the form \\(2s_i\\lambda_i = 0\\) (the partials taken with respect to the slack variables) are known as complementary slackness conditions. These conditions can be satisfied one of three ways:\n\n\\(\\lambda_i = 0\\) and \\(s_i \\neq 0\\): This implies that the slack is positive and thus the constraint does not bind.\n\\(\\lambda_i \\neq 0\\) and \\(s_i = 0\\): This implies that there is no slack in the constraint and the constraint does bind.\n\\(\\lambda_i = 0\\) and \\(s_i = 0\\): In this case, there is no slack but the constraint binds trivially, without changing the optimum.\n\nExample: Find the critical points for the following constrained optimization: \\[\\max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2) \\text{ s.t. } x_1 + x_2 \\le 4\\]\n\nRewrite with the slack variables: \\[\\max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2) \\text{ s.t. } x_1 + x_2 \\le 4 - s_1^2\\]\nWrite the Lagrangian: \\[L(x_1,x_2,\\lambda_1,s_1) = -(x_1^2 + 2x_2^2) - \\lambda_1 (x_1 + x_2 + s_1^2 - 4)\\]\nTake the partial derivatives and set equal to 0:\n\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial x_1} = -2x_1 - \\lambda_1  &= 0\\\\\n\\frac{\\partial L}{\\partial x_2}  = -4x_2 - \\lambda_1 &=  0\\\\\n\\frac{\\partial L}{\\partial \\lambda_1} = -(x_1 + x_2 + s_1^2 - 4)&= 0\\\\\n\\frac{\\partial L}{\\partial s_1} = -2s_1\\lambda_1 &= 0\\\\\n\\end{align*}\\]\n\nConsider all ways that the complementary slackness conditions are solved:\n\nThis shows that there are two critical points: \\((0,0)\\) and \\((\\frac{8}{3},\\frac{4}{3})\\).\n\nFind maximum: Looking at the values of \\(f(x_1,x_2)\\) at the critical points, we see that \\(f(x_1,x_2)\\) is maximized at \\(x_1^* = 0\\) and \\(x_2^*=0\\).\n\n\nExercise 5.3 (Constrained optimization) Example: Find the critical points for the following constrained optimization:\n\\[\\max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2) \\text{ s.t. }\n\\begin{array}{l}\nx_1 + x_2 \\le 4\\\\\nx_1 \\ge 0\\\\\nx_2 \\ge 0\n\\end{array}\\]\n\n\nRewrite with the slack variables: \\[\\phantom{max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2) \\text{ s.t. }\n\\begin{array}{l}\nx_1 + x_2 \\le 4 - s_1^2\\\\\n-x_1 \\le 0 - s_2^2\\\\\n-x_2 \\le 0 - s_3^2\n\\end{array}}\\]\nWrite the Lagrangian: \\[\\phantom{L(x_1, x_2, \\lambda_1, \\lambda_2, \\lambda_3, s_1, s_2, s_3) =  -(x_1^2 + 2x_2^2) - \\lambda_1(x_1 + x_2 + s_1^2  - 4) - \\lambda_2(-x_1 + s_2^2) - \\lambda_3(-x_2 + s_3^2)}\\]\nTake the partial derivatives and set equal to zero:\n\n\n\nConsider all ways that the complementary slackness conditions are solved:\n\n\nFind maximum:"
  },
  {
    "objectID": "05_optimization.html#kuhn-tucker-conditions",
    "href": "05_optimization.html#kuhn-tucker-conditions",
    "title": "5  Optimization",
    "section": "5.7 Kuhn-Tucker Conditions",
    "text": "5.7 Kuhn-Tucker Conditions\nAs you can see, this can be a pain. When dealing explicitly with non-negativity constraints, this process is simplified by using the Kuhn-Tucker method.\nBecause the problem of maximizing a function subject to inequality and non-negativity constraints arises frequently in economics, the Kuhn-Tucker conditions provides a method that often makes it easier to both calculate the critical points and identify points that are (local) maxima.\nGiven a two-dimensional set-up: \\[\\max_{x_1,x_2}/\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. }\n\\begin{array}{l}\nc(x_1,x_2) \\le a_1\\\\\nx_1 \\ge 0 \\\\\ngx_2 \\ge 0\n\\end{array}\\]\nWe define the Lagrangian function \\(L(x_1,x_2,\\lambda_1)\\) the same as if we did not have the non-negativity constraints: \\[L(x_1,x_2,\\lambda_2) = f(x_1,x_2) - \\lambda_1(c(x_1,x_2) - a_1)\\]\nMore generally, in n dimensions: \\[ L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k) = f(x_1, \\dots, x_n) - \\sum_{i=1}^k\\lambda_i(c_i(x_1,\\dots, x_n) - a_i)\\]\nKuhn-Tucker and Complementary Slackness Conditions: To find the critical points, we first calculate the Kuhn-Tucker conditions by taking the partial derivatives of the lagrangian function, \\(L(x_1,\\dots,x_n,\\lambda_1,\\dots,\\lambda_k)\\), with respect to each of its variables (all choice variables \\(x\\) and all lagrangian multipliers \\(\\lambda\\)) and we calculate the complementary slackness conditions by multiplying each partial derivative by its respective variable and include non-negativity conditions for all variables (choice variables \\(x\\) and lagrangian multipliers \\(\\lambda\\)).\nKuhn-Tucker Conditions\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial x_1} \\leq 0, & \\dots, \\frac{\\partial L}{\\partial x_n} \\leq 0\\\\\n\\frac{\\partial L}{\\partial \\lambda_1} \\geq 0, & \\dots, \\frac{\\partial L}{\\partial \\lambda_m} \\geq 0\n\\end{align*}\\]\nComplementary Slackness Conditions\n\\[\\begin{align*}\nx_1\\frac{\\partial L}{\\partial x_1} = 0, & \\dots, x_n\\frac{\\partial L}{\\partial x_n} = 0\\\\\n\\lambda_1\\frac{\\partial L}{\\partial \\lambda_1} = 0, & \\dots, \\lambda_m \\frac{\\partial L}{\\partial \\lambda_m} = 0\n\\end{align*}\\]\nNon-negativity Conditions \\[\\begin{eqnarray*}\nx_1 \\geq 0 & \\dots & x_n \\geq 0\\\\\n\\lambda_1 \\geq 0 & \\dots & \\lambda_m \\geq 0\n\\end{eqnarray*}\\]\nNote that some of these conditions are set equal to 0, while others are set as inequalities!\nNote also that to minimize the function \\(f(x_1, \\dots, x_n)\\), the simplest thing to do is maximize the function \\(-f(x_1, \\dots, x_n)\\); all of the conditions remain the same after reformulating as a maximization problem.\nThere are additional assumptions (notably, f(x) is quasi-concave and the constraints are convex) that are sufficient to ensure that a point satisfying the Kuhn-Tucker conditions is a global max; if these assumptions do not hold, you may have to check more than one point.\nFinding the Critical Points with Kuhn-Tucker Conditions: Given the above conditions, to find the critical points we solve the above system of equations. To do so, we must check border and interior solutions to see if they satisfy the above conditions.\nIn a two-dimensional set-up, this means we must check the following cases:\n\n\\(x_1 = 0, x_2 = 0\\) Border Solution\n\\(x_1 = 0, x_2 \\neq 0\\) Border Solution\n\\(x_1 \\neq 0, x_2 = 0\\) Border Solution\n\\(x_1 \\neq 0, x_2 \\neq 0\\) Interior Solution\n\n\nExample 5.5 (Kuhn-Tucker with two variables) Solve the following optimization problem with inequality constraints \\[\\max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2)\\]\n\\[\\begin{align*}\n\\text{ s.t. }\n\\begin{cases}\n&x_1 + x_2 *\\le 4\\\\\n&x_1 *\\ge 0\\\\\n&x_2 *\\ge 0\n\\end{cases}\n\\end{align*}\\]\n\n\nWrite the Lagrangian: \\[L(x_1, x_2, \\lambda) =  -(x_1^2 + 2x_2^2) - \\lambda(x_1 + x_2 - 4)\\]\nFind the First Order Conditions:\n\nKuhn-Tucker Conditions \\[\\begin{align*}\n\\frac{\\partial L}{\\partial x_1} = -2x_1 - \\lambda  &\\leq 0\\\\\n\\frac{\\partial L}{\\partial x_2}  = -4x_2 - \\lambda & \\leq  0\\\\\n\\frac{\\partial L}{\\partial \\lambda} = -(x_1 + x_2 - 4)& \\geq 0\n\\end{align*}\\]\nComplementary Slackness Conditions \\[\\begin{align*}\nx_1\\frac{\\partial L}{\\partial x_2} = x_1(-2x_1 - \\lambda)  &= 0\\\\\nx_2\\frac{\\partial L}{\\partial x_2} = x_2(-4x_2 - \\lambda)  &= 0\\\\\n\\lambda\\frac{\\partial L}{\\partial \\lambda} = -\\lambda(x_1 + x_2 - 4)&= 0\n\\end{align*}\\]\nNon-negativity Conditions \\[\\begin{align*}\nx_1 & \\geq  0\\\\\nx_2 & \\geq 0\\\\\n\\lambda & \\geq 0\n\\end{align*}\\]\n\nConsider all border and interior cases:\nFind Maximum: Three of the critical points violate the requirement that \\(\\lambda \\geq 0\\), so the point \\((0,0,0)\\) is the maximum.\n\n\nExercise 5.4 (Kuhn-Tucker with logs) \\[\\max_{x_1,x_2} f(x) = \\frac{1}{3}\\log (x_1 + 1) + \\frac{2}{3}\\log (x_2 + 1) \\text{ s.t. }  \n\\begin{array}{l}\nx_1 + 2x_2 \\leq 4\\\\\n     x_1 \\geq 0\\\\\n    x_2 \\geq 0\n\\end{array}\\]"
  },
  {
    "objectID": "05_optimization.html#applications-of-quadratic-forms",
    "href": "05_optimization.html#applications-of-quadratic-forms",
    "title": "5  Optimization",
    "section": "5.8 Applications of Quadratic Forms",
    "text": "5.8 Applications of Quadratic Forms\nCurvature and The Taylor Polynomial as a Quadratic Form: The Hessian is used in a Taylor polynomial approximation to \\(f(\\mathbf{x})\\) and provides information about the curvature of \\(f({\\mathbf x})\\) at \\(\\mathbf{x}\\) — e.g., which tells us whether a critical point \\(\\mathbf{x}^*\\) is a min, max, or saddle point.\n\nThe second order Taylor polynomial about the critical point \\({\\mathbf x}^*\\) is \\[f({\\mathbf x}^*+\\mathbf h)=f({\\mathbf x}^*)+\\nabla f({\\mathbf x}^*) \\mathbf h +\\frac{1}{2} \\mathbf h^\\top\n{\\mathbf H(x^*)} \\mathbf h + R(\\mathbf h)\\]\nSince we’re looking at a critical point, \\(\\nabla f({\\mathbf x}^*)=0\\); and for small \\(\\mathbf h\\), \\(R(\\mathbf h)\\) is negligible. Rearranging, we get \\[f({\\mathbf x}^*+\\mathbf h)-f({\\mathbf x}^*)\\approx \\frac{1}{2} \\mathbf h^\\top {\\mathbf H(x^*)}\n\\mathbf h \\]\nThe Righthand side here is a quadratic form and we can determine the definiteness of \\(\\mathbf H(x^*)\\)."
  },
  {
    "objectID": "06_probability.html",
    "href": "06_probability.html",
    "title": "6  Probability Theory",
    "section": "",
    "text": "Probability and Inferences are mirror images of each other, and both are integral to social science. Probability quantifies uncertainty, which is important because many things in the social world are at first uncertain. Inference is then the study of how to learn about facts you don’t observe from facts you do observe."
  },
  {
    "objectID": "06_probability.html#counting-rules",
    "href": "06_probability.html#counting-rules",
    "title": "6  Probability Theory",
    "section": "6.1 Counting rules",
    "text": "6.1 Counting rules\nProbability in high school is usually really about combinatorics: the probability of event A is the number of ways in which A can occur divided by the number of all other possibilities. This is a very simplified version of probability, which we can call the “counting definition of probability”, essentially because each possible event to count is often equally likely and discrete. But it is still good to review the underlying rules here.\nFundamental Theorem of Counting: If an object has \\(j\\) different characteristics that are independent of each other, and each characteristic \\(i\\) has \\(n_i\\) ways of being expressed, then there are \\(\\prod_{i = 1}^j n_i\\) possible unique objects.\n\nExample 6.1 (Counting Possibilities) Suppose we are given a stack of cards. Cards can be either red or black and can take on any of 13 values. There is only one of each color-number combination. In this case,\n\n\\(j =\\)\n\\(n_{\\text{color}} =\\)\n\\(n_{\\text{number}} =\\)\nNumber of Outcomes \\(=\\)\n\n\nWe often need to count the number of ways to choose a subset from some set of possibilities. The number of outcomes depends on two characteristics of the process: does the order matter and is replacement allowed?\nIt is useful to think of any problem concretely, e.g. through a sampling table: If there are \\(n\\) objects which are numbered 1 to \\(n\\) and we select \\(k < n\\) of them, how many different outcomes are possible?\nIf the order in which a given object is selected matters, selecting 4 numbered objects in the following order (1, 3, 7, 2) and selecting the same four objects but in a different order such as (7, 2, 1, 3) will be counted as different outcomes.\nIf replacement is allowed, there are always the same \\(n\\) objects to select from. However, if replacement is not allowed, there is always one less option than the previous round when making a selection. For example, if replacement is not allowed and I am selecting 3 elements from the following set {1, 2, 3, 4, 5, 6}, I will have 6 options at first, 5 options as I make my second selection, and 4 options as I make my third.\n\nSo if order matters AND we are sampling with replacement, the number of different outcomes is \\(n^k\\).\nIf order matters AND we are sampling without replacement, the number of different outcomes is \\(n(n-1)(n-2)...(n-k+1)=\\frac{n!}{(n-k)!}\\).\nIf order doesn’t matter AND we are sampling without replacement, the number of different outcomes is \\(\\binom{n}{k} = \\frac{n!}{(n-k)!k!}\\).\n\nExpression \\(\\binom{n}{k}\\) is read as “n choose k” and denotes \\(\\frac{n!}{(n-k)!k!}\\). Also, note that \\(0! = 1\\).\n\nExample 6.2 (Counting) There are five balls numbered from 1 through 5 in a jar. Three balls are chosen. How many possible choices are there?\n\nOrdered, with replacement \\(=\\)\nOrdered, without replacement \\(=\\)\nUnordered, without replacement \\(=\\)\n\n\n\nExercise 6.1 (Counting) Four cards are selected from a deck of 52 cards. Once a card has been drawn, it is not reshuffled back into the deck. Moreover, we care only about the complete hand that we get (i.e. we care about the set of selected cards, not the sequence in which it was drawn). How many possible outcomes are there?"
  },
  {
    "objectID": "06_probability.html#sec-probdef",
    "href": "06_probability.html#sec-probdef",
    "title": "6  Probability Theory",
    "section": "6.2 Probability",
    "text": "6.2 Probability\n\nProbability Definitions: Formal and Informal\nMany things in the world are uncertain. In everyday speech, we say that we are uncertain about the outcome of random events. Probability is a formal model of uncertainty which provides a measure of uncertainty governed by a particular set of rules. A different model of uncertainty would, of course, have a set of rules different from anything we discuss here. Our focus on probability is justified because it has proven to be a particularly useful model of uncertainty.\nSample Space (S): A set or collection of all possible outcomes from some process. Outcomes in the set can be discrete elements (countable) or points along a continuous interval (uncountable).\nProbability Distribution Function: a mapping of each event in the sample space \\(S\\) to the real numbers that satisfy the following three axioms (also called Kolmogorov’s Axioms).\nFormally,\n\nDefinition 6.1 (Probability) Probability is a function that maps events from a sample space to a real number, obeying the axioms of probability.\n\nThe axioms of probability make sure that the separate events add up in terms of probability, and – for standardization purposes – that they add up to 1.\n\n\nDefinition 6.2 (Axioms of Probability) \nFor any event \\(A\\), \\(P(A)\\ge 0\\).\n\\(P(S)=1\\)\nThe Countable Additivity Axiom: For any sequence of disjoint (mutually exclusive) events \\(A_1,A_2,\\ldots\\) (of which there may be infinitely many), \\[P\\left( \\bigcup\\limits_{i=1}^k\nA_i\\right)=\\sum\\limits_{i=1}^k P(A_i)\\]\n\nThe last axiom is an extension of a union to infinite sets. When there are only two events in the space, it boils down to:\n\\[\\begin{align*}\nP(A_1 \\cup A_2) = P(A_1) + P(A_2) \\quad\\text{for disjoint } A_1, A_2\n\\end{align*}\\]\n\n\n\nProbability Operations\nUsing these three axioms, we can define all of the common rules of probability.\n\n\\(P(\\emptyset)=0\\)\nFor any event \\(A\\), \\(0\\le P(A) \\le 1\\).\n\\(P({A}^C)=1-P(A)\\)\nIf \\(A\\subset B\\) (\\(A\\) is a subset of \\(B\\)), then \\(P(A)\\le P(B)\\).\nFor any two events \\(A\\) and \\(B\\), \\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\)\nBoole’s Inequality: For any sequence of \\(n\\) events (which need not be disjoint) \\(A_1,A_2,\\ldots,A_n\\), then \\(P\\left( \\bigcup\\limits_{i=1}^n A_i\\right) \\leq \\sum\\limits_{i=1}^n P(A_i)\\).\n\n\nExample 6.3 (Probability) Assume we have an evenly-balanced, six-sided die.\nThen,\n\nSample space S =\n\\(P(1)=\\cdots=P(6)=\\)\n\\(P(\\emptyset)=P(7)=\\)\n\\(P\\left( \\{ 1, 3, 5 \\} \\right)=\\)\n\\(P\\left( \\{ 1, 2 \\}^C \\right)= P\\left( \\{ 3, 4, 5, 6 \\}\\right)=\\)\nLet \\(A=\\{ 1,2,3,4,5 \\}\\subset S\\). Then \\(P(A)=5/6<P(S)=\\)\nLet \\(A=\\{ 1, 2, 3 \\}\\) and \\(B=\\{ 2, 4, 6 \\}\\). Then \\(A\\cup B\\)? \\(A\\cap B\\)? \\(P(A \\cup B)\\)?\n\n\n\nExercise 6.2 (Probability) Suppose you had a pair of four-sided dice. You sum the results from a single toss. Let us call this sum, or the outcome, X.\n\nWhat is \\(P(X = 5)\\), \\(P(X = 3)\\), \\(P(X = 6)\\)?\nWhat is \\(P(X=5 \\cup X = 3)^C\\)?"
  },
  {
    "objectID": "06_probability.html#conditional-probability-and-bayes-rule",
    "href": "06_probability.html#conditional-probability-and-bayes-rule",
    "title": "6  Probability Theory",
    "section": "6.3 Conditional Probability and Bayes Rule",
    "text": "6.3 Conditional Probability and Bayes Rule\nConditional Probability: The conditional probability \\(P(A|B)\\) of an event \\(A\\) is the probability of \\(A\\), given that another event \\(B\\) has occurred. Conditional probability allows for the inclusion of other information into the calculation of the probability of an event. It is calculated as\n\\[P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\]\nNote that conditional probabilities are probabilities and must also follow the Kolmagorov axioms of probability.\n\nExample 6.4 (Conditional Probability 1) Assume \\(A\\) and \\(B\\) occur with the following frequencies: \\(\\quad\\)\n\n\n\n\n\\(A\\)\n\\(A^c\\)\n\n\n\n\n\\(B\\)\n\\(n_{ab}\\)\n\\(n_{a^cb}\\)\n\n\n\\(B^C\\)\n\\(n_{ab^c}\\)\n\\(n_{(ab)^c}\\)\n\n\n\nand let \\(n_{ab}+n_{a^Cb}+n_{ab^C}+n_{(ab)^C}=N\\). Then\n\n\\(P(A)=\\)\n\\(P(B)=\\)\n\\(P(A\\cap B)=\\)\n\\(P(A|B)= \\frac{P(A\\cap B)}{P(B)}=\\)\n\\(P(B|A)= \\frac{P(A\\cap B)}{P(A)}=\\)\n\n\n\nExample 6.5 (Conditional Probability 2) A six-sided die is rolled. What is the probability of a 1, given the outcome is an odd number?\n\nYou could rearrange the fraction to highlight how a joint probability could be expressed as the product of a conditional probability.\n\nDefinition 6.3 (Multiplicative Law of Probability) The probability of the intersection of two events \\(A\\) and \\(B\\) is \\(P(A\\cap B)=P(A)P(B|A)=P(B)P(A|B)\\) which follows directly from the definition of conditional probability. More generally,\n\\[P(A_1\\cap \\cdots\\cap A_k) = P(A_k| A_{k-1}\\cap \\cdots \\cap A_1)\\times P(A_{k-1}|A_{k-2}\\cap \\cdots A_1) \\times \\ldots \\times P(A_2|A_1)\\times P(A_1)\\]\nSometimes it is easier to calculate these conditional probabilities and sum them than it is to calculate \\(P(A)\\) directly.\n\n\nDefinition 6.4 (Law of total probability) Let \\(S\\) be the sample space of some experiment and let the disjoint \\(k\\) events \\(B_1,\\ldots,B_k\\) partition \\(S\\), such that \\(P(B_1\\cup ... \\cup B_k) = P(S) = 1\\). If \\(A\\) is some other event in \\(S\\), then the events \\(A\\cap B_1, A\\cap B_2, \\ldots, A\\cap B_k\\) will form a partition of \\(A\\) and we can write \\(A\\) as \\[A=(A\\cap B_1)\\cup\\cdots\\cup (A\\cap B_k)\\].\nSince the \\(k\\) events are disjoint,\n\\[\\begin{eqnarray*}\nP(A)&=&\\sum\\limits_{i=1}^k P(A \\cap B_i)\\\\\n      &=&\\sum\\limits_{i=1}^k P(B_i)P(A|B_i)\n\\end{eqnarray*}\\]\n\nBayes Rule: Assume that events \\(B_1,\\ldots,B_k\\) form a partition of the space \\(S\\). Then by the Law of Total Probability\n\\[P(B_j|A)= \\frac{P(A \\cap B_j)} {P(A)} = \\frac{P(B_j) P(A|B_j)}{\\sum\\limits_{i=1}^k P(B_i)P(A|B_i)}\\]\nIf there are only two states of \\(B\\), then this is just \\[P(B_1|A)=\\frac{P(B_1)P(A|B_1)} {P(B_1)P(A|B_1)+P(B_2)P(A|B_2)}\\]\nBayes’ rule determines the posterior probability of a state \\(P(B_j|A)\\) by calculating the probability \\(P(A \\cap B_j)\\) that both the event \\(A\\) and the state \\(B_j\\) will occur and dividing it by the probability that the event will occur regardless of the state (by summing across all \\(B_i\\)). The states could be something like Normal/Defective, Healthy/Diseased, Republican/Democrat/Independent, etc. The event on which one conditions could be something like a sampling from a batch of components, a test for a disease, or a question about a policy position.\nPrior and Posterior Probabilities: Above, \\(P(B_1)\\) is often called the prior probability, since it’s the probability of \\(B_1\\) before anything else is known. \\(P(B_1|A)\\) is called the posterior probability, since it’s the probability after other information is taken into account.\n\nExample 6.6 (Bayes’ Rule) In a given town, 40% of the voters are Democrat and 60% are Republican. The president’s budget is supported by 50% of the Democrats and 90% of the Republicans. If a randomly (equally likely) selected voter is found to support the president’s budget, what is the probability that they are a Democrat?\n\n\nExercise 6.3 (Conditional Probability) Assume that 2% of the population of the U.S. are members of some extremist militia group. We develop a survey that positively classifies someone as being a member of a militia group given that they are a member 95% of the time and negatively classifies someone as not being a member of a militia group given that they are not a member 97% of the time. What is the probability that someone positively classified as being a member of a militia group is actually a militia member?"
  },
  {
    "objectID": "06_probability.html#independence",
    "href": "06_probability.html#independence",
    "title": "6  Probability Theory",
    "section": "6.4 Independence",
    "text": "6.4 Independence\n\nDefinition 6.5 (Independence) If the occurrence or nonoccurrence of either events \\(A\\) and \\(B\\) provides no information about the occurrence or nonoccurrence of the other, then \\(A\\) and \\(B\\) are independent.\n\nIf \\(A\\) and \\(B\\) are independent, then\n\n\\(P(A|B)=P(A)\\)\n\\(P(B|A)=P(B)\\)\n\\(P(A\\cap B)=P(A)P(B)\\)\nMore generally than the above, \\(P(\\bigcap_{i=1}^k A_i) = \\prod_{i = 1}^K P(A_i)\\)\n\nAre mutually exclusive events independent of each other?\nNo. If A and B are mutually exclusive, then they cannot happen simultaneously. If we know that A occurred, then we know that B couldn’t have occurred. Because of this, A and B aren’t independent.\nPairwise Independence: A set of more than two events \\(A_1, A_2, \\dots, A_k\\) is pairwise independent if \\(P(A_i\\cap A_j)=P(A_i)P(A_j)\\), \\(\\forall i\\neq j\\). Note that this does not necessarily imply joint independence.\nConditional Independence: If \\(A\\) and \\(B\\) are independent once you know the occurrence of a third event \\(C\\), then \\(A\\) and \\(B\\) are conditionally independent (conditional on \\(C\\)):\n\n\\(P(A|B \\cap C)=P(A|C)\\)\n\\(P(B|A \\cap C)=P(B|C)\\)\n\\(P(A\\cap B|C)=P(A|C)P(B|C)\\)\n\nJust because two events are conditionally independent does not mean that they are independent. Actually it is hard to think of real-world things that are “unconditionally” independent. That’s why it’s always important to ask about a finding: What was it conditioned on? For example, suppose that a graduate school admission decisions are done by only one professor, who picks a group of 50 bright students and flips a coin for each student to generate a class of about 25 students. Then the the probability that two students get accepted are conditionally independent, because they are determined by two separate coin tosses. However, this does not mean that their admittance is not completely independent. Knowing that student \\(A\\) got in gives us information about whether student \\(B\\) got in, if we think that the professor originally picked her pool of 50 students by merit.\nPerhaps more counter-intuitively: If two events are already independent, then it might seem that no amount of “conditioning” will make them dependent. But this is not always so. For example, imagine that you own a house with a lawn (a very extreme hypothetical!) Let \\(A\\) be the event that it rained yesterday and \\(B\\) the event that your sprinkler system went off yesterday. Suppose that your sprinkler system is set to randomly go off and so \\(A\\) and \\(B\\) are independent of one another. \\(P(A \\mid B) = P(A).\\) But now let \\(C\\) be the event that the grass is wet. The grass can be wet either due to the rain or due to the sprinkler. For conditional independence to hold here, then \\(P(A \\mid C)\\) must be equal to \\(P(A \\mid B \\cap C).\\) But this is not true.\nLet \\(P(A) = .5\\) and \\(P(B) = .5\\).\nThe marginal probability \\(P(C)\\) is\n\\[P(C) = P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = .75\\] The conditional probability \\(P(A | C)\\) is\n\\[P(A|C) = \\frac{P(C|A)P(A)}{P(C)} = \\frac{1 \\times .5}{.75} = \\frac{2}{3}\\]\nThe conditional probability \\(P(A | B \\cap C)\\) is\n\\[P(A | B \\cap C) = \\frac{P(C \\cap B| A) P(A)}{P(C \\cap B)} = \\frac{P(C \\cap B| A) P(A)}{P(C|B)P(B)}  \\frac{.5\\times .5}{.5} = \\frac{1}{2}\\] Intuitively, given that the grass is wet, knowing that it rained yesterday tells us that it is less likely that the sprinkler also went off!"
  },
  {
    "objectID": "06_probability.html#random-variables",
    "href": "06_probability.html#random-variables",
    "title": "6  Probability Theory",
    "section": "6.5 Random Variables",
    "text": "6.5 Random Variables\nMost questions in the social sciences involve events, rather than numbers per se. To analyze and reason about events quantitatively, we need a way of mapping events to numbers. A random variable does exactly that.\n\n\n\n\n\nFigure 6.1: The Random Variable as a Real-Valued Function\n\n\n\n\n\nDefinition 6.6 (Random Variable) A random variable is a measurable function \\(X\\) that maps from the sample space \\(S\\) to the set of real numbers \\(R.\\) It assigns a real number to every outcome \\(s \\in S\\).\n\nFigure 6.1 shows a image of the function. It might seem strange to define a random variable as a function – which is neither random nor variable. The randomness comes from the realization of an event from the sample space \\(s\\).\nRandomness means that the outcome of some experiment is not deterministic, i.e. there is some probability (\\(0 < P(A) < 1\\)) that the event will occur.\nThe support of a random variable is all values for which there is a positive probability of occurrence.\nExample: Flip a fair coin two times. What is the sample space?\nA random variable must map events to the real line. For example, let a random variable \\(X\\) be the number of heads. The event \\((H, H)\\) gets mapped to 2 \\(X(s) = 2\\), and the events \\(\\{(H, T), (T, H)\\}\\) gets mapped to 1 \\((X(s) = 1)\\), the event \\((T, T)\\) gets mapped to 0 \\((X(s) = 0)\\).\nWhat are other possible random variables?"
  },
  {
    "objectID": "06_probability.html#distributions",
    "href": "06_probability.html#distributions",
    "title": "6  Probability Theory",
    "section": "6.6 Distributions",
    "text": "6.6 Distributions\nWe now have two main concepts in this section – probability and random variables. Given a sample space \\(S\\) and the same experiment, both probability and random variables take events as their inputs. But they output different things (probabilities measure the “size” of events, random variables give a number in a way that the analyst chose to define the random variable). How do the two concepts relate?\nThe concept of distributions is the natural bridge between these two concepts.\n\nDefinition 6.7 (Distribution of a random variable) A distribution of a random variable is a function that specifies the probabilities of all events associated with that random variable. There are several types of distributions: A probability mass function for a discrete random variable and probability density function for a continuous random variable.\n\nNotice how the definition of distributions combines two ideas of random variables and probabilities of events. First, the distribution considers a random variable, call it \\(X\\). \\(X\\) can take a number of possible numeric values.\n\nExample 6.7 (Total Number of Occurrences) Consider three binary outcomes, one for each patient recovering from a disease: \\(R_i\\) denotes the event in which patient \\(i\\) (\\(i = 1, 2, 3\\)) recovers from a disease. \\(R_1\\), \\(R_2\\), and \\(R_3\\). How would we represent the total number of people who end up recovering from the disease?\n\n\nSolution. Define the random variable \\(X\\) be the total number of people (out of three) who recover from the disease. Random variables are functions, that take as an input a set of events (in the sample space \\(S\\)) and deterministically assigns them to a number of the analyst’s choice.\n\nRecall that with each of these numerical values there is a class of events. In the previous example, for \\(X = 3\\) there is one outcome (\\(R_1, R_2, R_3\\)) and for \\(X = 1\\) there are multiple (\\(\\{(R_1, R_2^c, R_3^c), (R_1^c, R_2, R_3^c), (R_1^c, R_2^c, R_3), \\}\\)). Now, the thing to notice here is that each of these events naturally come with a probability associated with them. That is, \\(P(R_1, R_2, R_3)\\) is a number from 0 to 1, as is \\(P(R_1, R_2^c, R_3^c)\\). These all have probabilities because they are in the sample space \\(S\\). The function that tells us these probabilities that are associated with a numerical value of a random variable is called a distribution.\nIn other words, a random variable \\(X\\) induces a probability distribution \\(P\\) (sometimes written \\(P_X\\) to emphasize that the probability density is about the r.v. \\(X\\))\n\nDiscrete Random Variables\nThe formal definition of a random variable is easier to given by separating out two cases: discrete random variables when the numeric summaries of the events are discrete, and continuous random variables when they are continuous.\n\nDefinition 6.8 (Discrete Random Variable) \\(X\\) is a discrete random variable if it can assume only a finite or countably infinite number of distinct values. Examples: number of wars per year, heads or tails.\n\nThe distribution of a discrete r.v. is a PMF:\n\nDefinition 6.9 (Probability Mass Function) For a discrete random variable \\(X\\), the probability mass function (Also referred to simply as the “probability distribution.”) (PMF), \\(p(x)=P(X=x)\\), assigns probabilities to a countable number of distinct \\(x\\) values such that\n\n\\(0\\le p(x)\\le 1\\)\n\\(\\sum\\limits_y p(x)=1\\)\n\n\nExample: For a fair six-sided die, there is an equal probability of rolling any number. Since there are six sides, the probability mass function is then \\(p(y)=1/6\\) for \\(y=1,\\ldots,6\\), 0 otherwise.}\nIn a discrete random variable, cumulative distribution function , \\(F(x)\\) or \\(P(X\\le x)\\), is the probability that \\(X\\) is less than or equal to some value \\(x\\), or \\[P(X\\le x)=\\sum\\limits_{i\\le x} p(i)\\]\nProperties a CDF must satisfy:\n\n\\(F(x)\\) is non-decreasing in \\(x\\).\n\\(\\lim\\limits_{x \\to -\\infty} F(x) = 0\\) and \\(\\lim\\limits_{x \\to \\infty} F(x) = 1\\)\n\\(F(x)\\) is right-continuous.\n\nNote that \\(P(X > x) = 1 - P(X \\le x)\\).\n\nDefinition 6.10 For a fair six-sided die with its value as \\(Y\\), What are the following?\n\n\\(P(Y\\le 1)\\)\n\\(P(Y\\le 3)\\)\n\\(P(Y\\le 6)\\)\n\n\n\n\nContinuous Random Variables\nWe also have a similar definition for continuous random variables.\n\nDefinition 6.11 (Continuous Random Variable) \\(X\\) is a continuous random variable if there exists a nonnegative function \\(f(x)\\) defined for all real \\(x\\in (-\\infty,\\infty)\\), such that for any interval \\(A\\), \\(P(X\\in A)=\\int\\limits_A f(x)dx\\). Examples: age, income, GNP, temperature.\n\n\nDefinition 6.12 (Probability density function) The function \\(f\\) above is called the probability density function (pdf) of \\(X\\) and must satisfy \\[f(x)\\ge 0\\] \\[\\int\\limits_{-\\infty}^\\infty f(x)dx=1\\]\nNote also that \\(P(X = x)=0\\) — i.e., the probability of any point \\(y\\) is zero.\n\nWhile continuous random variables do not have a PMF (since the PMF would be \\(0\\) at every point), the cumulative distribution function is defined in the exact same way. The cumulative distribution gives the probability that \\(Y\\) lies on the interval \\((-\\infty,y)\\) and is defined as \\[F(x)=P(X\\le x)=\\int\\limits_{-\\infty}^x f(s)ds\\]\nWe can also make statements about the probability of \\(Y\\) falling in an interval \\(a\\le y\\le b\\). \\[P(a\\le x\\le b)=\\int\\limits_a^b f(x)dx\\]\nThe PDF and CDF are linked by the integral: The CDF of the integral of the PDF: \\[f(x) = F'(x)=\\frac{dF(x)}{dx}\\]\n\nExample 6.8 (Continuous R.V.) For \\(f(y)=1, \\quad 0<y<1\\), find: (1) The CDF \\(F(y)\\) and (2) The probability \\(P(0.5<y<0.75)\\)."
  },
  {
    "objectID": "06_probability.html#joint-distributions",
    "href": "06_probability.html#joint-distributions",
    "title": "6  Probability Theory",
    "section": "6.7 Joint Distributions",
    "text": "6.7 Joint Distributions\nOften, we are interested in two or more random variables defined on the same sample space. The distribution of these variables is called a joint distribution. Joint distributions can be made up of any combination of discrete and continuous random variables.\nJoint Probability Distribution: If both \\(X\\) and \\(Y\\) are random variable, their joint probability mass/density function assigns probabilities to each pair of outcomes\nDiscrete:\n\\[p(x, y) = P(X = x, Y = y)\\]\nsuch that \\(p(x,y) \\in [0,1]\\) and \\[\\sum\\sum p(x,y) = 1\\]\nContinuous:\n\\[f(x,y);P((X,Y) \\in A) = \\int\\!\\!\\!\\int_A f(x,y)dx dy \\]\ns.t. \\(f(x,y)\\ge 0\\) and\n\\[\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty f(x,y)dxdy = 1\\]\nIf X and Y are independent, then \\(P(X=x,Y=y) = P(X=x)P(Y=y)\\) and \\(f(x,y) = f(x)f(y)\\)\nMarginal Probability Distribution: probability distribution of only one of the two variables (ignoring information about the other variable), we can obtain the marginal distribution by summing/integrating across the variable that we don’t care about:\n\nDiscrete: \\(p_X(x) = \\sum_i p(x, y_i)\\)\nContinuous: \\(f_X(x) = \\int_{-\\infty}^\\infty f(x,y)dy\\)\n\nConditional Probability Distribution: probability distribution for one variable, holding the other variable fixed. Recalling from the previous lecture that \\(P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\), we can write the conditional distribution as\n\nDiscrete: \\(p_{Y|X}(y|x) = \\frac{p(x,y)}{p_X(x)}, \\quad p_X(x) > 0\\)\nContinuous: \\(f_{Y|X}(y|x) = \\frac{f(x,y)}{f_X(x)},\\quad f_X(x) > 0\\)\n\n\nExercise 6.4 (Discrete, Joint Distributions) Suppose we are interested in the outcomes of flipping a coin and rolling a 6-sided die at the same time. The sample space for this process contains 12 elements: \\[\\{(H, 1), (H, 2), (H, 3), (H, 4), (H, 5), (H, 6), (T, 1), (T, 2), (T, 3), (T, 4), (T, 5), (T, 6)\\}\\] We can define two random variables \\(X\\) and \\(Y\\) such that \\(X = 1\\) if heads and \\(X = 0\\) if tails, while \\(Y\\) equals the number on the die.\nWe can then make statements about the joint distribution of \\(X\\) and \\(Y\\). What are the following?\n\n\\(P(X=x)\\)\n\\(P(Y=y)\\)\n\\(P(X=x, Y=y)\\)\n\\(P(X=x|Y=y)\\)\nAre X and Y independent?"
  },
  {
    "objectID": "06_probability.html#expectation",
    "href": "06_probability.html#expectation",
    "title": "6  Probability Theory",
    "section": "6.8 Expectation",
    "text": "6.8 Expectation\nWe often want to summarize some characteristics of the distribution of a random variable. The most important summary is the expectation (or expected value, or mean), in which the possible values of a random variable are weighted by their probabilities.\n\nDefinition 6.13 (Expectation of a discrete R.V.) The expected value of a discrete random variable \\(Y\\) is \\[E(Y)=\\sum\\limits_{y} y P(Y=y)= \\sum\\limits_{y} y p(y)\\]\nIn words, it is the weighted average of all possible values of \\(Y\\), weighted by the probability that \\(y\\) occurs. It is not necessarily the number we would expect \\(Y\\) to take on, but the average value of \\(Y\\) after a large number of repetitions of an experiment.\n\n\nExample 6.9 (Expectation of a discrete R.V.) What is the expectation of a fair, six-sided die?\n\nExpectation of a Continuous Random Variable: The expected value of a continuous random variable is similar in concept to that of the discrete random variable, except that instead of summing using probabilities as weights, we integrate using the density to weight. Hence, the expected value of the continuous variable \\(Y\\) is defined by \\[E(Y)=\\int\\limits_{y} y f(y) dy\\]\n\nExample 6.10 (Expectation of a continuous R.V.) Find \\(E(Y)\\) for \\(f(y)=\\frac{1}{1.5}, \\quad 0<y<1.5\\).\n\n\nExpected Value of a Function\nRemember: An Expected Value is a type of weighted average. We can extend this to composite functions. For random variable \\(Y\\),\nIf \\(Y\\) is Discrete with PMF \\(p(y)\\),\n\\[E[g(Y)]=\\sum\\limits_y g(y)p(y)\\]\nIf \\(Y\\) is Continuous with PDF \\(f(y)\\),\n\\[E[g(Y)]=\\int\\limits_{-\\infty}^\\infty g(y)f(y)dy\\]\n\n\nProperties of Expected Values\nDealing with Expectations is easier when the thing inside is a sum. The intuition behind this that Expectation is an integral, which is a type of sum.\n\nExpectation of a constant is a constant \\[E(c)=c\\]\nConstants come out \\[E(c g(Y))= c E(g(Y))\\]\nExpectation is Linear \\[E(g(Y_1) + \\cdots + g(Y_n))=E(g(Y_1)) +\\cdots+E(g(Y_n)),\\] regardless of independence\nExpected Value of Expected Values: \\[E(E(Y)) = E(Y)\\] (because the expected value of a random variable is a constant)\n\nFinally, if \\(X\\) and \\(Y\\) are independent, even products are easy:\n\\[E(XY) = E(X)E(Y)\\]\nConditional Expectation: With joint distributions, we are often interested in the expected value of a variable \\(Y\\) if we could hold the other variable \\(X\\) fixed. This is the conditional expectation of \\(Y\\) given \\(X = x\\):\n\n\\(Y\\) discrete: \\(E(Y|X = x) = \\sum_y yp_{Y|X}(y|x)\\)\n\\(Y\\) continuous: \\(E(Y|X = x) = \\int_y yf_{Y|X}(y|x)dy\\)\n\nThe conditional expectation is often used for prediction when one knows the value of \\(X\\) but not \\(Y\\)"
  },
  {
    "objectID": "06_probability.html#variance-and-covariance",
    "href": "06_probability.html#variance-and-covariance",
    "title": "6  Probability Theory",
    "section": "6.9 Variance and Covariance",
    "text": "6.9 Variance and Covariance\nWe can also look at other summaries of the distribution, which build on the idea of taking expectations. Variance tells us about the “spread” of the distribution; it is the expected value of the squared deviations from the mean of the distribution. The standard deviation is simply the square root of the variance.\n\nDefinition 6.14 The Variance of a Random Variable \\(Y\\) is\n\\[\\text{Var}(Y) = E[(Y - E(Y))^2] =  E(Y^2)-[E(Y)]^2\\]\nThe Standard Deviation is the square root of the variance : \\[SD(Y) = \\sigma_Y= \\sqrt{\\text{Var}(Y)}\\]\n\n\nExample 6.11 Given the following PMF: \\[f(x) =  \\begin{cases}\n              \\frac{3!}{x!(3-x)!}(\\frac{1}{2})^3 \\quad x = 0,1,2,3\\\\\n               0 \\quad otherwise\n            \\end{cases}\n               \\]\nWhat is \\(\\text{Var}(x)\\)?\nHint: First calculate \\(E(X)\\) and \\(E(X^2)\\)\n\n\nDefinition 6.15 (Covariance) The covariance measures the degree to which two random variables vary together; if the covariance between \\(X\\) and \\(Y\\) is positive, X tends to be larger than its mean when Y is larger than its mean.\n\\[\\text{Cov}(X,Y) = E[(X - E(X))(Y - E(Y))] \\] We can also write this as\n\\[\\begin{align*}\n\\text{Cov}(X,Y) &= E\\left(XY - XE(Y) - E(X)Y + E(X)E(Y)\\right)\\\\\n&= E(XY) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y)\\\\\n&= E(XY) - E(X)E(Y)\n\\end{align*}\\]\n\nThe covariance of a variable with itself is the variance of that variable.\nThe Covariance is unfortunately hard to interpret in magnitude. The correlation is a standardized version of the covariance, and always ranges from -1 to 1.\n\nDefinition 6.16 (Correlation) The correlation coefficient is the covariance divided by the standard deviations of \\(X\\) and \\(Y\\). It is a unitless measure and always takes on values in the interval \\([-1,1]\\).\n\\[\\text{Corr}(X, Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} = \\frac{\\text{Cov}(X,Y)}{SD(X)SD(Y)}\\]\n\nProperties of Variance and Covariance:\n\n\\(\\text{Var}(c) = 0\\)\n\\(\\text{Var}(cY) = c^2 \\text{Var}(Y)\\)\n\\(\\text{Cov}(Y,Y) = \\text{Var}(Y)\\)\n\\(\\text{Cov}(X,Y) = \\text{Cov}(Y,X)\\)\n\\(\\text{Cov}(aX,bY) = ab \\text{Cov}(X,Y)\\)\n\\(\\text{Cov}(X+a,Y) = \\text{Cov}(X,Y)\\)\n\\(\\text{Cov}(X+Z,Y+W) = \\text{Cov}(X,Y) + \\text{Cov}(X,W) + \\text{Cov}(Z,Y) + \\text{Cov}(Z,W)\\)\n\\(\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y)\\)\n\n\nExercise 6.5 (Expectation and Variance 1) Suppose we have a PMF with the following characteristics: \\[\\begin{eqnarray*}\n  P(X = -2) = \\frac{1}{5}\\\\\n  P(X = -1) = \\frac{1}{6}\\\\\n  P(X = 0) = \\frac{1}{5}\\\\\n  P(X = 1) = \\frac{1}{15}\\\\\n  P(X = 2) = \\frac{11}{30}\n\\end{eqnarray*}\\]\n\nCalculate the expected value of X\n\nDefine the random variable \\(Y = X^2\\).\n\nCalculate the expected value of Y. (Hint: It would help to derive the PMF of Y first in order to calculate the expected value of Y in a straightforward way)\nCalculate the variance of X.\n\n\n\n\nExercise 6.6 (Expectation and Variance 2) \nFind the expectation and variance\n\nGiven the following PDF: \\[f(x) =  \\begin{cases}\n              \\frac{3}{10}(3x - x^2) \\quad 0 \\leq x \\leq 2\\\\\n               0 \\quad otherwise\n            \\end{cases}\n               \\]\n\n\n\nExercise 6.7 (Expectation and Variance 3) \nFind the mean and standard deviation of random variable X. The PDF of this X is as follows:\n\n\\[f(x) =  \\begin{cases}\n              \\frac{1}{4}x \\quad 0 \\leq x \\leq 2\\\\\n               \\frac{1}{4}(4 - x)  \\quad 2 \\leq x \\leq 4\\\\\n               0 \\quad otherwise\n            \\end{cases}\n               \\]\n\nNext, calculate \\(P(X < \\mu - \\sigma)\\) Remember, \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation"
  },
  {
    "objectID": "06_probability.html#distributions-1",
    "href": "06_probability.html#distributions-1",
    "title": "6  Probability Theory",
    "section": "6.10 Distributions",
    "text": "6.10 Distributions\nA distribution is defined by its cumulative distribution function. There are many common distributions that have useful properties that appear in probability and statistics.\nTwo discrete distributions used often are:\n\nDefinition 6.17 (Binomial Distribution) \\(Y\\) is distributed binomial if it represents the number of “successes” observed in \\(n\\) independent, identical “trials,” where the probability of success in any trial is \\(p\\) and the probability of failure is \\(q=1-p\\).\n\nFor any particular sequence of \\(y\\) successes and \\(n-y\\) failures, the probability of obtaining that sequence is \\(p^y q^{n-y}\\) (by the multiplicative law and independence). However, there are \\(\\binom{n}{y}=\\frac{n!}{(n-y)!y!}\\) ways of obtaining a sequence with \\(y\\) successes and \\(n-y\\) failures. So the binomial distribution is given by \\[p(y)=\\binom{n}{y}p^y q^{n-y}, \\quad y=0,1,2,\\ldots,n\\] with mean \\(\\mu=E(Y)=np\\) and variance \\(\\sigma^2=\\text{Var}(Y)=npq\\).\n\nExample 6.12 (Binomial distribution) Republicans vote for Democrat-sponsored bills 2% of the time. What is the probability that out of 10 Republicans questioned, half voted for a particular Democrat-sponsored bill? What is the mean number of Republicans voting for Democrat-sponsored bills? The variance? 1. \\(P(Y=5)=\\) 1. \\(E(Y)=\\) 1. \\(\\text{Var}(Y)=6\\)\n\n\nDefinition 6.18 (Poisson Distribution) A random variable \\(Y\\) has a Poisson distribution if\n\\[P(Y = y)=\\frac{\\lambda^y}{y!}e^{-\\lambda}, \\quad y=0,1,2,\\ldots, \\quad \\lambda>0\\]\nThe Poisson has the unusual feature that its expectation equals its variance: \\(E(Y)=\\text{Var}(Y)=\\lambda\\). The Poisson distribution is often used to model rare event counts: counts of the number of events that occur during some unit of time. \\(\\lambda\\) is often called the “arrival rate.”\n\n\nExample 6.13 (Poisson Distribution) Border disputes occur between two countries through a Poisson Distribution, at a rate of 2 per month. What is the probability of 0, 2, and less than 5 disputes occurring in a month?\n\nTwo continuous distributions used often are:\n\nDefinition 6.19 (Uniform Distribution) A random variable \\(Y\\) has a continuous uniform distribution on the interval \\((\\alpha,\\beta)\\) if its density is given by \\[f(y)=\\frac{1}{\\beta-\\alpha}, \\quad \\alpha\\le y\\le \\beta\\] The mean and variance of \\(Y\\) are \\(E(Y)=\\frac{\\alpha+\\beta}{2}\\) and \\(\\text{Var}(Y)=\\frac{(\\beta-\\alpha)^2}{12}\\).\n\n\nExample 6.14 (Uniform) For \\(Y\\) uniformly distributed over \\((1,3)\\), what are the following probabilities?\n\n\\(P(Y=2)\\)\nIts density evaluated at 2, or \\(f(2)\\)\n\\(P(Y \\le 2)\\)\n\\(P(Y > 2)\\)\n\n\n\nDefinition 6.20 (Normal Distribution) A random variable \\(Y\\) is normally distributed with mean \\(E(Y)=\\mu\\) and variance \\(\\text{Var}(Y)=\\sigma^2\\) if its density is\n\\[f(y)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\\]\n\nSee Figure 6.2 are various Normal Distributions with the same \\(\\mu = 1\\) and two versions of the variance.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 6.2: Normal Distribution Density"
  },
  {
    "objectID": "06_probability.html#summarizing-observed-events-data",
    "href": "06_probability.html#summarizing-observed-events-data",
    "title": "6  Probability Theory",
    "section": "6.11 Summarizing Observed Events (Data)",
    "text": "6.11 Summarizing Observed Events (Data)\nSo far, we’ve talked about distributions in a theoretical sense, looking at different properties of random variables. We don’t observe random variables; we observe realizations of the random variable. These realizations of events are roughly equivalent to what we mean by “data”. We’ll spend more time in the intro class talking about this from the standpoint of estimands, estimators and estimates.\nSample mean: This is the most common measure of central tendency, calculated by summing across the observations and dividing by the number of observations. \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\]\nDispersion: We also typically want to know how spread out the data are relative to the center of the observed distribution. There are several ways to measure dispersion.\nSample variance: The sample variance is the sum of the squared deviations from the sample mean, divided by the number of observations minus 1. \\[\\widehat{\\text{Var}}(X) = \\frac{1}{n-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2\\]\nAgain, this is an estimator of the variance of a random variable; we divide by \\(n - 1\\) instead of \\(n\\) in order to get an unbiased estimator.\nStandard deviation: The sample standard deviation is the square root of the sample variance. \\[\\widehat{SD}(X) = \\sqrt{\\hat{\\text{Var}}(X)} = \\sqrt{\\frac{1}{n-1}\\sum_{i = 1}^n (x_i - \\bar{x})^2}\\]\nCovariance and Correlation: Both of these quantities measure the degree to which two variables vary together, and are estimates of the covariance and correlation of two random variables as defined above.\n\nSample covariance: \\(\\hat{\\text{Cov}}(X,Y) = \\frac{1}{n-1}\\sum_{i = 1}^n(x_i - \\bar{x})(y_i - \\bar{y})\\)\nSample correlation: \\(\\hat{\\text{Corr}} = \\frac{\\hat{\\text{Cov}}(X,Y)}{\\sqrt{\\hat{\\text{Var}}(X)\\hat{\\text{Var}}(Y)}}\\)\n\n\nExample 6.15 (Sample Covariance and Correlation) Example: Using the above table, calculate the sample versions of:\n\n\\(\\text{Cov}(X,Y)\\)\n\\(\\text{Corr}(X, Y)\\)"
  },
  {
    "objectID": "06_probability.html#asymptotic-theory",
    "href": "06_probability.html#asymptotic-theory",
    "title": "6  Probability Theory",
    "section": "6.12 Asymptotic Theory",
    "text": "6.12 Asymptotic Theory\nIn theoretical and applied research, asymptotic arguments are often made. In this section we briefly introduce some of this material.\nWhat are asymptotics? In probability theory, asymptotic analysis is the study of limiting behavior. By limiting behavior, we mean the behavior of some random process as the number of observations gets larger and larger.\nWhy is this important? We rarely know the true process governing the events we see in the social world. It is helpful to understand how such unknown processes theoretically must behave and asymptotic theory helps us do this.\n\n6.12.1 CLT and LLN\nWe are now finally ready to revisit, with a bit more precise terms, the two pillars of statistical theory we motivated Section @ref(limitsfun) with.\n\nTheorem 6.1 (Central Limit Theorem) Let \\(\\{X_n\\} = \\{X_1, X_2, \\ldots\\}\\) be a sequence of i.i.d. random variables with finite mean (\\(\\mu\\)) and variance (\\(\\sigma^2\\)). Then, the sample mean \\(\\bar{X}_n = \\frac{X_1 + X_2 + \\cdots + X_n}{n}\\) increasingly converges into a Normal distribution.\n\\[\\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} \\text{Normal}(0, 1),\\]\nAnother way to write this as a probability statement is that for all real numbers \\(a\\),\n\\[P\\left(\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\le a\\right) \\rightarrow \\Phi(a)\\] as \\(n\\to \\infty\\), where \\[\\Phi(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}} \\, dx\\] is the CDF of a Normal distribution with mean 0 and variance 1.\nThis result means that, as \\(n\\) grows, the distribution of the sample mean \\(\\bar X_n = \\frac{1}{n} (X_1 + X_2 + \\cdots + X_n)\\) is approximately normal with mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt n}\\), i.e., \\[\\bar{X}_n \\approx \\mathcal{N}\\bigg(\\mu, \\frac{\\sigma^2}{n}\\bigg).\\] The standard deviation of \\(\\bar X_n\\) (which is roughly a measure of the precision of \\(\\bar X_n\\) as an estimator of \\(\\mu\\)) decreases at the rate \\(1/\\sqrt{n}\\), so, for example, to increase its precision by \\(10\\) (i.e., to get one more digit right), one needs to collect \\(10^2=100\\) times more units of data.\nIntuitively, this result also justifies that whenever a lot of small, independent processes somehow combine together to form the realized observations, practitioners often feel comfortable assuming Normality.\n\n\nTheorem 6.2 (Weak Law of Large Numbers (LLN)) For any draw of independent random variables with the same mean \\(\\mu\\), the sample average after \\(n\\) draws, \\(\\bar{X}_n = \\frac{1}{n}(X_1 + X_2 + \\ldots + X_n)\\), converges in probability to the expected value of \\(X\\), \\(\\mu\\) as \\(n \\rightarrow \\infty\\):\n\\[\\lim\\limits_{n\\to \\infty} P(|\\bar{X}_n - \\mu | > \\varepsilon) = 0\\]\nA shorthand of which is \\(\\bar{X}_n \\xrightarrow{p} \\mu\\), where the arrow is read as “converges in probability to” as \\(n\\to \\infty\\). In other words, \\(P( \\lim_{n\\to\\infty}\\bar{X}_n = \\mu) = 1\\). This is an important motivation for the widespread use of the sample mean, as well as the intuition link between averages and expected values.\n\nMore precisely this version of the LLN is called the weak law of large numbers because it leaves open the possibility that \\(|\\bar{X}_n - \\mu | > \\varepsilon\\) occurs many times. The strong law of large numbers states that, under a few more conditions, the probability that the limit of the sample average is the true mean is 1 (and other possibilities occur with probability 0), but the difference is rarely consequential in practice.\nThe Strong Law of Large Numbers holds so long as the expected value exists; no other assumptions are needed. However, the rate of convergence will differ greatly depending on the distribution underlying the observed data. When extreme observations occur often (i.e. kurtosis is large), the rate of convergence is much slower. Cf. The distribution of financial returns.\n\n\n6.12.2 Big \\(\\mathcal{O}\\) Notation\nSome of you may encounter “big-OH’’-notation. If \\(f, g\\) are two functions, we say that \\(f = \\mathcal{O}(g)\\) if there exists some constant, \\(c\\), such that \\(f(n) \\leq c \\times g(n)\\) for large enough \\(n\\). This notation is useful for simplifying complex problems in game theory, computer science, and statistics.\n\nExample 6.16 What is \\(\\mathcal{O}( 5\\exp(0.5 n) + n^2 + n / 2)\\)? Answer: \\(\\exp(n)\\). Why? Because, for large \\(n\\), \\[\n\\frac{ 5\\exp(0.5 n) + n^2 + n / 2 }{ \\exp(n)} \\leq \\frac{ c \\exp(n) }{ \\exp(n)} = c.\n\\] whenever \\(n > 4\\) and where \\(c = 1\\)."
  },
  {
    "objectID": "07_linear-algebra.html",
    "href": "07_linear-algebra.html",
    "title": "7  Linear Algebra",
    "section": "",
    "text": "Topics: \\(\\bullet\\) Working with Vectors \\(\\bullet\\) Linear Independence \\(\\bullet\\) Basics of Matrix Algebra \\(\\bullet\\) Square Matrices \\(\\bullet\\) Linear Equations \\(\\bullet\\) Systems of Linear Equations \\(\\bullet\\) Systems of Equations as Matrices \\(\\bullet\\) Solving Augmented Matrices and Systems of Equations \\(\\bullet\\) Rank \\(\\bullet\\) The Inverse of a Matrix \\(\\bullet\\) Inverse of Larger Matrices"
  },
  {
    "objectID": "07_linear-algebra.html#sec-vector-def",
    "href": "07_linear-algebra.html#sec-vector-def",
    "title": "7  Linear Algebra",
    "section": "7.1 Working with Vectors",
    "text": "7.1 Working with Vectors\nVector: A vector in \\(n\\)-space is an ordered list of \\(n\\) numbers. These numbers can be represented as either a row vector or a column vector: \\[ {\\mathbf v} \\begin{pmatrix} v_1 & v_2 & \\dots & v_n\\end{pmatrix} , {\\mathbf v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix}\\]\nWe can also think of a vector as defining a point in \\(n\\)-dimensional space, usually \\({\\mathbf R}^n\\); each element of the vector defines the coordinate of the point in a particular direction.\nVector Addition and Subtraction: If two vectors, \\({\\mathbf u}\\) and \\({\\mathbf v}\\), have the same length (i.e. have the same number of elements), they can be added (subtracted) together: \\[ {\\mathbf u} + {\\mathbf v} = \\begin{pmatrix} u_1 + v_1 & u_2 + v_2 & \\cdots & u_k + v_n \\end{pmatrix}\\] \\[ {\\mathbf u} - {\\mathbf v} = \\begin{pmatrix} u_1 - v_1 & u_2 - v_2 & \\cdots & u_k - v_n \\end{pmatrix}\\]\nScalar Multiplication: The product of a scalar \\(c\\) (i.e. a constant) and vector \\({\\mathbf v}\\) is:\n\\[ c{\\mathbf v} =  \\begin{pmatrix} cv_1 & cv_2 & \\dots & cv_n \\end{pmatrix} \\]\nVector Inner Product: The inner product (also called the dot product or scalar product) of two vectors \\({\\mathbf u}\\) and \\({\\mathbf v}\\) is again defined if and only if they have the same number of elements \\[ {\\mathbf u} \\cdot {\\mathbf v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n = \\sum_{i = 1}^n u_iv_i\\] If \\({\\mathbf u} \\cdot {\\mathbf v} = 0\\), the two vectors are orthogonal (or perpendicular).\nVector Norm: The norm of a vector is a measure of its length. There are many different ways to calculate the norm, but the most common is the Euclidean norm (which corresponds to our usual conception of distance in three-dimensional space): \\[ ||{\\mathbf v}|| = \\sqrt{{\\mathbf v}\\cdot{\\mathbf v}} = \\sqrt{ v_1v_1 + v_2v_2 + \\cdots + v_nv_n}\\]\n\nExample 7.1 (Vector Algebra) Let \\(a = \\begin{pmatrix} 2&1&2\\end{pmatrix}\\), \\(b = \\begin{pmatrix} 3&4&5 \\end{pmatrix}\\). Calculate the following:\n\n\\(a - b\\)\n\\(a \\cdot b\\)\n\n\n\nExercise 7.1 (Vector Algebra) Let \\(u = \\begin{pmatrix} 7&1&-5&3\\end{pmatrix}\\), \\(v = \\begin{pmatrix} 9&-3&2&8 \\end{pmatrix}\\), \\(w = \\begin{pmatrix} 1&13& -7&2 &15 \\end{pmatrix}\\), and \\(c = 2\\). Calculate the following:\n\n\\(u-v\\)\n\\(cw\\)\n\\(u \\cdot v\\)\n\\(w \\cdot v\\)"
  },
  {
    "objectID": "07_linear-algebra.html#sec-linearindependence",
    "href": "07_linear-algebra.html#sec-linearindependence",
    "title": "7  Linear Algebra",
    "section": "7.2 Linear Independence",
    "text": "7.2 Linear Independence\nLinear combinations: The vector \\({\\mathbf u}\\) is a linear combination of the vectors \\({\\mathbf v}_1, {\\mathbf v}_2, \\cdots , {\\mathbf v}_k\\) if \\[{\\mathbf u} = c_1{\\mathbf v}_1 + c_2{\\mathbf v}_2 +  \\cdots + c_k{\\mathbf v}_k\\]\nFor example, \\(\\begin{pmatrix}9 & 13 & 17 \\end{pmatrix}\\) is a linear combination of the following three vectors: \\(\\begin{pmatrix}1 & 2 & 3 \\end{pmatrix}\\), \\(\\begin{pmatrix} 2 & 3& 4\\end{pmatrix}\\), and \\(\\begin{pmatrix} 3 & 4 & 5 \\end{pmatrix}\\). This is because \\(\\begin{pmatrix}9 & 13 & 17 \\end{pmatrix}\\) = \\((2)\\begin{pmatrix}1 & 2 & 3 \\end{pmatrix}\\) \\(+ (-1)\\begin{pmatrix} 2 & 3& 4\\end{pmatrix}\\) + \\(3\\begin{pmatrix} 3 & 4 & 5 \\end{pmatrix}\\)\nLinear independence: A set of vectors \\({\\mathbf v}_1, {\\mathbf v}_2, \\cdots , {\\mathbf v}_k\\) is linearly independent if the only solution to the equation \\[c_1{\\mathbf v}_1 + c_2{\\mathbf v}_2 +  \\cdots + c_k{\\mathbf v}_k = 0\\] is \\(c_1 = c_2 = \\cdots = c_k = 0\\). If another solution exists, the set of vectors is linearly dependent.\nA set \\(S\\) of vectors is linearly dependent if and only if at least one of the vectors in \\(S\\) can be written as a linear combination of the other vectors in \\(S\\).\nLinear independence is only defined for sets of vectors with the same number of elements; any linearly independent set of vectors in \\(n\\)-space contains at most \\(n\\) vectors.\nSince \\(\\begin{pmatrix}9 & 13 & 17 \\end{pmatrix}\\) is a linear combination of \\(\\begin{pmatrix}1 & 2 & 3 \\end{pmatrix}\\), \\(\\begin{pmatrix} 2 & 3& 4\\end{pmatrix}\\), and \\(\\begin{pmatrix} 3 & 4 & 5 \\end{pmatrix}\\), these 4 vectors constitute a linearly dependent set.\n\nExample 7.2 (Linear indepencence) Are the following sets of vectors linearly independent?\n\n\\(\\begin{pmatrix}2 & 3 & 1 \\end{pmatrix}\\) and \\(\\begin{pmatrix}4 & 6 & 1 \\end{pmatrix}\\)\n\\(\\begin{pmatrix}1 & 0 & 0 \\end{pmatrix}\\), \\(\\begin{pmatrix}0 & 5 & 0 \\end{pmatrix}\\), and \\(\\begin{pmatrix}10 & 10 & 0 \\end{pmatrix}\\)\n\n\n\nExercise 7.2 (Linear indepencence) Are the following sets of vectors linearly independent?\n\n\\({\\mathbf v}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} , {\\mathbf v}_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} , {\\mathbf v}_3 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\)\n\\({\\mathbf v}_1 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} , {\\mathbf v}_2 = \\begin{pmatrix} -4 \\\\ 6 \\\\ 5 \\end{pmatrix} , {\\mathbf v}_3 = \\begin{pmatrix} -2 \\\\ 8 \\\\ 6 \\end{pmatrix}\\)"
  },
  {
    "objectID": "07_linear-algebra.html#matrixbasics",
    "href": "07_linear-algebra.html#matrixbasics",
    "title": "7  Linear Algebra",
    "section": "7.3 Basics of Matrix Algebra",
    "text": "7.3 Basics of Matrix Algebra\nMatrix: A matrix is an array of real numbers arranged in \\(m\\) rows by \\(n\\) columns. The dimensionality of the matrix is defined as the number of rows by the number of columns, \\(m \\times n\\).\n\\[{\\mathbf A}=\\begin{pmatrix}\n            a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n            a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            a_{m1} & a_{m2} & \\cdots & a_{mn}\n        \\end{pmatrix}\\]\nNote that you can think of vectors as special cases of matrices; a column vector of length \\(k\\) is a \\(k \\times 1\\) matrix, while a row vector of the same length is a \\(1 \\times k\\) matrix.\nIt’s also useful to think of matrices as being made up of a collection of row or column vectors. For example, \\[\\mathbf A = \\begin{pmatrix} {\\mathbf a}_1 & {\\mathbf a}_2 &  \\cdots & {\\mathbf a}_m \\end{pmatrix}\\]\nMatrix Addition: Let \\(\\mathbf A\\) and \\(\\mathbf B\\) be two \\(m\\times n\\) matrices. \\[{\\mathbf A+B}=\\begin{pmatrix}\n            a_{11}+b_{11} & a_{12}+b_{12} & \\cdots & a_{1n}+b_{1n} \\\\\n            a_{21}+b_{21} & a_{22}+b_{22} & \\cdots & a_{2n}+b_{2n} \\\\\n            \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n            a_{m1}+b_{m1} & a_{m2}+b_{m2} & \\cdots & a_{mn}+b_{mn}\n        \\end{pmatrix}\\]\nNote that matrices \\({\\mathbf A}\\) and \\({\\mathbf B}\\) must have the same dimensionality, in which case they are conformable for addition.\n\nExample 7.3 (Matrix addition) \\[{\\mathbf A}=\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{pmatrix}, \\qquad\n            {\\mathbf B}=\\begin{pmatrix} 1 & 2 & 1 \\\\ 2 & 1 & 2 \\end{pmatrix}\\] \\({\\mathbf A+B}=\\)\n\nScalar Multiplication: Given the scalar \\(s\\), the scalar multiplication of \\(s {\\mathbf A}\\) is \\[ s {\\mathbf A}=  s \\begin{pmatrix}\n            a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n            a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            a_{m1} & a_{m2} & \\cdots & a_{mn}\n        \\end{pmatrix}\n        = \\begin{pmatrix}\n            s a_{11} & s a_{12} & \\cdots & s a_{1n} \\\\\n            s a_{21} & s a_{22} & \\cdots & s a_{2n} \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            s a_{m1} & s a_{m2} & \\cdots & s a_{mn}\n        \\end{pmatrix}\\]\n\nExample 7.4 (Scalar Multiplication) \\(s=2, \\qquad {\\mathbf A}=\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{pmatrix}\\)\n\\(s {\\mathbf A} =\\)\n\nMatrix Multiplication: If \\({\\mathbf A}\\) is an \\(m\\times k\\) matrix and \\(\\mathbf B\\) is a \\(k\\times n\\) matrix, then their product \\(\\mathbf C = A B\\) is the \\(m\\times n\\) matrix where \\[c_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+\\cdots+a_{ik}b_{kj}\\]\n\n\nExample 7.5 (Matrix multiplication) \n\\(\\begin{pmatrix} a&b\\\\c&d\\\\e&f \\end{pmatrix} \\begin{pmatrix} A&B\\\\C&D \\end{pmatrix} =\\)\n\\(\\begin{pmatrix} 1&2&-1\\\\3&1&4 \\end{pmatrix} \\begin{pmatrix} -2&5\\\\4&-3\\\\2&1\\end{pmatrix} =\\)\n\n\nNote that the number of columns of the first matrix must equal the number of rows of the second matrix, in which case they are conformable for multiplication. The sizes of the matrices (including the resulting product) must be \\[(m\\times k)(k\\times n)=(m\\times n)\\]\nAlso note that if AB exists, BA exists only if \\(\\dim({\\mathbf A}) = m \\times n\\) and \\(\\dim({\\mathbf B}) = n \\times m\\).\nThis does not mean that AB = BA. AB = BA is true only in special circumstances, like when \\({\\mathbf A}\\) or \\({\\mathbf B}\\) is an identity matrix or \\({\\mathbf A} = {\\mathbf B}^{-1}\\).\nLaws of Matrix Algebra:\nCommutative law for multiplication does not hold – the order of multiplication matters: \\[\\mathbf AB\\ne BA\\]\nFor example, \\[{\\mathbf A}=\\begin{pmatrix} 1&2\\\\-1&3\\end{pmatrix}, \\qquad {\\mathbf B}=\\begin{pmatrix} 2&1\\\\0&1\\end{pmatrix}\\] \\[{\\mathbf AB}=\\begin{pmatrix} 2&3\\\\-2&2\\end{pmatrix}, \\qquad {\\mathbf BA}=\\begin{pmatrix} 1&7\\\\-1&3\\end{pmatrix}\\]\nTranspose: The transpose of the \\(m\\times n\\) matrix \\(\\mathbf A\\) is the \\(n\\times m\\) matrix \\({\\mathbf A}^T\\) (also written \\({\\mathbf A}'\\)) obtained by interchanging the rows and columns of \\(\\mathbf A\\).\nFor example,\n\\({\\mathbf A}=\\begin{pmatrix} 4&-2&3\\\\0&5&-1\\end{pmatrix}, \\qquad {\\mathbf A}^T=\\begin{pmatrix} 4&0\\\\-2&5\\\\3&-1 \\end{pmatrix}\\)\n\\({\\mathbf B}=\\begin{pmatrix} 2\\\\-1\\\\3 \\end{pmatrix}, \\qquad {\\mathbf B}^T=\\begin{pmatrix} 2&-1&3\\end{pmatrix}\\)\nThe following rules apply for transposed matrices:\nExample of \\(({\\mathbf AB})^T = {\\mathbf B}^T{\\mathbf A}^T\\): \\[{\\mathbf A}=\\begin{pmatrix} 1&3&2\\\\2&-1&3\\end{pmatrix}, \\qquad {\\mathbf B}=\\begin{pmatrix} 0&1\\\\2&2\\\\3&-1\\end{pmatrix}\\] \\[ ({\\mathbf AB})^T = \\left[ \\begin{pmatrix} 1&3&2\\\\2&-1&3\\end{pmatrix} \\begin{pmatrix} 0&1\\\\2&2\\\\3&-1\\end{pmatrix} \\right]^T = \\begin{pmatrix} 12&7\\\\5&-3 \\end{pmatrix}\\] \\[ {\\mathbf B}^T{\\mathbf A}^T= \\begin{pmatrix} 0&2&3\\\\1&2&-1 \\end{pmatrix}  \\begin{pmatrix} 1&2\\\\3&-1\\\\2&3 \\end{pmatrix} = \\begin{pmatrix} 12&7\\\\5&-3 \\end{pmatrix}\\]\n\nExercise 7.3 (Matrix Multiplication) Let \\[A =  \\begin{pmatrix} 2&0&-1&1\\\\1&2&0&1 \\end{pmatrix}\\]\n\\[B = \\begin{pmatrix} 1&5&-7\\\\1&1&0\\\\0&-1&1\\\\2&0&0\\end{pmatrix} \\]\n\\[C =  \\begin{pmatrix} 3&2&-1\\\\0&4&6 \\end{pmatrix}\\]\nCalculate the following:\n\n\\[AB\\]\n\\[BA\\]\n\\[(BC)^T\\]\n\\[BC^T\\]"
  },
  {
    "objectID": "07_linear-algebra.html#systems-of-linear-equations",
    "href": "07_linear-algebra.html#systems-of-linear-equations",
    "title": "7  Linear Algebra",
    "section": "7.4 Systems of Linear Equations",
    "text": "7.4 Systems of Linear Equations\nLinear Equation: \\(a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n = b\\)\n\\(a_i\\) are parameters or coefficients. \\(x_i\\) are variables or unknowns.\nLinear because only one variable per term and degree is at most 1.\nWe are often interested in solving linear systems like\n\\[\\begin{matrix}\n            x  & - & 3y & = & -3\\\\\n            2x & + &  y & = &  8\n            \\end{matrix}\\]\nMore generally, we might have a system of \\(m\\) equations in \\(n\\) unknowns\n\\[\\begin{matrix}\n            a_{11}x_1  & + & a_{12}x_2 & + & \\cdots & + & a_{1n}x_n & = & b_1\\\\\n            a_{21}x_1  & + & a_{22}x_2 & + & \\cdots & + & a_{2n}x_n & = & b_2\\\\\n            \\vdots     &   &     &   & \\vdots &   &     & \\vdots & \\\\\n            a_{m1}x_1  & + & a_{m2}x_2 & + & \\cdots & + & a_{mn}x_n & = & b_m\n            \\end{matrix}\\]\nA solution to a linear system of \\(m\\) equations in \\(n\\) unknowns is a set of \\(n\\) numbers \\(x_1, x_2, \\cdots, x_n\\) that satisfy each of the \\(m\\) equations.\nExample: \\(x=3\\) and \\(y=2\\) is the solution to the above \\(2\\times 2\\) linear system. If you graph the two lines, you will find that they intersect at \\((3,2)\\).\nDoes a linear system have one, no, or multiple solutions? For a system of 2 equations with 2 unknowns (i.e., two lines): _\nOne solution: The lines intersect at exactly one point.\nNo solution: The lines are parallel.\nInfinite solutions: The lines coincide.\nMethods to solve linear systems:\n\nSubstitution\nElimination of variables\nMatrix methods\n\n\nExercise 7.4 (Linear Equations) Provide a system of 2 equations with 2 unknowns that has\n\none solution\nno solution\ninfinite solutions"
  },
  {
    "objectID": "07_linear-algebra.html#systems-of-equations-as-matrices",
    "href": "07_linear-algebra.html#systems-of-equations-as-matrices",
    "title": "7  Linear Algebra",
    "section": "7.5 Systems of Equations as Matrices",
    "text": "7.5 Systems of Equations as Matrices\nMatrices provide an easy and efficient way to represent linear systems such as \\[\\begin{matrix}\n        a_{11}x_1  & + & a_{12}x_2 & + & \\cdots & + & a_{1n}x_n & = & b_1\\\\\n        a_{21}x_1  & + & a_{22}x_2 & + & \\cdots & + & a_{2n}x_n & = & b_2\\\\\n        \\vdots     &   &     &   & \\vdots &   &     & \\vdots & \\\\\n        a_{m1}x_1  & + & a_{m2}x_2 & + & \\cdots & + & a_{mn}x_n & = & b_m\n        \\end{matrix}\\]\nas \\[{\\mathbf A x = b}\\] where\nThe \\(m \\times n\\) \\({\\mathbf A}\\) is an array of \\(m n\\) real numbers arranged in \\(m\\) rows by \\(n\\) columns: \\[{\\mathbf A}=\\begin{pmatrix}\n            a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n            a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n            \\vdots &  & \\ddots & \\vdots \\\\\n            a_{m1} & a_{m2} & \\cdots & a_{mn}\n            \\end{pmatrix}\\]\nThe unknown quantities are represented by the vector \\({\\mathbf x}=\\begin{pmatrix} x_1\\\\x_2\\\\\\vdots\\\\x_n \\end{pmatrix}\\).\nThe right hand side of the linear system is represented by the vector \\({\\mathbf b}=\\begin{pmatrix} b_1\\\\b_2\\\\\\vdots\\\\b_m \\end{pmatrix}\\).\nAugmented Matrix: When we append \\(\\mathbf b\\) to the coefficient matrix \\(\\mathbf A\\), we get the augmented matrix \\(\\widehat{\\mathbf A}=[\\mathbf A | b]\\) \\[\\begin{pmatrix}\n            a_{11} & a_{12} & \\cdots & a_{1n} & | & b_1\\\\\n            a_{21} & a_{22} & \\cdots & a_{2n} & | & b_2\\\\\n            \\vdots &  & \\ddots & \\vdots & | & \\vdots\\\\\n            a_{m1} & a_{m2} & \\cdots & a_{mn} & | & b_m\n            \\end{pmatrix}\\]\n\nExercise 7.5 Create an augmented matrix that represent the following system of equations:\n\\[2x_1 -7x_2 + 9x_3 -4x_4 = 8\\] \\[41x_2 + 9x_3 -5x_6 = 11\\] \\[x_1 -15x_2 -11x_5 = 9\\]"
  },
  {
    "objectID": "07_linear-algebra.html#finding-solutions-to-augmented-matrices-and-systems-of-equations",
    "href": "07_linear-algebra.html#finding-solutions-to-augmented-matrices-and-systems-of-equations",
    "title": "7  Linear Algebra",
    "section": "7.6 Finding Solutions to Augmented Matrices and Systems of Equations",
    "text": "7.6 Finding Solutions to Augmented Matrices and Systems of Equations\nRow Echelon Form: Our goal is to translate our augmented matrix or system of equations into row echelon form. This will provide us with the values of the vector x which solve the system. We use the row operations to change coefficients in the lower triangle of the augmented matrix to 0. An augmented matrix of the form\n\\[\\begin{pmatrix}\n         a'_{11} & a'_{12} & a'_{13}& \\cdots & a'_{1n} & | & b'_1\\\\\n            0 & a'_{22} & a'_{23}& \\cdots & a'_{2n} & | & b'_2\\\\\n            0 & 0 & a'_{33} & \\cdots & a'_{3n} & | & b'_3\\\\\n            0 & 0 &0 & \\ddots & \\vdots  & | & \\vdots \\\\\n            0 & 0 &0 &0 & a'_{mn} & | & b'_m\n            \\end{pmatrix}\\]\nis said to be in row echelon form — each row has more leading zeros than the row preceding it.\nReduced Row Echelon Form: We can go one step further and put the matrix into reduced row echelon form. Reduced row echelon form makes the value of x which solves the system very obvious. For a system of \\(m\\) equations in \\(m\\) unknowns, with no all-zero rows, the reduced row echelon form would be\n\\[\\begin{pmatrix}\n            \\fbox{1}  &  0 &   0 &    0  &   0 & | & b^*_1\\\\\n            0  &  \\fbox{1} &   0 &    0  &   0 & | & b^*_2\\\\\n            0  &  0 &   \\fbox{1} &    0  &   0 & | & b^*_3\\\\\n            0  &  0 &   0 &\\ddots &   0 & | &\\vdots\\\\\n            0  &  0 &   0 &    0  &   \\fbox{1} & | & b^*_m\n            \\end{pmatrix}\\]\nGaussian and Gauss-Jordan elimination: We can conduct elementary row operations to get our augmented matrix into row echelon or reduced row echelon form. The methods of transforming a matrix or system into row echelon and reduced row echelon form are referred to as Gaussian elimination and Gauss-Jordan elimination, respectively.\nElementary Row Operations: To do Gaussian and Gauss-Jordan elimination, we use three basic operations to transform the augmented matrix into another augmented matrix that represents an equivalent linear system – equivalent in the sense that the same values of \\(x_j\\) solve both the original and transformed matrix/system:\nInterchanging Rows: Suppose we have the augmented matrix \\[{\\widehat{\\mathbf A}}=\\begin{pmatrix} a_{11} & a_{12} & | & b_1\\\\\n            a_{21} & a_{22} & | & b_2\n            \\end{pmatrix}\\] If we interchange the two rows, we get the augmented matrix \\[\\begin{pmatrix}\n            a_{21} & a_{22} & | & b_2\\\\\n            a_{11} & a_{12} & | & b_1\n            \\end{pmatrix}\\] which represents a linear system equivalent to that represented by matrix \\(\\widehat{\\mathbf A}\\).\nMultiplying by a Constant: If we multiply the second row of matrix \\(\\widehat{\\mathbf A}\\) by a constant \\(c\\), we get the augmented matrix \\[\\begin{pmatrix}\n            a_{11} & a_{12} & | & b_1\\\\\n            c a_{21} & c a_{22} & | & c b_2\n            \\end{pmatrix}\\] which represents a linear system equivalent to that represented by matrix \\(\\widehat{\\mathbf A}\\).\nAdding (subtracting) Rows: If we add (subtract) the first row of matrix \\(\\widehat{\\mathbf A}\\) to the second, we obtain the augmented matrix \\[\\begin{pmatrix}\n            a_{11} & a_{12} & | & b_1\\\\\n            a_{11}+a_{21} & a_{12}+a_{22} & | & b_1+b_2\n            \\end{pmatrix}\\] which represents a linear system equivalent to that represented by matrix \\(\\widehat{\\mathbf A}\\).\n\nExample 7.6 (Solving systems of equations) Solve the following system of equations by using elementary row operations:\n\\[\\begin{matrix}\n            x  & - & 3y & = & -3\\\\\n            2x & + &  y & = &  8\n            \\end{matrix}\\]\n\n\nExercise 7.6 (Solving Systems of Equations) Put the following system of equations into augmented matrix form. Then, using Gaussian or Gauss-Jordan elimination, solve the system of equations by putting the matrix into row echelon or reduced row echelon form.\n\\[1. \\begin{cases}\n               x + y + 2z = 2\\\\\n               3x - 2y + z = 1\\\\\n               y - z = 3\n            \\end{cases}\\]\n\\[2. \\begin{cases}\n               2x + 3y - z = -8\\\\\n               x + 2y - z = 12\\\\\n             -x -4y + z = -6\n            \\end{cases}\n              \\]"
  },
  {
    "objectID": "07_linear-algebra.html#rank-and-whether-a-system-has-one-infinite-or-no-solutions",
    "href": "07_linear-algebra.html#rank-and-whether-a-system-has-one-infinite-or-no-solutions",
    "title": "7  Linear Algebra",
    "section": "7.7 Rank — and Whether a System Has One, Infinite, or No Solutions",
    "text": "7.7 Rank — and Whether a System Has One, Infinite, or No Solutions\nTo determine how many solutions exist, we can use information about (1) the number of equations \\(m\\), (2) the number of unknowns \\(n\\), and (3) the rank of the matrix representing the linear system.\nRank: The maximum number of linearly independent row or column vectors in the matrix. This is equivalent to the number of nonzero rows of a matrix in row echelon form. For any matrix A, the row rank always equals column rank, and we refer to this number as the rank of A.\nFor example\n\\[\\begin{pmatrix} 1 & 2 & 3 \\\\\n            0 & 4 & 5 \\\\\n            0 & 0 & 6 \\end{pmatrix}\\]\nRank = 3\n\\[\\begin{pmatrix} 1 & 2 & 3 \\\\\n0 & 4 & 5 \\\\\n0 & 0 & 0 \\end{pmatrix}\\]\nRank = 2\n\nExercise 7.7 (Rank of Matrices) Find the rank of each matrix below:\n(Hint: transform the matrices into row echelon form. Remember that the number of nonzero rows of a matrix in row echelon form is the rank of that matrix)\n1.\\[\\begin{pmatrix} 1 & 1 & 2 \\\\\n2 & 1 & 3 \\\\\n1 & 2 & 3 \\end{pmatrix}\\]\n2.\\[\\begin{pmatrix} 1 & 3 & 3 & -3 & 3\\\\\n1 & 3 & 1 & 1 & 3 \\\\\n1 & 3 & 2 & -1 & -2 \\\\\n1 & 3 & 0 & 3 & -2 \\end{pmatrix}\\]"
  },
  {
    "objectID": "07_linear-algebra.html#the-inverse-of-a-matrix",
    "href": "07_linear-algebra.html#the-inverse-of-a-matrix",
    "title": "7  Linear Algebra",
    "section": "7.8 The Inverse of a Matrix",
    "text": "7.8 The Inverse of a Matrix\nIdentity Matrix: The \\(n\\times n\\) identity matrix \\({\\mathbf I}_n\\) is the matrix whose diagonal elements are 1 and all off-diagonal elements are 0. Examples: \\[ {\\mathbf I}_2=\\begin{pmatrix} 1&0\\\\0&1 \\end{pmatrix}, \\qquad {\\mathbf I}_3=\\begin{pmatrix} 1&0&0\\\\ 0&1&0\\\\\n            0&0&1 \\end{pmatrix}\\]\nInverse Matrix: An \\(n\\times n\\) matrix \\({\\mathbf A}\\) is nonsingular or invertible if there exists an \\(n\\times n\\) matrix \\({\\mathbf A}^{-1}\\) such that \\[{\\mathbf A} {\\mathbf A}^{-1} = {\\mathbf A}^{-1} {\\mathbf A} = {\\mathbf I}_n\\] where \\({\\mathbf A}^{-1}\\) is the inverse of \\({\\mathbf A}\\). If there is no such \\({\\mathbf A}^{-1}\\), then \\({\\mathbf A}\\) is singular or not invertible.\nExample: Let \\[{\\mathbf A} = \\begin{pmatrix} 2&3\\\\2&2 \\end{pmatrix}, \\qquad {\\mathbf B}=\\begin{pmatrix} -1&\\frac{3}{2}\\\\ 1&-1\n        \\end{pmatrix}\\] Since \\[{\\mathbf A} {\\mathbf B} = {\\mathbf B} {\\mathbf A} = {\\mathbf I}_n\\] we conclude that \\({\\mathbf B}\\) is the inverse, \\({\\mathbf A}^{-1}\\), of \\({\\mathbf A}\\) and that \\({\\mathbf A}\\) is nonsingular.\nProperties of the Inverse:\n\nIf the inverse exists, it is unique.\nIf \\({\\mathbf A}\\) is nonsingular, then \\({\\mathbf A}^{-1}\\) is nonsingular.\n\\(({\\mathbf A}^{-1})^{-1} = {\\mathbf A}\\)\nIf \\({\\mathbf A}\\) and \\({\\mathbf B}\\) are nonsingular, then \\({\\mathbf A}{\\mathbf B}\\) is nonsingular\n\\(({\\mathbf A}{\\mathbf B})^{-1} = {\\mathbf B}^{-1}{\\mathbf A}^{-1}\\)\nIf \\({\\mathbf A}\\) is nonsingular, then \\(({\\mathbf A}^T)^{-1}=({\\mathbf A}^{-1})^T\\)\n\nProcedure to Find \\({\\mathbf A}^{-1}\\): We know that if \\({\\mathbf B}\\) is the inverse of \\({\\mathbf A}\\), then \\[{\\mathbf A} {\\mathbf B} = {\\mathbf B} {\\mathbf A} = {\\mathbf I}_n\\] Looking only at the first and last parts of this \\[{\\mathbf A} {\\mathbf B} = {\\mathbf I}_n\\] Solving for \\({\\mathbf B}\\) is equivalent to solving for \\(n\\) linear systems, where each column of \\({\\mathbf B}\\) is solved for the corresponding column in \\({\\mathbf I}_n\\). We can solve the systems simultaneously by augmenting \\({\\mathbf A}\\) with \\({\\mathbf I}_n\\) and performing Gauss-Jordan elimination on \\({\\mathbf A}\\). If Gauss-Jordan elimination on \\([{\\mathbf A} | {\\mathbf I}_n]\\) results in \\([{\\mathbf I}_n | {\\mathbf B} ]\\), then \\({\\mathbf B}\\) is the inverse of \\({\\mathbf A}\\). Otherwise, \\({\\mathbf A}\\) is singular.\nTo summarize: To calculate the inverse of \\({\\mathbf A}\\)\n\nForm the augmented matrix \\([ {\\mathbf A} | {\\mathbf I}_n]\\)\nUsing elementary row operations, transform the augmented matrix to reduced row echelon form.\nThe result of step 2 is an augmented matrix \\([ {\\mathbf C} | {\\mathbf B} ]\\).\n\nIf \\({\\mathbf C}={\\mathbf I}_n\\), then \\({\\mathbf B}={\\mathbf A}^{-1}\\).\nIf \\({\\mathbf C}\\ne{\\mathbf I}_n\\), then \\(\\mathbf C\\) has a row of zeros. This means \\({\\mathbf A}\\) is singular and \\({\\mathbf A}^{-1}\\) does not exist.\n\n\n\nExample 7.7 (Matrix Inverse) Find the inverse of the following matricies:\n\n\\({\\mathbf A}=\\begin{pmatrix} 1&1&1\\\\0&2&3\\\\5&5&1 \\end{pmatrix}\\)\n\n\n\nExercise 7.8 (Matrix Inverse) Find the inverse of the following matrix:\n\n\\({\\mathbf A}=\\begin{pmatrix} 1&0&4\\\\0&2&0\\\\0&0&1 \\end{pmatrix}\\)"
  },
  {
    "objectID": "07_linear-algebra.html#linear-systems-and-inverses",
    "href": "07_linear-algebra.html#linear-systems-and-inverses",
    "title": "7  Linear Algebra",
    "section": "7.9 Linear Systems and Inverses",
    "text": "7.9 Linear Systems and Inverses\nLet’s return to the matrix representation of a linear system\n\\[\\mathbf{Ax} = \\mathbf{b}\\]\nIf \\(\\mathbf{A}\\) is an \\(n\\times n\\) matrix,then \\(\\mathbf{Ax}=\\mathbf{b}\\) is a system of \\(n\\) equations in \\(n\\) unknowns. Suppose \\(\\mathbf{A}\\) is nonsingular. Then \\(\\mathbf{A}^{-1}\\) exists. To solve this system, we can multiply each side by \\(\\mathbf{A}^{-1}\\) and reduce it as follows:\n\\[\\begin{eqnarray*}\n\\mathbf{A}^{-1} (\\mathbf{A} \\mathbf{x}) & = & \\mathbf{A}^{-1} \\mathbf{b} \\\\\n(\\mathbf{A}^{-1} \\mathbf{A})\\mathbf{x} & = & \\mathbf{A}^{-1} \\mathbf{b}\\\\\n\\mathbf{I}_n \\mathbf{x}     & = & \\mathbf{A}^{-1} \\mathbf{b}\\\\\n\\mathbf{x} & = & \\mathbf{A}^{-1} \\mathbf{b}\n\\end{eqnarray*}\\]\nHence, given \\(\\mathbf{A}\\) and \\(\\mathbf{b}\\) and given that \\(\\mathbf{A}\\) is nonsingular, then \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\) is a unique solution to this system.\n\nExercise 7.9 (Solve linear system using inverses) Use the inverse matrix to solve the following linear system:\n\\[\\begin{align*}\n-3x + 4y &= 5 \\\\\n2x - y &= -10\n\\end{align*}\\]\nHint: the linear system above can be written in the matrix form\n\\(\\textbf{A}\\textbf{z} = \\textbf{b}\\)\ngiven \\(\\textbf{A} = \\begin{pmatrix} -3&4\\\\2&-1 \\end{pmatrix}\\)\n\\(\\textbf{z} = \\begin{pmatrix} x\\\\y \\end{pmatrix}\\) and \\(\\textbf{b} = \\begin{pmatrix} 5\\\\-10 \\end{pmatrix}\\)"
  },
  {
    "objectID": "07_linear-algebra.html#determinants",
    "href": "07_linear-algebra.html#determinants",
    "title": "7  Linear Algebra",
    "section": "7.10 Determinants",
    "text": "7.10 Determinants\nSingularity: Determinants can be used to determine whether a square matrix is nonsingular.\nA square matrix is nonsingular if and only if its determinant is not zero.\nDeterminant of a \\(1 \\times 1\\) matrix, A, equals \\(a_{11}\\)\nDeterminant of a \\(2 \\times 2\\) matrix, A, \\(\\begin{vmatrix} a_{11}&a_{12}\\\\  a_{21}&a_{22} \\end{vmatrix}\\):\n\\[\\begin{eqnarray*}\n\\det({\\mathbf A}) &=& |{\\mathbf A}|\\\\\n            &=& a_{11}|a_{22}| - a_{12}|a_{21}|\\\\\n            &=& a_{11}a_{22} - a_{12}a_{21}\n\\end{eqnarray*}\\]\nWe can extend the second to last equation above to get the definition of the determinant of a \\(3 \\times 3\\) matrix:\n\\[\\begin{eqnarray*}\n            \\begin{vmatrix} a_{11}&a_{12}&a_{13}\\\\  a_{21} & a_{22}&a_{23}\\\\ a_{31}&a_{32}&a_{33} \\end{vmatrix}\n                &=&\n                a_{11} \\begin{vmatrix} a_{22}&a_{23}\\\\ a_{32}&a_{33} \\end{vmatrix}\n                - a_{12} \\begin{vmatrix} a_{21}&a_{23}\\\\ a_{31}&a_{33} \\end{vmatrix}\n                + a_{13} \\begin{vmatrix} a_{21}&a_{22}\\\\ a_{31}&a_{32}\n                \\end{vmatrix}\\\\\n                &=& a_{11}(a_{22}a_{33} - a_{23}a_{32}) - a_{12}(a_{21}a_{33} - a_{23}a_{31}) + a_{13}(a_{21}a_{32} - a_{22}a_{31})\n\\end{eqnarray*}\\]\nLet’s extend this now to any \\(n\\times n\\) matrix. Let’s define \\({\\mathbf A}_{ij}\\) as the \\((n-1)\\times (n-1)\\) submatrix of \\({\\mathbf A}\\) obtained by deleting row \\(i\\) and column \\(j\\). Let the \\((i,j)\\)th minor of \\({\\mathbf A}\\) be the determinant of \\({\\mathbf A}_{ij}\\): \\[M_{ij}=|{\\mathbf A}_{ij}|\\] Then for any \\(n\\times n\\) matrix \\({\\mathbf A}\\) \\[|{\\mathbf A}|= a_{11}M_{11} - a_{12}M_{12} + \\cdots + (-1)^{n+1} a_{1n} M_{1n}\\]\nFor example, in figuring out whether the following matrix has an inverse? \\[{\\mathbf A}=\\begin{pmatrix} 1&1&1\\\\0&2&3\\\\5&5&1 \\end{pmatrix}\\]\n\nCalculate its determinant. \\[\\begin{eqnarray}\n             &=& 1(2-15) - 1(0-15) + 1(0-10) \\nonumber\\\\\n             &=& -13+15-10 \\nonumber\\\\\n             &=& -8\\nonumber\n\\end{eqnarray}\\]\nSince \\(|{\\mathbf A}|\\ne 0\\), we conclude that \\({\\mathbf A}\\) has an inverse."
  },
  {
    "objectID": "07_linear-algebra.html#getting-inverse-of-a-matrix-using-its-determinant",
    "href": "07_linear-algebra.html#getting-inverse-of-a-matrix-using-its-determinant",
    "title": "7  Linear Algebra",
    "section": "8.1 Getting Inverse of a Matrix using its Determinant",
    "text": "8.1 Getting Inverse of a Matrix using its Determinant\nThus far, we have a number of algorithms to\n\nFind the solution of a linear system,\nFind the inverse of a matrix\n\nbut these remain just that — algorithms. At this point, we have no way of telling how the solutions \\(x_j\\) change as the parameters \\(a_{ij}\\) and \\(b_i\\) change, except by changing the values and “rerunning” the algorithms.\nWith determinants, we can provide an explicit formula for the inverse and therefore provide an explicit formula for the solution of an \\(n\\times n\\) linear system.\nHence, we can examine how changes in the parameters and \\(b_i\\) affect the solutions \\(x_j\\).\nDeterminant Formula for the Inverse of a \\(2 \\times 2\\):\nThe determinant of a \\(2 \\times 2\\) matrix A \\(\\begin{pmatrix} a & b\\\\ c & d\\\\ \\end{pmatrix}\\) is defined as: \\[\\frac{1}{\\det({\\mathbf A})} \\begin{pmatrix}\n            d & -b\\\\\n            -c & a\\\\\n        \\end{pmatrix}\\]\nFor example, Let’s calculate the inverse of matrix A from Exercise @ref(exr:invlinsys) using the determinant formula.\nRecall,\n\\[A = \\begin{pmatrix}\n            -3 & 4\\\\\n            2 & -1\\\\\n        \\end{pmatrix}\\]\n\\[\\det({\\mathbf A}) = (-3)(-1) - (4)(2) = 3 - 8  = -5\\]\n\\[\\frac{1}{\\det({\\mathbf A})} \\begin{pmatrix}\n            -1 & -4\\\\\n            -2 & -3\\\\\n        \\end{pmatrix}\\]\n\\[\\frac{1}{-5} \\begin{pmatrix}\n            -1 & -4\\\\\n            -2 & -3\\\\\n        \\end{pmatrix}\\]\n\\[\\begin{pmatrix}\n            \\frac{1}{5} & \\frac{4}{5}\\\\\n            \\frac{2}{5} & \\frac{3}{5}\\\\\n        \\end{pmatrix}\\]"
  },
  {
    "objectID": "11_data-handling_counting.html",
    "href": "11_data-handling_counting.html",
    "title": "8  Programming: Orientation and Reading in Data1",
    "section": "",
    "text": "The modal social science project starts by importing existing datasets. Datasets come in all shapes and sizes. As you search for new data you may encounter dozens of file extensions – csv, xlsx, dta, sav, por, Rdata, Rds, txt, xml, json, shp … the list continues. Although these files can often be cumbersome, its a good to be able to find a way to encounter any file that your research may call for.\nReviewing data import will allow us to get on the same page on how computer systems work.\n\n\nToday we’ll cover:\n\nWhat’s what in RStudio\nWhat R is, at a high level\nHow to read in data\nComment on coding style on the way\n\n\n\n\n\nWhat is the difference between a file and a folder?\nIn the RStudio windows, what is the difference between the “Source” Pane and the “Console”? What is a “code chunk”?\nHow do you read a R help page? What is the Usage section, the Values section, and the Examples section?\nWhat use is the “Environment” Pane?\nHow would you read in a spreadsheet in R?\nHow would you figure out what variables are in the data? size of the data?\nHow would you read in a csv file, a dta file, a sav file?"
  },
  {
    "objectID": "11_data-handling_counting.html#general-orientation",
    "href": "11_data-handling_counting.html#general-orientation",
    "title": "8  Programming: Orientation and Reading in Data1",
    "section": "8.1 General Orientation",
    "text": "8.1 General Orientation\n\nRStudio is a GUI and an IDE for the programming language R. A Graphical User Interface allows users to interface with the software (in this case R) using graphical aids like buttons and tabs. Often we don’t think of GUIs because to most computer users, everything is a GUI (like Microsoft Word or your “Control Panel”), but it’s always there! A Integrated Development Environment just says that the software to interface with R comes with useful useful bells and whistles to give you shortcuts.\n\nThe Console is kind of a the core window through which you see your GUI actually operating through R. It’s not graphical so might not be as intuitive. But all your results, commands, errors, warnings.. you see them in here. A console tells you what’s going on now.\n\nVia the GUI, you the analyst needs to send instructions, or commands, to the R application. The verb for this is “run” or “execute” the command. Computer programs ask users to provide instructions in very specific formats. While a English-speaking human can understand a sentence with a few typos in it by filling in the blanks, the same typo or misplaced character would halt a computer program. Each program has its own requirements for how commands should be typed; after all, each of these is its own language. We refer to the way a program needs its commands to be formatted as its syntax.\nTheoretically, one could do all their work by typing in commands into the Console. But that would be a lot of work, because you’d have to give instructions each time you start your data analysis. Moreover, you’ll have no record of what you did. That’s why you need a script. This is a type of code. It can be referred to as a source because that is the source of your commands. Source is also used as a verb; “source the script” just means execute it. RStudio doesn’t start out with a script, so you can make one from “File > New” or the New file icon.\nYou can also open scripts that are in folders in your computer. A script is a type of File. Find your Files in the bottom-right “Files” pane.\n\nTo load a dataset, you need to specify where that file is. Computer files (data, documents, programs) are organized hiearchically, like a branching tree. Folders can contain files, and also other folders. The GUI toolbar makes this lineaer and hiearchical relationship apparent. When we turn to locate the file in our commands, we need another set of syntax. Importantly, denote the hierarchy of a folder by the / (slash) symbol. data/input/2018-08 indicates the 2018-08 folder, which is included in the input folder, which is in turn included in the data folder.\nFiles (but not folders) have “file extensions” which you are probably familiar with already: .docx, .pdf, and .pdf. The file extensions you will see in a stats or quantitative social science class are:\n\n.pdf: PDF, a convenient format to view documents and slides in, regardless of Mac/Windows.\n.csv: A comma separated values file\n.xlsx: Microsoft Excel file\n.dta: Stata data\n.sav: SPSS data\n.R: R code (script)\n.Rmd: Rmarkdown code (text + code)\n.do: Stata code (script)\n\n\nIn R, there are two main types of scripts. A classic .R file and a .Rmd file (for Rmarkdown). A .R file is just lines and lines of R code that is meant to be inserted right into the Console. A .Rmd tries to weave code and English together, to make it easier for users to create reports that interact with data and intersperse R code with explanation.\n\nRmarkdown facilitates is the use of code chunks, which are used here. These start and end with three back-ticks. In the beginning, we can add options in curly braces ({}). Specifying r in the beginning tells to render it as R code. Options like echo = TRUE switch between showing the code that was executed or not; eval = TRUE switch between evaluating the code. More about Rmarkdown in later sections. For example, this code chunk would evaluate 1 + 1 and show its output when compiled, but not display the code that was executed."
  },
  {
    "objectID": "11_data-handling_counting.html#but-what-is-r",
    "href": "11_data-handling_counting.html#but-what-is-r",
    "title": "8  Programming: Orientation and Reading in Data1",
    "section": "8.2 But what is R",
    "text": "8.2 But what is R\nR is a programming language primarily used for statistical computing. It’s free, open source and has an extensive community that is constantly developing new tools and packages that extend its functionality in a lot of different ways (for example, the tidyverse project).\nOne feature of many programming languages is that they allow for “object-oriented” programming. In an object-oriented programming paradigm, we work with “objects” – some sort of structure that exists in the computer memory – that contains attributes and on which we can execute code (methods). R’s object-oriented support has some interesting quirks compared to other languages like C or Python, but\nEverything in R is an object, including its most basic data types. R has six basic data types:\n\ncharacter\nnumeric (real or decimal)\ninteger\nlogical\ncomplex\n\nThese data types make up the basic data structures of R\n\natomic vectors\nlists\nmatrix\ndata frame\nfactors\n\nBeyond that, many R routines"
  },
  {
    "objectID": "11_data-handling_counting.html#the-computer-and-you-giving-instructions",
    "href": "11_data-handling_counting.html#the-computer-and-you-giving-instructions",
    "title": "8  Programming: Orientation and Reading in Data1",
    "section": "8.3 The Computer and You: Giving Instructions",
    "text": "8.3 The Computer and You: Giving Instructions\nWe’ll do the Peanut Butter and Jelly Exercise in class as an introduction to programming for those who are new.\nAssignment: Take 5 minutes to write down on a piece of paper, how to make a peanut butter and jelly sandwich. Be as concise and unambiguous as possible so that a robot (who doesn’t know what a PBJ is) would understand. You can assume that there will be loaf of sliced bread, a jar of jelly, a jar of peanut butter, and a knife.\nSimpler assignment: Say we just want a robot to be able to tell us if we have enough ingredients to make a peanut butter and jelly sandwich. Write down instructions so that if told how many slices of bread, servings of peanut butter, and servings of jelly you have, the robot can tell you if you can make a PBJ.\nNow, translate the simpler assignment into R code using the code below as a starting point:\n\nn_bread <- 8\nn_pb <- 3\nn_jelly <- 9\n\n# write instructions in R here"
  },
  {
    "objectID": "11_data-handling_counting.html#base-r-vs.-tidyverse",
    "href": "11_data-handling_counting.html#base-r-vs.-tidyverse",
    "title": "8  Programming: Orientation and Reading in Data1",
    "section": "8.4 Base-R vs. tidyverse",
    "text": "8.4 Base-R vs. tidyverse\nOne last thing before we jump into data. Many things in R and other open source packages have competing standards. A lecture on a technique inevitably biases one standard over another. Right now among R users in this area, there are two families of functions: base-R and tidyverse. R instructors thus face a dilemma about which to teach primarily.2\nIn this prefresher, we try our best to choose the one that is most useful to the modal task of social science researchers, and make use of the tidyverse functions in most applications. but feel free to suggest changes to us or to the booklet.\nAlthough you do not need to choose one over the other, for beginners it is confusing what is a tidyverse function and what is not. Many of the tidyverse packages are covered in this 2017 graphic below, and the cheat-sheets that other programmers have written: https://www.rstudio.com/resources/cheatsheets/\n\n\n\nNames of Packages in the tidyverse Family\n\n\nThe following side-by-side comparison of commands for a particular function compares some tidyverse and non-tidyverse functions (which we refer to loosely as base-R). This list is not meant to be comprehensive and more to give you a quick rule of thumb.\n\nDataframe subsetting\n\n\n\n\n\n\n\n\nIn order to …\nin tidyverse:\nin base-R:\n\n\n\n\nCount each category\ncount(df, var)\ntable(df$var)\n\n\nFilter rows by condition\nfilter(df, var == \"Female\")\ndf[df$var == \"Female\", ] or subset(df, var == \"Female\")\n\n\nExtract columns\nselect(df, var1, var2)\ndf[, c(\"var1\", \"var2\")]\n\n\nExtract a single column as a vector\npull(df, var)\ndf[[\"var\"]] or df[, \"var\"]\n\n\nCombine rows\nbind_rows()\nrbind()\n\n\nCombine columns\nbind_cols()\ncbind()\n\n\nCreate a dataframe\ntibble(x = vec1, y = vec2)\ndata.frame(x = vec1, y = vec2)\n\n\nTurn a dataframe into a tidyverse dataframe\ntbl_df(df)\n\n\n\n\nRemember that tidyverse applies to dataframes only, not vectors. For subsetting vectors, use the base-R functions with the square brackets.\n\n\nRead data\nSome non-tidyverse functions are not quite “base-R” but have similar relationships to tidyverse. For these, we recommend using the tidyverse functions as a general rule due to their common format, simplicity, and scalability.\n\n\n\n\n\n\n\n\nIn order to …\nin tidyverse:\nin base-R:\n\n\n\n\nRead a Excel file\nread_excel()\nread.xlsx()\n\n\nRead a csv\nread_csv()\nread.csv()\n\n\nRead a Stata file\nread_dta()\nread.dta()\n\n\nSubstitute strings\nstr_replace()\ngsub()\n\n\nReturn matching strings\nstr_subset()\ngrep(., value = TRUE)\n\n\nMerge data1 and data2 on variables x1 and x2\nleft_join(data1, data2, by = c(\"x1\", \"x2\"))\nmerge(data1, data2, by.x = \"x1\", by.y = \"x2\", all.x = TRUE)\n\n\n\n\n\nVisualization\nPlotting by ggplot2 (from your tutorials) is also a tidyverse family.\n\n\n\n\n\n\n\n\nIn order to …\nin tidyverse:\nin base-R:\n\n\n\n\nMake a scatter plot\nggplot(data, aes(x, y)) + geom_point()\nplot(data$x, data$y)\n\n\nMake a line plot\nggplot(data, aes(x, y)) + geom_line()\nplot(data$x, data$y, type = \"l\")\n\n\nMake a histogram\nggplot(data, aes(x, y)) + geom_histogram()\nhist(data$x, data$y)"
  },
  {
    "objectID": "11_data-handling_counting.html#a-is-for-athens",
    "href": "11_data-handling_counting.html#a-is-for-athens",
    "title": "8  Programming: Orientation and Reading in Data1",
    "section": "8.5 A is for Athens",
    "text": "8.5 A is for Athens\nFor our first dataset, let’s try reading in a dataset on the Ancient Greek world. Political Theorists and Political Historians study the domestic systems, international wars, cultures and writing of this era to understand the first instance of democracy, the rise and overturning of tyranny, and the legacies of political institutions.\nThis POLIS dataset was generously provided by Professor Josiah Ober of Stanford University. This dataset includes information on city states in the Ancient Greek world, parts of it collected by careful work by historians and archaeologists. It is part of his recent books on Greece (Ober 2015), “The Rise and Fall of Classical Greece”3 and Institutions in Ancient Athens (Ober 2010) , “Democracy and Knowledge: Innovation and Learning in Classical Athens.”4\n\n8.5.1 Locating the Data\nWhat files do we have in the data/input folder?\n\n\ndata/input/CES Guide 2022.pdf           data/input/Nunn_Wantchekon_AER_2011.dta \ndata/input/Nunn_Wantchekon_sample.dta   data/input/acs2015_1percent.csv         \ndata/input/ces22_subset.dta             data/input/gapminder_wide.Rds           \ndata/input/gapminder_wide.tab           data/input/german_credit.sav            \ndata/input/justices_court-median.csv    data/input/ober_2018.xlsx               \ndata/input/sample_mid.csv               data/input/sample_polity.csv            \ndata/input/upshot-siena-polls.csv       data/input/usc2010_001percent.Rds       \ndata/input/usc2010_001percent.csv       \n\n\nA typical file format is Microsoft Excel. Although this is not usually the best format for R because of its highly formatted structure as opposed to plain text, recent packages have made this fairly easy.\n\n\n8.5.2 Reading in Data\nIn Rstudio, a good way to start is to use the GUI and the Import tool. Once you click a file, an option to “Import Dataset” comes up. RStudio picks the right function for you, and you can copy that code, but it’s important to eventually be able to write that code yourself.\nFor the first time using an outside package, you first need to install it.\n\ninstall.packages(\"readxl\")\n\nAfter that, you don’t need to install it again. But you do need to load it each time.\n\nlibrary(readxl)\n\nThe package readxl has a website: https://readxl.tidyverse.org/. Other packages are not as user-friendly, but they have a help page with a table of contents of all their functions.\n\nhelp(package = readxl)\n\nFrom the help page, we see that read_excel() is the function that we want to use.\nLet’s try it.\n\nlibrary(readxl)\nober <- read_excel(\"data/input/ober_2018.xlsx\")\n\nReview: what does the / mean? Why do we need the data term first? Does the argument need to be in quotes?\n\n\n8.5.3 Inspecting\nFor almost any dataset, you usually want to do a couple of standard checks first to understand what you loaded.\n\nober\n\n# A tibble: 1,035 × 10\n   polis_number Name  Latitude Longitude Hellenicity  Fame Size  Colonies Regime\n          <dbl> <chr>    <dbl>     <dbl> <chr>       <dbl> <chr>    <dbl> <chr> \n 1            1 Alal…     42.1      9.51 most Greek   1.12 100-…        0 <NA>  \n 2            2 Empo…     42.1      3.11 most barba…  2.12 25-1…        0 <NA>  \n 3            3 Mass…     43.3      5.38 most Greek   4    25-1…        2 no ev…\n 4            4 Rhode     42.3      3.17 most Greek   0.87 <NA>         0 <NA>  \n 5            5 Abak…     38.1     15.1  most barba…  1    <NA>         0 <NA>  \n 6            6 Adra…     37.7     14.8  most Greek   1    <NA>         0 <NA>  \n 7            7 Agyr…     37.7     14.5  most Greek   1.25 <NA>         0 no ev…\n 8            8 Aitna     38.2     15.6  most Greek   3.25 200-…        1 no ev…\n 9            9 Akra…     37.3     13.6  most Greek   6.37 500 …        0 evide…\n10           10 Akrai     37.1     14.9  most Greek   1.25 <NA>         0 <NA>  \n# ℹ 1,025 more rows\n# ℹ 1 more variable: Delian <chr>\n\n\n\ndim(ober)\n\n[1] 1035   10\n\n\nGraphics are useful for grasping your data - we will cover them more deeply later on.\n\nggplot(ober, aes(x = Fame)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWhat about the distribution of fame by regime?\n\nggplot(ober, aes(y = Fame, x = Regime, group = Regime)) +\n  geom_boxplot()\n\n\n\n\nWhat do the 1’s, 2’s, and 3’s stand for?\n\n\n8.5.4 Finding observations\nThese tidyverse commands from the dplyr package are newer and not built-in, but they are one of the increasingly more popular ways to wrangle data.\n\n80 percent of your data wrangling needs might be doable with these basic dplyr functions: select, mutate, group_by, summarize, and arrange.\nThese verbs roughly correspond to the same commands in SQL, another important language in data science.\nThe %>% symbol is a pipe. It takes the thing on the left side and pipes it down to the function on the right side. We could have done count(cen10, race) as cen10 %>% count(race). That means take cen10 and pass it on to the function count, which will count observations by race and return a collapsed dataset with the categories in its own variable and their respective counts in n."
  },
  {
    "objectID": "11_data-handling_counting.html#exercises",
    "href": "11_data-handling_counting.html#exercises",
    "title": "8  Programming: Orientation and Reading in Data1",
    "section": "Exercises",
    "text": "Exercises\n\n1\nWhat is the Fame value of Delphoi?\n\n# Enter here\n\n\n\n2\nFind the polis with the top 10 Fame values.\n\n# Enter here\n\n\n\n3\nMake a scatterplot with the number of colonies on the x-axis and Fame on the y-axis.\n\n# Enter here\n\n\n\n4\nFind the correct function to read the following datasets into your R instance.\n\ndata/input/acs2015_1percent.csv: A one percent sample of the American Community Survey\ndata/input/gapminder_wide.tab: Country-level wealth and health from Gapminder5\ndata/input/gapminder_wide.Rds: A Rds version of the Gapminder (What is a Rds file? What’s the difference?)\ndata/input/Nunn_Wantchekon_sample.dta: A sample from the Afrobarometer survey (which we’ll explore tomorrow). .dta is a Stata format.\ndata/input/german_credit.sav: A hypothetical dataset on consumer credit. .sav is a SPSS format.\n\nOur Recommendations: Look at the packages haven and readr\n\n# Enter here, perhaps making a chunk for each file.\n\n\n\n5\nRead Ober’s codebook and find a variable that you think is interesting. Check the distribution of that variable in your data, get a couple of statistics, and summarize it in English.\n\n# Enter here"
  },
  {
    "objectID": "12_matricies-manipulation.html",
    "href": "12_matricies-manipulation.html",
    "title": "9  Programming: Manipulating Vectors and Matrices1",
    "section": "",
    "text": "Nunn and Wantchekon (2011) – “The Slave Trade and the Origins of Mistrust in Africa”2 – argues that across African countries, the distrust of co-ethnics fueled by the slave trade has had long-lasting effects on modern day trust in these territories. They argued that the slave trade created distrust in these societies in part because as some African groups were employed by European traders to capture their neighbors and bring them to the slave ships.\nNunn and Wantchekon use a variety of statistical tools to make their case (adding controls, ordered logit, instrumental variables, falsification tests, causal mechanisms), many of which will be covered in future courses. In this module we will only touch on their first set of analysis that use Ordinary Least Squares (OLS). OLS is likely the most common application of linear algebra in the social sciences. We will cover some linear algebra, matrix manipulation, and vector manipulation from this data."
  },
  {
    "objectID": "12_matricies-manipulation.html#read-data",
    "href": "12_matricies-manipulation.html#read-data",
    "title": "9  Programming: Manipulating Vectors and Matrices1",
    "section": "9.1 Read Data",
    "text": "9.1 Read Data\n\nlibrary(haven)\nnunn_full <- read_dta(\"data/input/Nunn_Wantchekon_AER_2011.dta\")\n\nNunn and Wantchekon’s main dataset has more than 20,000 observations. Each observation is a respondent from the Afrobarometer survey.\n\nhead(nunn_full)\n\n# A tibble: 6 × 59\n  respno  ethnicity murdock_name isocode region    district townvill location_id\n  <chr>   <chr>     <chr>        <chr>   <chr>     <chr>    <chr>          <dbl>\n1 BEN0001 fon       FON          BEN     atlnatiq… KPOMASSE TOKPA-D…          30\n2 BEN0002 fon       FON          BEN     atlnatiq… KPOMASSE TOKPA-D…          30\n3 BEN0003 fon       FON          BEN     atlnatiq… OUIDAH   3ARROND           31\n4 BEN0004 fon       FON          BEN     atlnatiq… OUIDAH   3ARROND           31\n5 BEN0005 fon       FON          BEN     atlnatiq… OUIDAH   PAHOU             32\n6 BEN0006 fon       FON          BEN     atlnatiq… OUIDAH   PAHOU             32\n# ℹ 51 more variables: trust_relatives <dbl>, trust_neighbors <dbl>,\n#   intra_group_trust <dbl>, inter_group_trust <dbl>,\n#   trust_local_council <dbl>, ln_export_area <dbl>, export_area <dbl>,\n#   export_pop <dbl>, ln_export_pop <dbl>, age <dbl>, age2 <dbl>, male <dbl>,\n#   urban_dum <dbl>, occupation <dbl>, religion <dbl>, living_conditions <dbl>,\n#   education <dbl>, near_dist <dbl>, distsea <dbl>, loc_murdock_name <chr>,\n#   loc_ln_export_area <dbl>, local_council_performance <dbl>, …\n\ncolnames(nunn_full)\n\n [1] \"respno\"                          \"ethnicity\"                      \n [3] \"murdock_name\"                    \"isocode\"                        \n [5] \"region\"                          \"district\"                       \n [7] \"townvill\"                        \"location_id\"                    \n [9] \"trust_relatives\"                 \"trust_neighbors\"                \n[11] \"intra_group_trust\"               \"inter_group_trust\"              \n[13] \"trust_local_council\"             \"ln_export_area\"                 \n[15] \"export_area\"                     \"export_pop\"                     \n[17] \"ln_export_pop\"                   \"age\"                            \n[19] \"age2\"                            \"male\"                           \n[21] \"urban_dum\"                       \"occupation\"                     \n[23] \"religion\"                        \"living_conditions\"              \n[25] \"education\"                       \"near_dist\"                      \n[27] \"distsea\"                         \"loc_murdock_name\"               \n[29] \"loc_ln_export_area\"              \"local_council_performance\"      \n[31] \"council_listen\"                  \"corrupt_local_council\"          \n[33] \"school_present\"                  \"electricity_present\"            \n[35] \"piped_water_present\"             \"sewage_present\"                 \n[37] \"health_clinic_present\"           \"district_ethnic_frac\"           \n[39] \"frac_ethnicity_in_district\"      \"townvill_nonethnic_mean_exports\"\n[41] \"district_nonethnic_mean_exports\" \"region_nonethnic_mean_exports\"  \n[43] \"country_nonethnic_mean_exports\"  \"murdock_centr_dist_coast\"       \n[45] \"centroid_lat\"                    \"centroid_long\"                  \n[47] \"explorer_contact\"                \"railway_contact\"                \n[49] \"dist_Saharan_node\"               \"dist_Saharan_line\"              \n[51] \"malaria_ecology\"                 \"v30\"                            \n[53] \"v33\"                             \"fishing\"                        \n[55] \"exports\"                         \"ln_exports\"                     \n[57] \"total_missions_area\"             \"ln_init_pop_density\"            \n[59] \"cities_1400_dum\"                \n\n\nFirst, let’s consider a small subset of this dataset.\n\nnunn <- read_dta(\"data/input/Nunn_Wantchekon_sample.dta\")\n\n\nnunn\n\n# A tibble: 10 × 5\n   trust_neighbors exports ln_exports export_area ln_export_area\n             <dbl>   <dbl>      <dbl>       <dbl>          <dbl>\n 1               3   0.388      0.328     0.00407        0.00406\n 2               3   0.631      0.489     0.0971         0.0926 \n 3               3   0.994      0.690     0.0125         0.0124 \n 4               0 183.         5.21      1.82           1.04   \n 5               3   0          0         0              0      \n 6               2   0          0         0              0      \n 7               2 666.         6.50     14.0            2.71   \n 8               0   0.348      0.298     0.00608        0.00606\n 9               3   0.435      0.361     0.0383         0.0376 \n10               3   0          0         0              0"
  },
  {
    "objectID": "12_matricies-manipulation.html#data.frame-vs.-matricies",
    "href": "12_matricies-manipulation.html#data.frame-vs.-matricies",
    "title": "9  Programming: Manipulating Vectors and Matrices1",
    "section": "9.2 data.frame vs. matricies",
    "text": "9.2 data.frame vs. matricies\nThis is a data.frame object.\n\nclass(nunn)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nBut it can be also consider a matrix in the linear algebra sense. What are the dimensions of this matrix?\n\nnrow(nunn)\n\n[1] 10\n\n\ndata.frames and matrices have much overlap in R, but to explicitly treat an object as a matrix, you’d need to coerce its class. Let’s call this matrix X.\n\nX <- as.matrix(nunn)\n\nWhat is the difference between a data.frame and a matrix? A data.frame can have columns that are of different types, whereas — in a matrix — all columns must be of the same type (usually either “numeric” or “character”).\nYou can think of data frames maybe as matrices-plus, because a column can take on characters as well as numbers. As we just saw, this is often useful for real data analyses.\nAnother way to think about data frames is that it is a type of list. Try the str() code below and notice how it is organized in slots. Each slot is a vector. They can be vectors of numbers or characters.\n\n# enter this on your console\nstr(cen10)"
  },
  {
    "objectID": "12_matricies-manipulation.html#handling-matricies-in-r",
    "href": "12_matricies-manipulation.html#handling-matricies-in-r",
    "title": "9  Programming: Manipulating Vectors and Matrices1",
    "section": "9.3 Handling matricies in R",
    "text": "9.3 Handling matricies in R\nYou can easily transpose a matrix\n\nX\n\n      trust_neighbors     exports ln_exports  export_area ln_export_area\n [1,]               3   0.3883497  0.3281158  0.004067405    0.004059155\n [2,]               3   0.6311236  0.4892691  0.097059444    0.092633367\n [3,]               3   0.9941893  0.6902376  0.012524694    0.012446908\n [4,]               0 182.5891266  5.2127004  1.824284434    1.038255095\n [5,]               3   0.0000000  0.0000000  0.000000000    0.000000000\n [6,]               2   0.0000000  0.0000000  0.000000000    0.000000000\n [7,]               2 665.9652100  6.5027380 13.975566864    2.706419945\n [8,]               0   0.3476418  0.2983562  0.006082553    0.006064130\n [9,]               3   0.4349871  0.3611559  0.038332380    0.037615947\n[10,]               3   0.0000000  0.0000000  0.000000000    0.000000000\n\nt(X)\n\n                       [,1]       [,2]       [,3]       [,4] [,5] [,6]\ntrust_neighbors 3.000000000 3.00000000 3.00000000   0.000000    3    2\nexports         0.388349682 0.63112360 0.99418926 182.589127    0    0\nln_exports      0.328115761 0.48926911 0.69023758   5.212700    0    0\nexport_area     0.004067405 0.09705944 0.01252469   1.824284    0    0\nln_export_area  0.004059155 0.09263337 0.01244691   1.038255    0    0\n                      [,7]        [,8]       [,9] [,10]\ntrust_neighbors   2.000000 0.000000000 3.00000000     3\nexports         665.965210 0.347641766 0.43498713     0\nln_exports        6.502738 0.298356235 0.36115587     0\nexport_area      13.975567 0.006082553 0.03833238     0\nln_export_area    2.706420 0.006064130 0.03761595     0\n\n\nWhat are the values of all rows in the first column?\n\nX[, 1]\n\n [1] 3 3 3 0 3 2 2 0 3 3\n\n\nWhat are all the values of “exports”? (i.e. return the whole “exports” column)\n\nX[, \"exports\"]\n\n [1]   0.3883497   0.6311236   0.9941893 182.5891266   0.0000000   0.0000000\n [7] 665.9652100   0.3476418   0.4349871   0.0000000\n\n\nWhat is the first observation (i.e. first row)?\n\nX[1, ]\n\ntrust_neighbors         exports      ln_exports     export_area  ln_export_area \n    3.000000000     0.388349682     0.328115761     0.004067405     0.004059155 \n\n\nWhat is the value of the first variable of the first observation?\n\nX[1, 1]\n\ntrust_neighbors \n              3 \n\n\nPause and consider the following problem on your own. What is the following code doing?\n\nX[X[, \"trust_neighbors\"] == 0, \"export_area\"]\n\n[1] 1.824284434 0.006082553\n\n\nWhy does it give the same output as the following?\n\nX[which(X[, \"trust_neighbors\"] == 0), \"export_area\"]\n\n[1] 1.824284434 0.006082553\n\n\nSome more manipulation\n\nX + X\n\n      trust_neighbors      exports ln_exports  export_area ln_export_area\n [1,]               6    0.7766994  0.6562315  0.008134809     0.00811831\n [2,]               6    1.2622472  0.9785382  0.194118887     0.18526673\n [3,]               6    1.9883785  1.3804752  0.025049388     0.02489382\n [4,]               0  365.1782532 10.4254007  3.648568869     2.07651019\n [5,]               6    0.0000000  0.0000000  0.000000000     0.00000000\n [6,]               4    0.0000000  0.0000000  0.000000000     0.00000000\n [7,]               4 1331.9304199 13.0054760 27.951133728     5.41283989\n [8,]               0    0.6952835  0.5967125  0.012165107     0.01212826\n [9,]               6    0.8699743  0.7223117  0.076664761     0.07523189\n[10,]               6    0.0000000  0.0000000  0.000000000     0.00000000\n\n\n\nX - X\n\n      trust_neighbors exports ln_exports export_area ln_export_area\n [1,]               0       0          0           0              0\n [2,]               0       0          0           0              0\n [3,]               0       0          0           0              0\n [4,]               0       0          0           0              0\n [5,]               0       0          0           0              0\n [6,]               0       0          0           0              0\n [7,]               0       0          0           0              0\n [8,]               0       0          0           0              0\n [9,]               0       0          0           0              0\n[10,]               0       0          0           0              0\n\n\n\nt(X) %*% X\n\n                trust_neighbors    exports ln_exports export_area\ntrust_neighbors       62.000000   1339.276   18.61181    28.40709\nexports             1339.276369 476850.298 5283.76294  9640.42990\nln_exports            18.611811   5283.763   70.50077   100.46202\nexport_area           28.407085   9640.430  100.46202   198.65558\nln_export_area         5.853106   1992.047   23.08189    39.72847\n                ln_export_area\ntrust_neighbors       5.853106\nexports            1992.046502\nln_exports           23.081893\nexport_area          39.728468\nln_export_area        8.412887\n\n\n\ncbind(X, 1:10)\n\n      trust_neighbors     exports ln_exports  export_area ln_export_area   \n [1,]               3   0.3883497  0.3281158  0.004067405    0.004059155  1\n [2,]               3   0.6311236  0.4892691  0.097059444    0.092633367  2\n [3,]               3   0.9941893  0.6902376  0.012524694    0.012446908  3\n [4,]               0 182.5891266  5.2127004  1.824284434    1.038255095  4\n [5,]               3   0.0000000  0.0000000  0.000000000    0.000000000  5\n [6,]               2   0.0000000  0.0000000  0.000000000    0.000000000  6\n [7,]               2 665.9652100  6.5027380 13.975566864    2.706419945  7\n [8,]               0   0.3476418  0.2983562  0.006082553    0.006064130  8\n [9,]               3   0.4349871  0.3611559  0.038332380    0.037615947  9\n[10,]               3   0.0000000  0.0000000  0.000000000    0.000000000 10\n\n\n\ncbind(X, 1)\n\n      trust_neighbors     exports ln_exports  export_area ln_export_area  \n [1,]               3   0.3883497  0.3281158  0.004067405    0.004059155 1\n [2,]               3   0.6311236  0.4892691  0.097059444    0.092633367 1\n [3,]               3   0.9941893  0.6902376  0.012524694    0.012446908 1\n [4,]               0 182.5891266  5.2127004  1.824284434    1.038255095 1\n [5,]               3   0.0000000  0.0000000  0.000000000    0.000000000 1\n [6,]               2   0.0000000  0.0000000  0.000000000    0.000000000 1\n [7,]               2 665.9652100  6.5027380 13.975566864    2.706419945 1\n [8,]               0   0.3476418  0.2983562  0.006082553    0.006064130 1\n [9,]               3   0.4349871  0.3611559  0.038332380    0.037615947 1\n[10,]               3   0.0000000  0.0000000  0.000000000    0.000000000 1\n\n\n\ncolnames(X)\n\n[1] \"trust_neighbors\" \"exports\"         \"ln_exports\"      \"export_area\"    \n[5] \"ln_export_area\""
  },
  {
    "objectID": "12_matricies-manipulation.html#variable-transformations",
    "href": "12_matricies-manipulation.html#variable-transformations",
    "title": "9  Programming: Manipulating Vectors and Matrices1",
    "section": "9.4 Variable Transformations",
    "text": "9.4 Variable Transformations\nexports is the total number of slaves that were taken from the individual’s ethnic group between Africa’s four slave trades between 1400-1900.\nWhat is ln_exports? The article describes this as the natural log of one plus the exports. This is a transformation of one column by a particular function\n\nlog(1 + X[, \"exports\"])\n\n [1] 0.3281158 0.4892691 0.6902376 5.2127003 0.0000000 0.0000000 6.5027379\n [8] 0.2983562 0.3611559 0.0000000\n\n\nQuestion for you: why add the 1?\nVerify that this is the same as X[, \"ln_exports\"]"
  },
  {
    "objectID": "12_matricies-manipulation.html#linear-combinations",
    "href": "12_matricies-manipulation.html#linear-combinations",
    "title": "9  Programming: Manipulating Vectors and Matrices1",
    "section": "9.5 Linear Combinations",
    "text": "9.5 Linear Combinations\nIn Table 1 we see “OLS Estimates”. These are estimates of OLS coefficients and standard errors. You do not need to know what these are for now, but it doesn’t hurt to getting used to seeing them.\n\nA very crude way to describe regression is through linear combinations. The simplest linear combination is a one-to-one transformation.\nTake the first number in Table 1, which is -0.00068. Now, multiply this by exports\n\n-0.00068 * X[, \"exports\"]\n\n [1] -0.0002640778 -0.0004291640 -0.0006760487 -0.1241606061  0.0000000000\n [6]  0.0000000000 -0.4528563428 -0.0002363964 -0.0002957912  0.0000000000\n\n\nNow, just one more step. Make a new matrix with just exports and the value 1\n\nX2 <- cbind(1, X[, \"exports\"])\n\nname this new column “intercept”\n\ncolnames(X2)\n\nNULL\n\n\n\ncolnames(X2) <- c(\"intercept\", \"exports\")\n\nWhat are the dimensions of the matrix X2?\n\ndim(X2)\n\n[1] 10  2\n\n\nNow consider a new matrix, called B.\n\nB <- matrix(c(1.62, -0.00068))\n\nWhat are the dimensions of B?\n\ndim(B)\n\n[1] 2 1\n\n\nWhat is the product of X2 and B? From the dimensions, can you tell if it will be conformable?\n\nX2 %*% B\n\n          [,1]\n [1,] 1.619736\n [2,] 1.619571\n [3,] 1.619324\n [4,] 1.495839\n [5,] 1.620000\n [6,] 1.620000\n [7,] 1.167144\n [8,] 1.619764\n [9,] 1.619704\n[10,] 1.620000\n\n\nWhat is this multiplication doing in terms of equations?"
  },
  {
    "objectID": "12_matricies-manipulation.html#matrix-basics",
    "href": "12_matricies-manipulation.html#matrix-basics",
    "title": "9  Programming: Manipulating Vectors and Matrices1",
    "section": "9.6 Matrix Basics",
    "text": "9.6 Matrix Basics\nLet’s take a look at Matrices in the context of R\n\ncen10 <- read_csv(\"data/input/usc2010_001percent.csv\")\nhead(cen10)\n\n# A tibble: 6 × 4\n  state         sex      age race       \n  <chr>         <chr>  <dbl> <chr>      \n1 New York      Female     8 White      \n2 Ohio          Male      24 White      \n3 Nevada        Male      37 White      \n4 Michigan      Female    12 White      \n5 Maryland      Female    18 Black/Negro\n6 New Hampshire Male      50 White      \n\n\nWhat is the dimension of this dataframe? What does the number of rows represent? What does the number of columns represent?\n\ndim(cen10)\n\n[1] 30871     4\n\nnrow(cen10)\n\n[1] 30871\n\nncol(cen10)\n\n[1] 4\n\n\nWhat variables does this dataset hold? What kind of information does it have?\n\ncolnames(cen10)\n\n[1] \"state\" \"sex\"   \"age\"   \"race\" \n\n\nWe can access column vectors, or vectors that contain values of variables by using the $ sign\n\nhead(cen10$state)\n\n[1] \"New York\"      \"Ohio\"          \"Nevada\"        \"Michigan\"     \n[5] \"Maryland\"      \"New Hampshire\"\n\nhead(cen10$race)\n\n[1] \"White\"       \"White\"       \"White\"       \"White\"       \"Black/Negro\"\n[6] \"White\"      \n\n\nWe can look at a unique set of variable values by calling the unique function\n\nunique(cen10$state)\n\n [1] \"New York\"             \"Ohio\"                 \"Nevada\"              \n [4] \"Michigan\"             \"Maryland\"             \"New Hampshire\"       \n [7] \"Iowa\"                 \"Missouri\"             \"New Jersey\"          \n[10] \"California\"           \"Texas\"                \"Pennsylvania\"        \n[13] \"Washington\"           \"West Virginia\"        \"Idaho\"               \n[16] \"North Carolina\"       \"Massachusetts\"        \"Connecticut\"         \n[19] \"Arkansas\"             \"Indiana\"              \"Wisconsin\"           \n[22] \"Maine\"                \"Tennessee\"            \"Minnesota\"           \n[25] \"Florida\"              \"Oklahoma\"             \"Montana\"             \n[28] \"Georgia\"              \"Arizona\"              \"Colorado\"            \n[31] \"Virginia\"             \"Illinois\"             \"Oregon\"              \n[34] \"Kentucky\"             \"South Carolina\"       \"Kansas\"              \n[37] \"Louisiana\"            \"Alabama\"              \"District of Columbia\"\n[40] \"Mississippi\"          \"Utah\"                 \"Delaware\"            \n[43] \"Nebraska\"             \"Alaska\"               \"New Mexico\"          \n[46] \"South Dakota\"         \"Hawaii\"               \"Vermont\"             \n[49] \"Rhode Island\"         \"Wyoming\"              \"North Dakota\"        \n\n\nHow many different states are represented (this dataset includes DC as a state)?\n\nlength(unique(cen10$state))\n\n[1] 51\n\n\nMatrices are rectangular structures of numbers (they have to be numbers, and they can’t be characters).\nA cross-tab can be considered a matrix:\n\ntable(cen10$race, cen10$sex)\n\n                                  \n                                   Female  Male\n  American Indian or Alaska Native    142   153\n  Black/Negro                        2070  1943\n  Chinese                             192   162\n  Japanese                             51    26\n  Other Asian or Pacific Islander     587   542\n  Other race, nec                     877   962\n  Three or more major races            37    51\n  Two major races                     443   426\n  White                             11252 10955\n\n\n\ncross_tab <- table(cen10$race, cen10$sex)\ndim(cross_tab)\n\n[1] 9 2\n\ncross_tab[6, 2]\n\n[1] 962\n\n\nBut a subset of your data – individual values– can be considered a matrix too.\n\n# First 20 rows of the entire data\n# Below two lines of code do the same thing\ncen10[1:20, ]\n\n# A tibble: 20 × 4\n   state         sex      age race           \n   <chr>         <chr>  <dbl> <chr>          \n 1 New York      Female     8 White          \n 2 Ohio          Male      24 White          \n 3 Nevada        Male      37 White          \n 4 Michigan      Female    12 White          \n 5 Maryland      Female    18 Black/Negro    \n 6 New Hampshire Male      50 White          \n 7 Iowa          Female    51 White          \n 8 Missouri      Female    41 White          \n 9 New Jersey    Male      62 White          \n10 California    Male      25 White          \n11 Texas         Female    23 White          \n12 Pennsylvania  Female    66 White          \n13 California    Female    57 White          \n14 Texas         Female    73 Other race, nec\n15 California    Male      43 White          \n16 Washington    Male      29 White          \n17 Texas         Male       8 White          \n18 Missouri      Male      78 White          \n19 West Virginia Male      10 White          \n20 Idaho         Female     9 White          \n\ncen10 %>% slice(1:20)\n\n# A tibble: 20 × 4\n   state         sex      age race           \n   <chr>         <chr>  <dbl> <chr>          \n 1 New York      Female     8 White          \n 2 Ohio          Male      24 White          \n 3 Nevada        Male      37 White          \n 4 Michigan      Female    12 White          \n 5 Maryland      Female    18 Black/Negro    \n 6 New Hampshire Male      50 White          \n 7 Iowa          Female    51 White          \n 8 Missouri      Female    41 White          \n 9 New Jersey    Male      62 White          \n10 California    Male      25 White          \n11 Texas         Female    23 White          \n12 Pennsylvania  Female    66 White          \n13 California    Female    57 White          \n14 Texas         Female    73 Other race, nec\n15 California    Male      43 White          \n16 Washington    Male      29 White          \n17 Texas         Male       8 White          \n18 Missouri      Male      78 White          \n19 West Virginia Male      10 White          \n20 Idaho         Female     9 White          \n\n# Of the first 20 rows of the entire data, look at values of just race and age\n# Below two lines of code do the same thing\ncen10[1:20, c(\"race\", \"age\")]\n\n# A tibble: 20 × 2\n   race              age\n   <chr>           <dbl>\n 1 White               8\n 2 White              24\n 3 White              37\n 4 White              12\n 5 Black/Negro        18\n 6 White              50\n 7 White              51\n 8 White              41\n 9 White              62\n10 White              25\n11 White              23\n12 White              66\n13 White              57\n14 Other race, nec    73\n15 White              43\n16 White              29\n17 White               8\n18 White              78\n19 White              10\n20 White               9\n\ncen10 %>% slice(1:20) %>% select(race, age)\n\n# A tibble: 20 × 2\n   race              age\n   <chr>           <dbl>\n 1 White               8\n 2 White              24\n 3 White              37\n 4 White              12\n 5 Black/Negro        18\n 6 White              50\n 7 White              51\n 8 White              41\n 9 White              62\n10 White              25\n11 White              23\n12 White              66\n13 White              57\n14 Other race, nec    73\n15 White              43\n16 White              29\n17 White               8\n18 White              78\n19 White              10\n20 White               9\n\n\nA vector is a special type of matrix with only one column or only one row\n\n# One column\ncen10[1:10, c(\"age\")]\n\n# A tibble: 10 × 1\n     age\n   <dbl>\n 1     8\n 2    24\n 3    37\n 4    12\n 5    18\n 6    50\n 7    51\n 8    41\n 9    62\n10    25\n\ncen10 %>% slice(1:10) %>% select(c(\"age\"))\n\n# A tibble: 10 × 1\n     age\n   <dbl>\n 1     8\n 2    24\n 3    37\n 4    12\n 5    18\n 6    50\n 7    51\n 8    41\n 9    62\n10    25\n\n# One row\ncen10[2, ]\n\n# A tibble: 1 × 4\n  state sex     age race \n  <chr> <chr> <dbl> <chr>\n1 Ohio  Male     24 White\n\ncen10 %>% slice(2)\n\n# A tibble: 1 × 4\n  state sex     age race \n  <chr> <chr> <dbl> <chr>\n1 Ohio  Male     24 White\n\n\nWhat if we want a special subset of the data? For example, what if I only want the records of individuals in California? What if I just want the age and race of individuals in California?\n\n# subset for CA rows\nca_subset <- cen10[cen10$state == \"California\", ]\n\nca_subset_tidy <- cen10 %>% filter(state == \"California\")\n\nall_equal(ca_subset, ca_subset_tidy)\n\nWarning: `all_equal()` was deprecated in dplyr 1.1.0.\nℹ Please use `all.equal()` instead.\nℹ And manually order the rows/cols as needed\n\n\n[1] TRUE\n\n# subset for CA rows and select age and race\nca_subset_age_race <- cen10[cen10$state == \"California\", c(\"age\", \"race\")]\n\nca_subset_age_race_tidy <- cen10 %>% filter(state == \"California\") %>% select(age, race)\n\nall_equal(ca_subset_age_race, ca_subset_age_race_tidy)\n\n[1] TRUE\n\n\nSome common operators that can be used to filter or to use as a condition. Remember, you can use the unique function to look at the set of all values a variable holds in the dataset.\n\n# all individuals older than 30 and younger than 70\ns1 <- cen10[cen10$age > 30 & cen10$age < 70, ]\ns2 <- cen10 %>% filter(age > 30 & age < 70)\nall_equal(s1, s2)\n\n[1] TRUE\n\n# all individuals in either New York or California\ns3 <- cen10[cen10$state == \"New York\" | cen10$state == \"California\", ]\ns4 <- cen10 %>% filter(state == \"New York\" | state == \"California\")\nall_equal(s3, s4)\n\n[1] TRUE\n\n# all individuals in any of the following states: California, Ohio, Nevada, Michigan\ns5 <- cen10[cen10$state %in% c(\"California\", \"Ohio\", \"Nevada\", \"Michigan\"), ]\ns6 <- cen10 %>% filter(state %in% c(\"California\", \"Ohio\", \"Nevada\", \"Michigan\"))\nall_equal(s5, s6)\n\n[1] TRUE\n\n# all individuals NOT in any of the following states: California, Ohio, Nevada, Michigan\ns7 <- cen10[!(cen10$state %in% c(\"California\", \"Ohio\", \"Nevada\", \"Michigan\")), ]\ns8 <- cen10 %>% filter(!state %in% c(\"California\", \"Ohio\", \"Nevada\", \"Michigan\"))\nall_equal(s7, s8)\n\n[1] TRUE"
  },
  {
    "objectID": "12_matricies-manipulation.html#checkpoint",
    "href": "12_matricies-manipulation.html#checkpoint",
    "title": "9  Programming: Manipulating Vectors and Matrices1",
    "section": "Checkpoint",
    "text": "Checkpoint\n\n1\nGet the subset of cen10 for non-white individuals (Hint: look at the set of values for the race variable by using the unique function)\n\n# Enter here\n\n\n\n2\nGet the subset of cen10 for females over the age of 40\n\n# Enter here\n\n\n\n3\nGet all the serial numbers for black, male individuals who don’t live in Ohio or Nevada.\n\n# Enter here"
  },
  {
    "objectID": "12_matricies-manipulation.html#exercises",
    "href": "12_matricies-manipulation.html#exercises",
    "title": "9  Programming: Manipulating Vectors and Matrices1",
    "section": "Exercises",
    "text": "Exercises\n\n1\nLet \\[\\mathbf{A} = \\left[\\begin{array}\n{rrr}\n0.6 & 0.2\\\\\n0.4 & 0.8\\\\\n\\end{array}\\right]\\]\nUse R to write code that will create the matrix \\(A\\), and then consecutively multiply \\(A\\) to itself 4 times. What is the value of \\(A^{4}\\)?\n\n## Enter yourself\n\nNote that R notation of matrices is different from the math notation. Simply trying X^n where X is a matrix will only take the power of each element to n. Instead, this problem asks you to perform matrix multiplication.\n\n\n2\nLet’s apply what we learned about subsetting or filtering/selecting. Use the nunn_full dataset you have already loaded\n\nFirst, show all observations (rows) that have a \"male\" variable higher than 0.5\n\n\n## Enter yourself\n\n\nNext, create a matrix / dataframe with only two columns: \"trust_neighbors\" and \"age\"\n\n\n## Enter yourself\n\n\nLastly, show all values of \"trust_neighbors\" and \"age\" for observations (rows) that have the “male” variable value that is higher than 0.5\n\n\n## Enter yourself\n\n\n\n3\nFind a way to generate a vector of “column averages” of the matrix X from the Nunn and Wantchekon data in one line of code. Each entry in the vector should contain the sample average of the values in the column. So a 100 by 4 matrix should generate a length-4 matrix.\n\n\n4\nSimilarly, generate a vector of “column medians”.\n\n\n5\nConsider the regression that was run to generate Table 1:\n\nform <- \"trust_neighbors ~ exports + age + age2 +  male + urban_dum + factor(education) + factor(occupation) + factor(religion) + factor(living_conditions) + district_ethnic_frac + frac_ethnicity_in_district + isocode\"\nlm_1_1 <- lm(as.formula(form), nunn_full)\n\n# The below coef function returns a vector of OLS coefficiants\ncoef(lm_1_1)\n\n               (Intercept)                    exports \n              1.619913e+00              -6.791360e-04 \n                       age                       age2 \n              8.395936e-03              -5.473436e-05 \n                      male                  urban_dum \n              4.550246e-02              -1.404551e-01 \n        factor(education)1         factor(education)2 \n              1.709816e-02              -5.224591e-02 \n        factor(education)3         factor(education)4 \n             -1.373770e-01              -1.889619e-01 \n        factor(education)5         factor(education)6 \n             -1.893494e-01              -2.400767e-01 \n        factor(education)7         factor(education)8 \n             -2.850748e-01              -1.232085e-01 \n        factor(education)9        factor(occupation)1 \n             -2.406437e-01               6.185655e-02 \n       factor(occupation)2        factor(occupation)3 \n              7.392168e-02               3.356158e-02 \n       factor(occupation)4        factor(occupation)5 \n              7.942048e-03               6.661126e-02 \n       factor(occupation)6        factor(occupation)7 \n             -7.563297e-02               1.699699e-02 \n       factor(occupation)8        factor(occupation)9 \n             -9.428177e-02              -9.981440e-02 \n      factor(occupation)10       factor(occupation)11 \n             -3.307068e-02              -2.300045e-02 \n      factor(occupation)12       factor(occupation)13 \n             -1.564540e-01              -1.441370e-02 \n      factor(occupation)14       factor(occupation)15 \n             -5.566414e-02              -2.343762e-01 \n      factor(occupation)16       factor(occupation)18 \n             -1.306947e-02              -1.729589e-01 \n      factor(occupation)19       factor(occupation)20 \n             -1.770261e-01              -2.457800e-02 \n      factor(occupation)21       factor(occupation)22 \n             -4.936813e-02              -1.068511e-01 \n      factor(occupation)23       factor(occupation)24 \n             -9.712205e-02               1.292371e-02 \n      factor(occupation)25      factor(occupation)995 \n              2.623186e-02              -1.195063e-03 \n         factor(religion)2          factor(religion)3 \n              5.395953e-02               7.887878e-02 \n         factor(religion)4          factor(religion)5 \n              4.749150e-02               4.318455e-02 \n         factor(religion)6          factor(religion)7 \n             -1.787694e-02              -3.616542e-02 \n        factor(religion)10         factor(religion)11 \n              6.015041e-02               2.237845e-01 \n        factor(religion)12         factor(religion)13 \n              2.627086e-01              -6.812813e-02 \n        factor(religion)14         factor(religion)15 \n              4.673681e-02               3.844555e-01 \n       factor(religion)360        factor(religion)361 \n              3.656843e-01               3.416413e-01 \n       factor(religion)362        factor(religion)363 \n              8.230393e-01               3.856565e-01 \n       factor(religion)995 factor(living_conditions)2 \n              4.161301e-02               4.395862e-02 \nfactor(living_conditions)3 factor(living_conditions)4 \n              8.627372e-02               1.197428e-01 \nfactor(living_conditions)5       district_ethnic_frac \n              1.203606e-01              -1.553648e-02 \nfrac_ethnicity_in_district                 isocodeBWA \n              1.011222e-01              -4.258953e-01 \n                isocodeGHA                 isocodeKEN \n              1.135307e-02              -1.819556e-01 \n                isocodeLSO                 isocodeMDG \n             -5.511200e-01              -3.315727e-01 \n                isocodeMLI                 isocodeMOZ \n              7.528101e-02               8.223730e-02 \n                isocodeMWI                 isocodeNAM \n              3.062497e-01              -1.397541e-01 \n                isocodeNGA                 isocodeSEN \n             -2.381525e-01               3.867371e-01 \n                isocodeTZA                 isocodeUGA \n              2.079366e-01              -6.443732e-02 \n                isocodeZAF                 isocodeZMB \n             -2.179153e-01              -2.172868e-01 \n\n\nFirst, get a small subset of the nunn_full dataset. This time, sample 20 rows and select for variables exports, age, age2, male, and urban_dum. To this small subset, add (bind_cols() in tidyverse or cbind() in base R) a column of 1’s; this represents the intercept. If you need some guidance, look at how we sampled 10 rows selected for a different set of variables above in the lecture portion.\n\n# Enter here\n\nNext let’s try calculating predicted values of levels of trust in neighbors by multiplying coefficients for the intercept, exports, age, age2, male, and urban_dum to the actual observed values for those variables in the small subset you’ve just created.\n\n# Hint: You can get just selected elements from the vector returned by coef(lm_1_1)\n\n# For example, the below code gives you the first 3 elements of the original vector\ncoef(lm_1_1)[1:3]\n\n (Intercept)      exports          age \n 1.619913146 -0.000679136  0.008395936 \n\n# Also, the below code gives you the coefficient elements for intercept and male\ncoef(lm_1_1)[c(\"(Intercept)\", \"male\")]\n\n(Intercept)        male \n 1.61991315  0.04550246"
  },
  {
    "objectID": "13_functions_obj_loops.html",
    "href": "13_functions_obj_loops.html",
    "title": "10  Objects, Functions, Loops",
    "section": "",
    "text": "Up till now, you should have covered:\n\nR basic programming\nData Import\nStatistical Summaries\nVisualization\n\nToday we’ll cover\n\nObjects\nFunctions\nLoops"
  },
  {
    "objectID": "13_functions_obj_loops.html#what-is-an-object",
    "href": "13_functions_obj_loops.html#what-is-an-object",
    "title": "10  Objects, Functions, Loops",
    "section": "10.1 What is an object?",
    "text": "10.1 What is an object?\nNow that we have covered some hands-on ways to use graphics, let’s go into some fundamentals of the R language.\nLet’s first set up\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(haven)\nlibrary(ggplot2)\n\n\ncen10 <- read_csv(\"data/input/usc2010_001percent.csv\", col_types = cols())\n\nObjects are abstract symbols in which you store data. Here we will create an object from copy, and assign cen10 to it.\n\ncopy <- cen10 \n\nThis looks the same as the original dataset:\n\ncopy\n\n# A tibble: 30,871 × 4\n   state         sex      age race       \n   <chr>         <chr>  <dbl> <chr>      \n 1 New York      Female     8 White      \n 2 Ohio          Male      24 White      \n 3 Nevada        Male      37 White      \n 4 Michigan      Female    12 White      \n 5 Maryland      Female    18 Black/Negro\n 6 New Hampshire Male      50 White      \n 7 Iowa          Female    51 White      \n 8 Missouri      Female    41 White      \n 9 New Jersey    Male      62 White      \n10 California    Male      25 White      \n# ℹ 30,861 more rows\n\n\nWhat happens if you do this next?\n\ncopy <- \"\"\n\nIt got reassigned:\n\ncopy\n\n[1] \"\"\n\n\n\n10.1.1 Lists\nLists are one of the most generic and flexible type of object. You can make an empty list by the function list()\n\nmy_list <- list()\nmy_list\n\nlist()\n\n\nAnd start filling it in. Slots on the list are invoked by double square brackets [[]]\n\nmy_list[[1]] <- \"contents of the first slot -- this is a string\"\nmy_list[[\"slot 2\"]] <- \"contents of slot named slot 2\"\nmy_list\n\n[[1]]\n[1] \"contents of the first slot -- this is a string\"\n\n$`slot 2`\n[1] \"contents of slot named slot 2\"\n\n\neach slot can be anything. What are we doing here? We are defining the 1st slot of the list my_list to be a vector c(1, 2, 3, 4, 5)\n\nmy_list[[1]] <- c(1, 2, 3, 4, 5)\nmy_list\n\n[[1]]\n[1] 1 2 3 4 5\n\n$`slot 2`\n[1] \"contents of slot named slot 2\"\n\n\nYou can even make nested lists. Let’s say we want the 1st slot of the list to be another list of three elements.\n\nmy_list[[1]][[1]] <- \"subitem 1 in slot 1 of my_list\"\nmy_list[[1]][[2]] <- \"subitem 1 in slot 2 of my_list\"\nmy_list[[1]][[3]] <- \"subitem 1 in slot 3 of my_list\"\n\nmy_list\n\n[[1]]\n[1] \"subitem 1 in slot 1 of my_list\" \"subitem 1 in slot 2 of my_list\"\n[3] \"subitem 1 in slot 3 of my_list\" \"4\"                             \n[5] \"5\"                             \n\n$`slot 2`\n[1] \"contents of slot named slot 2\""
  },
  {
    "objectID": "13_functions_obj_loops.html#making-your-own-objects",
    "href": "13_functions_obj_loops.html#making-your-own-objects",
    "title": "10  Objects, Functions, Loops",
    "section": "10.2 Making your own objects",
    "text": "10.2 Making your own objects\nWe’ve covered one type of object, which is a list. You saw it was quite flexible. How many types of objects are there?\nThere are an infinite number of objects, because people make their own class of object. You can detect the type of the object (the class) by the function class\nObject can be said to be an instance of a class.\nAnalogies:\nclass - Pokemon, object - Pikachu\nclass - Book, object - To Kill a Mockingbird\nclass - DataFrame, object - 2010 census data\nclass - Character, object - “Programming is Fun”\nWhat is type (class) of object is cen10?\n\nclass(cen10)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nWhat about this text?\n\nclass(\"some random text\")\n\n[1] \"character\"\n\n\nTo change or create the class of any object, you can assign it. To do this, assign the name of your class to character to an object’s class().\nWe can start from a simple list. For example, say we wanted to store data about pokemon. Because there is no pre-made package for this, we decide to make our own class.\n\npikachu <- list(name = \"Pikachu\",\n                number = 25,\n                type = \"Electric\",\n                color = \"Yellow\")\n\nand we can give it any class name we want.\n\nclass(pikachu) <- \"Pokemon\"\nstr(pikachu)\n\nList of 4\n $ name  : chr \"Pikachu\"\n $ number: num 25\n $ type  : chr \"Electric\"\n $ color : chr \"Yellow\"\n - attr(*, \"class\")= chr \"Pokemon\"\n\npikachu$type\n\n[1] \"Electric\"\n\n\nWe can even define class-specific methods. For example, the summary() function is commonly used to summarize the output of a particular object (such as an lm() regression object). However, summary() will behave differently depending on what object you give it. Why? Because there is a version of summary defined specifically for lm() objects. Let’s define a summary() method for the Pokemon class\n\n# Input: an object of class \"Pokemon\"\n# Output: A text summary of the Pokemon\nsummary.Pokemon <- function(x){\n  out_text <- paste(x$name, \" is a(n) \", x$type, \n  \" type Pokemon. Its PokeDex number is \", \n  x$number, \". It is \", x$color, \".\\n\", sep=\"\")\n  cat(out_text)\n}\n\nNow let’s call the generic summary() function on pikachu\n\nsummary(pikachu)\n\nPikachu is a(n) Electric type Pokemon. Its PokeDex number is 25. It is Yellow.\n\n\n\n10.2.1 Seeing R through objects\nMost of the R objects that you will see as you advance are their own objects. For example, here’s a linear regression object\n\nols <- lm(mpg ~ wt + vs + gear + carb, mtcars)\nclass(ols)\n\n[1] \"lm\"\n\n\nAnything can be an object! Even graphs (in ggplot) can be assigned, re-assigned, and edited.\n\ngrp_race <- group_by(cen10, race)%>%\n  summarize(count = n())\n\ngrp_race_ordered <- arrange(grp_race, count) %>% \n  mutate(race = forcats::as_factor(race))\n\ngg_tab <- ggplot(data = grp_race_ordered) +\n  aes(x = race, y = count) +\n  geom_col() +\n  labs(caption = \"Source: U.S. Census 2010\")\n\ngg_tab\n\n\n\n\nYou can change the orientation\n\ngg_tab<- gg_tab + coord_flip()\n\n\n\n10.2.2 Parsing an object by str()s\nIt can be hard to understand an R object because it’s contents are unknown. The function str, short for structure, is a quick way to look into the innards of an object\n\nstr(my_list)\n\nList of 2\n $       : chr [1:5] \"subitem 1 in slot 1 of my_list\" \"subitem 1 in slot 2 of my_list\" \"subitem 1 in slot 3 of my_list\" \"4\" ...\n $ slot 2: chr \"contents of slot named slot 2\"\n\nclass(my_list)\n\n[1] \"list\"\n\n\nSame for the object we just made\n\nstr(pikachu)\n\nList of 4\n $ name  : chr \"Pikachu\"\n $ number: num 25\n $ type  : chr \"Electric\"\n $ color : chr \"Yellow\"\n - attr(*, \"class\")= chr \"Pokemon\"\n\n\nWhat does a ggplot object look like? Very complicated, but at least you can see it:\n\n# enter this on your console\nstr(gg_tab)"
  },
  {
    "objectID": "13_functions_obj_loops.html#types-of-variables",
    "href": "13_functions_obj_loops.html#types-of-variables",
    "title": "10  Objects, Functions, Loops",
    "section": "10.3 Types of variables",
    "text": "10.3 Types of variables\nIn the social science we often analyze variables. As you saw in the tutorial, different types of variables require different care.\nA key link with what we just learned is that variables are also types of R objects.\n\n10.3.1 scalars\nOne number. How many people did we count in our Census sample?\n\nnrow(cen10)\n\n[1] 30871\n\n\nQuestion: What proportion of our census sample is Native American? This number is also a scalar\n\n# Enter yourself\nunique(cen10$race)\n\n[1] \"White\"                            \"Black/Negro\"                     \n[3] \"Other race, nec\"                  \"American Indian or Alaska Native\"\n[5] \"Chinese\"                          \"Other Asian or Pacific Islander\" \n[7] \"Two major races\"                  \"Three or more major races\"       \n[9] \"Japanese\"                        \n\nmean(cen10$race == \"American Indian or Alaska Native\")\n\n[1] 0.009555894\n\n\nHint: you can use the function mean() to calcualte the sample mean. The sample proportion is the mean of a sequence of number, where your event of interest is a 1 (or TRUE) and others are 0 (or FALSE).\n\n\n10.3.2 numeric vectors\nA sequence of numbers.\n\ngrp_race_ordered$count\n\n[1]    77    88   295   354   869  1129  1839  4013 22207\n\nclass(grp_race_ordered$count)\n\n[1] \"integer\"\n\n\nOr even, all the ages of the millions of people in our Census. Here are just the first few numbers of the list.\n\nhead(cen10$age)\n\n[1]  8 24 37 12 18 50\n\n\n\n\n10.3.3 characters (aka strings)\nThis can be just one stretch of characters\n\nmy_name <- \"Anton\"\nmy_name\n\n[1] \"Anton\"\n\nclass(my_name)\n\n[1] \"character\"\n\n\nor more characters. Notice here that there’s a difference between a vector of individual characters and a length-one object of characters.\n\nmy_name_letters <-  c(\"A\",\"n\",\"t\",\"o\",\"n\")\nmy_name_letters\n\n[1] \"A\" \"n\" \"t\" \"o\" \"n\"\n\nclass(my_name_letters)\n\n[1] \"character\"\n\n\nFinally, remember that lower vs. upper case matters in R!\n\nmy_name2 <- \"anton\"\nmy_name == my_name2\n\n[1] FALSE"
  },
  {
    "objectID": "13_functions_obj_loops.html#what-is-a-function",
    "href": "13_functions_obj_loops.html#what-is-a-function",
    "title": "10  Objects, Functions, Loops",
    "section": "10.4 What is a function?",
    "text": "10.4 What is a function?\nMost of what we do in R is executing a function. read_csv(), nrow(), ggplot() .. pretty much anything with a parentheses is a function. And even things like <- and [ are functions as well.\nA function is a set of instructions with specified ingredients. It takes an input, then manipulates it – changes it in some way – and then returns the manipulated product.\nOne way to see what a function actually does is to enter it without parentheses.\n\n# enter this on your console\ntable\n\nYou’ll see below that the most basic functions are quite complicated internally.\nYou’ll notice that functions contain other functions. wrapper functions are functions that “wrap around” existing functions. This sounds redundant, but it’s an important feature of programming. If you find yourself repeating a command more than two times, you should make your own function, rather than writing the same type of code.\n\n10.4.1 Write your own function\nIt’s worth remembering the basic structure of a function. You create a new function, call it my_fun by this:\n\nmy_fun <- function() {\n  \n}\n\nIf we wanted to generate a function that computed the number of men in your data, what would that look like?\n\ncount_men <- function(data) {\n  \n  nmen <- sum(data$sex == \"Male\")\n  \n  return(nmen)\n}\n\nThen all we need to do is feed this function a dataset\n\ncount_men(cen10)\n\n[1] 15220\n\n\nThe point of a function is that you can use it again and again without typing up the set of constituent manipulations. So, what if we wanted to figure out the number of men in California?\n\ncount_men(cen10[cen10$state == \"California\",])\n\n[1] 1876\n\n\nLet’s go one step further. What if we want to know the proportion of non-whites in a state, just by entering the name of the state? There’s multiple ways to do it, but it could look something like this\n\nnw_in_state <- function(data, state) {\n  \n  s.subset <- data[data$state == state,]\n  total.s <- nrow(s.subset)\n  nw.s <- sum(s.subset$race != \"White\")\n  \n  nw.s / total.s\n}\n\nThe last line is what gets generated from the function. To be more explicit you can wrap the last line around return(). (as in return(nw.s/total.s). return() is used when you want to break out of a function in the middle of it and not wait till the last line.\nTry it on your favorite state!\n\nnw_in_state(cen10, \"Massachusetts\")\n\n[1] 0.2040185"
  },
  {
    "objectID": "13_functions_obj_loops.html#checkpoint",
    "href": "13_functions_obj_loops.html#checkpoint",
    "title": "10  Objects, Functions, Loops",
    "section": "Checkpoint",
    "text": "Checkpoint\n\n1\nTry making your own function, average_age_in_state, that will give you the average age of people in a given state.\n\n# Enter on your own\n\n\n\n2\nTry making your own function, asians_in_state, that will give you the number of Chinese, Japanese, and Other Asian or Pacific Islander people in a given state.\n\n# Enter on your own\n\n\n\n3\nTry making your own function, ‘top_10_oldest_cities’, that will give you the names of cities whose population’s average age is top 10 oldest.\n\n# Enter on your own"
  },
  {
    "objectID": "13_functions_obj_loops.html#what-is-a-package",
    "href": "13_functions_obj_loops.html#what-is-a-package",
    "title": "10  Objects, Functions, Loops",
    "section": "10.5 What is a package?",
    "text": "10.5 What is a package?\nYou can think of a package as a suite of functions that other people have already built for you to make your life easier.\n\nhelp(package = \"ggplot2\")\n\nTo use a package, you need to do two things: (1) install it, and then (2) load it.\nInstalling is a one-time thing\n\ninstall.packages(\"ggplot2\")\n\nBut you need to load each time you start a R instance. So always keep these commands on a script.\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "13_functions_obj_loops.html#conditionals",
    "href": "13_functions_obj_loops.html#conditionals",
    "title": "10  Objects, Functions, Loops",
    "section": "10.6 Conditionals",
    "text": "10.6 Conditionals\nSometimes, you want to execute a command only under certain conditions. This is done through the almost universal function, if(). Inside the if function we enter a logical statement. The line that is adjacent to, or follows, the if() statement only gets executed if the statement returns TRUE.\nFor example,\nFor example,\n\nx <- 5\nif (x >0) {\n  print(\"positive number\")\n} else if (x == 0)  {\n  print (\"zero\")\n} else {\n  print(\"negative number\")\n}\n\n[1] \"positive number\"\n\n\nYou can wrap that whole things in a function\n\nis_positive <- function(number) {\n  if (number >0) {\n    print(\"positive number\")\n  } else if (number == 0)  {\n    print (\"zero\")\n  } else {\n    print(\"negative number\")\n  }\n}\n\nis_positive(5)\n\n[1] \"positive number\"\n\nis_positive(-3)\n\n[1] \"negative number\""
  },
  {
    "objectID": "13_functions_obj_loops.html#for-loops",
    "href": "13_functions_obj_loops.html#for-loops",
    "title": "10  Objects, Functions, Loops",
    "section": "10.7 For-loops",
    "text": "10.7 For-loops\nLoops repeat the same statement, although the statement can be “the same” only in an abstract sense. Use the for(x in X) syntax to repeat the subsequent command as many times as there are elements in the right-hand object X. Each of these elements will be referred to the left-hand index x\nFirst, come up with a vector.\n\nfruits <- c(\"apples\", \"oranges\", \"grapes\")\n\nNow we use the fruits vector in a for loop.\n\nfor (fruit in fruits) {\n  print(paste(\"I love\", fruit))\n}\n\n[1] \"I love apples\"\n[1] \"I love oranges\"\n[1] \"I love grapes\"\n\n\nHere for() and in must be part of any for loop. The right hand side fruits must be a thing that exists. Finally the left-hand side object is “Pick your favor name.” It is analogous to how we can index a sum with any letter. \\(\\sum_{i=1}^{10}i\\) and sum_{j = 1}^{10}j are in fact the same thing.\n\nfor (i in 1:length(fruits)) {\n  print(paste(\"I love\", fruits[i]))\n}\n\n[1] \"I love apples\"\n[1] \"I love oranges\"\n[1] \"I love grapes\"\n\n\n\nstates_of_interest <- c(\"California\", \"Massachusetts\", \"New Hampshire\", \"Washington\")\n\nfor( state in states_of_interest){\n  state_data <- cen10[cen10$state == state,]\n  nmen <- sum(state_data$sex == \"Male\")\n\n  n <- nrow(state_data)\n  men_perc <- round(100*(nmen/n), digits=2)\n  print(paste(\"Percentage of men in\",state, \"is\", men_perc))\n\n}\n\n[1] \"Percentage of men in California is 49.85\"\n[1] \"Percentage of men in Massachusetts is 47.6\"\n[1] \"Percentage of men in New Hampshire is 48.55\"\n[1] \"Percentage of men in Washington is 48.19\"\n\n\nInstead of printing, you can store the information in a vector\n\nstates_of_interest <- c(\"California\", \"Massachusetts\", \"New Hampshire\", \"Washington\")\nmale_percentages <- c()\niter <-1 \n\nfor( state in states_of_interest){\n  state_data <- cen10[cen10$state == state,]\n  nmen <- sum(state_data$sex == \"Male\")\n  n <- nrow(state_data)\n  men_perc <- round(100*(nmen/n), digits=2)\n  \n  male_percentages <- c(male_percentages, men_perc)\n  names(male_percentages)[iter] <- state\n  iter <- iter + 1\n}\n\nmale_percentages\n\n   California Massachusetts New Hampshire    Washington \n        49.85         47.60         48.55         48.19"
  },
  {
    "objectID": "13_functions_obj_loops.html#nested-loops",
    "href": "13_functions_obj_loops.html#nested-loops",
    "title": "10  Objects, Functions, Loops",
    "section": "10.8 Nested Loops",
    "text": "10.8 Nested Loops\nWhat if I want to calculate the population percentage of a race group for all race groups in states of interest? You could probably use tidyverse functions to do this, but let’s try using loops!\n\nstates_of_interest <- c(\"California\", \"Massachusetts\", \"New Hampshire\", \"Washington\")\nfor (state in states_of_interest) {\n  for (race in unique(cen10$race)) {\n    race_state_num <- nrow(cen10[cen10$race == race & cen10$state == state, ])\n    state_pop <- nrow(cen10[cen10$state == state, ])\n    race_perc <- round(100*(race_state_num/(state_pop)), digits=2)\n    print(paste(\"Percentage of \", race , \"in\", state, \"is\", race_perc))\n  }\n}\n\n[1] \"Percentage of  White in California is 57.61\"\n[1] \"Percentage of  Black/Negro in California is 6.72\"\n[1] \"Percentage of  Other race, nec in California is 15.55\"\n[1] \"Percentage of  American Indian or Alaska Native in California is 1.12\"\n[1] \"Percentage of  Chinese in California is 3.75\"\n[1] \"Percentage of  Other Asian or Pacific Islander in California is 9.54\"\n[1] \"Percentage of  Two major races in California is 4.62\"\n[1] \"Percentage of  Three or more major races in California is 0.37\"\n[1] \"Percentage of  Japanese in California is 0.72\"\n[1] \"Percentage of  White in Massachusetts is 79.6\"\n[1] \"Percentage of  Black/Negro in Massachusetts is 5.87\"\n[1] \"Percentage of  Other race, nec in Massachusetts is 4.02\"\n[1] \"Percentage of  American Indian or Alaska Native in Massachusetts is 0.77\"\n[1] \"Percentage of  Chinese in Massachusetts is 2.32\"\n[1] \"Percentage of  Other Asian or Pacific Islander in Massachusetts is 4.33\"\n[1] \"Percentage of  Two major races in Massachusetts is 2.78\"\n[1] \"Percentage of  Three or more major races in Massachusetts is 0\"\n[1] \"Percentage of  Japanese in Massachusetts is 0.31\"\n[1] \"Percentage of  White in New Hampshire is 93.48\"\n[1] \"Percentage of  Black/Negro in New Hampshire is 0.72\"\n[1] \"Percentage of  Other race, nec in New Hampshire is 0.72\"\n[1] \"Percentage of  American Indian or Alaska Native in New Hampshire is 0.72\"\n[1] \"Percentage of  Chinese in New Hampshire is 0.72\"\n[1] \"Percentage of  Other Asian or Pacific Islander in New Hampshire is 2.17\"\n[1] \"Percentage of  Two major races in New Hampshire is 0.72\"\n[1] \"Percentage of  Three or more major races in New Hampshire is 0\"\n[1] \"Percentage of  Japanese in New Hampshire is 0.72\"\n[1] \"Percentage of  White in Washington is 76.05\"\n[1] \"Percentage of  Black/Negro in Washington is 2.9\"\n[1] \"Percentage of  Other race, nec in Washington is 5.37\"\n[1] \"Percentage of  American Indian or Alaska Native in Washington is 2.03\"\n[1] \"Percentage of  Chinese in Washington is 1.31\"\n[1] \"Percentage of  Other Asian or Pacific Islander in Washington is 6.68\"\n[1] \"Percentage of  Two major races in Washington is 4.79\"\n[1] \"Percentage of  Three or more major races in Washington is 0.29\"\n[1] \"Percentage of  Japanese in Washington is 0.58\""
  },
  {
    "objectID": "13_functions_obj_loops.html#exercises",
    "href": "13_functions_obj_loops.html#exercises",
    "title": "10  Objects, Functions, Loops",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Write your own function\nWrite your own function that makes some task of data analysis simpler. Ideally, it would be a function that helps you do either of the previous tasks in fewer lines of code. You can use the three lines of code that was provided in exercise 1 to wrap that into another function too!\n\n# Enter yourself\n\n\n\nExercise 2: Using Loops\nUsing a loop, create a crosstab of sex and race for each state in the set “states_of_interest”\n\nstates_of_interest <- c(\"California\", \"Massachusetts\", \"New Hampshire\", \"Washington\")\n# Enter yourself\n\n\n\nExercise 3: Storing information derived within loops in a global dataframe\nRecall the following nested loop\n\nstates_of_interest <- c(\"California\", \"Massachusetts\", \"New Hampshire\", \"Washington\")\nfor (state in states_of_interest) {\n  for (race in unique(cen10$race)) {\n    race_state_num <- nrow(cen10[cen10$race == race & cen10$state == state, ])\n    state_pop <- nrow(cen10[cen10$state == state, ])\n    race_perc <- round(100*(race_state_num/(state_pop)), digits=2)\n    print(paste(\"Percentage of \", race , \"in\", state, \"is\", race_perc))\n  }\n}\n\n[1] \"Percentage of  White in California is 57.61\"\n[1] \"Percentage of  Black/Negro in California is 6.72\"\n[1] \"Percentage of  Other race, nec in California is 15.55\"\n[1] \"Percentage of  American Indian or Alaska Native in California is 1.12\"\n[1] \"Percentage of  Chinese in California is 3.75\"\n[1] \"Percentage of  Other Asian or Pacific Islander in California is 9.54\"\n[1] \"Percentage of  Two major races in California is 4.62\"\n[1] \"Percentage of  Three or more major races in California is 0.37\"\n[1] \"Percentage of  Japanese in California is 0.72\"\n[1] \"Percentage of  White in Massachusetts is 79.6\"\n[1] \"Percentage of  Black/Negro in Massachusetts is 5.87\"\n[1] \"Percentage of  Other race, nec in Massachusetts is 4.02\"\n[1] \"Percentage of  American Indian or Alaska Native in Massachusetts is 0.77\"\n[1] \"Percentage of  Chinese in Massachusetts is 2.32\"\n[1] \"Percentage of  Other Asian or Pacific Islander in Massachusetts is 4.33\"\n[1] \"Percentage of  Two major races in Massachusetts is 2.78\"\n[1] \"Percentage of  Three or more major races in Massachusetts is 0\"\n[1] \"Percentage of  Japanese in Massachusetts is 0.31\"\n[1] \"Percentage of  White in New Hampshire is 93.48\"\n[1] \"Percentage of  Black/Negro in New Hampshire is 0.72\"\n[1] \"Percentage of  Other race, nec in New Hampshire is 0.72\"\n[1] \"Percentage of  American Indian or Alaska Native in New Hampshire is 0.72\"\n[1] \"Percentage of  Chinese in New Hampshire is 0.72\"\n[1] \"Percentage of  Other Asian or Pacific Islander in New Hampshire is 2.17\"\n[1] \"Percentage of  Two major races in New Hampshire is 0.72\"\n[1] \"Percentage of  Three or more major races in New Hampshire is 0\"\n[1] \"Percentage of  Japanese in New Hampshire is 0.72\"\n[1] \"Percentage of  White in Washington is 76.05\"\n[1] \"Percentage of  Black/Negro in Washington is 2.9\"\n[1] \"Percentage of  Other race, nec in Washington is 5.37\"\n[1] \"Percentage of  American Indian or Alaska Native in Washington is 2.03\"\n[1] \"Percentage of  Chinese in Washington is 1.31\"\n[1] \"Percentage of  Other Asian or Pacific Islander in Washington is 6.68\"\n[1] \"Percentage of  Two major races in Washington is 4.79\"\n[1] \"Percentage of  Three or more major races in Washington is 0.29\"\n[1] \"Percentage of  Japanese in Washington is 0.58\"\n\n\nInstead of printing the percentage of each race in each state, create a dataframe, and store all that information in that dataframe. (Hint: look at how I stored information about male percentage in each state of interest in a vector.)"
  },
  {
    "objectID": "15_project-dempeace.html",
    "href": "15_project-dempeace.html",
    "title": "11  Joins and Merges, Wide and Long1",
    "section": "",
    "text": "The “Democratic Peace” is one of the most widely discussed propositions in political science, covering the fields of International Relations and Comparative Politics, with insights to domestic politics of democracies (e.g. American Politics). The one-sentence idea is that democracies do not fight with each other. There have been much theoretical debate – for example in earlier work, Oneal and Russet (1999) argue that the democratic peace is not due to the hegemony of strong democracies like the U.S. and attempt to distinguish between realist and what they call Kantian propositions (e.g. democratic governance, international organizations)2.\nAn empirical demonstration of the democratic peace is also a good example of a Time Series Cross Sectional (or panel) dataset, where the same units (in this case countries) are observed repeatedly for multiple time periods. Experience in assembling and analyzing a TSCS dataset will prepare you for any future research in this area."
  },
  {
    "objectID": "15_project-dempeace.html#where-are-we-where-are-we-headed",
    "href": "15_project-dempeace.html#where-are-we-where-are-we-headed",
    "title": "11  Joins and Merges, Wide and Long1",
    "section": "Where are we? Where are we headed?",
    "text": "Where are we? Where are we headed?\nUp till now, you should have covered:\n\nR basic programming\nCounting.\nVisualization.\nObjects and Classes.\nMatrix algebra in R\nFunctions.\n\nToday you will work on your own, but feel free to ask a fellow classmate nearby or the instructor. The objective for this session is to get more experience using R, but in the process (a) test a prominent theory in the political science literature and (b) explore related ideas of interest to you."
  },
  {
    "objectID": "15_project-dempeace.html#setting-up",
    "href": "15_project-dempeace.html#setting-up",
    "title": "11  Joins and Merges, Wide and Long1",
    "section": "11.1 Setting up",
    "text": "11.1 Setting up\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "15_project-dempeace.html#create-a-project-directory",
    "href": "15_project-dempeace.html#create-a-project-directory",
    "title": "11  Joins and Merges, Wide and Long1",
    "section": "11.2 Create a project directory",
    "text": "11.2 Create a project directory\nFirst start a directory for this project. This can be done manually or through RStudio’s Project feature(File > New Project...)\nDirectories is the computer science / programming name for folders. While advice about how to structure your working directories might strike you as petty, we believe that starting from some well-tested guides will go a long way in improving the quality and efficiency of your work.\nChapter 4 of Gentzkow and Shapiro’s memo, Code and Data for the Social Scientist] provides a good template."
  },
  {
    "objectID": "15_project-dempeace.html#data-sources",
    "href": "15_project-dempeace.html#data-sources",
    "title": "11  Joins and Merges, Wide and Long1",
    "section": "11.3 Data Sources",
    "text": "11.3 Data Sources\nMost projects you do will start with downloading data from elsewhere. For this task, you’ll probably want to track down and download the following:\n\nCorrelates of war dataset (COW): Find and download the Militarized Interstate Disputes (MIDs) data from the Correlates of War website: http://www.correlatesofwar.org/data-sets. Or a dyad-version on dataverse: https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/11489\nPRIO Data on Armed Conflict: Find and download the Uppsala Conflict Data Program (UCDP) and PRIO dyad-year data on armed conflict(https://www.prio.org) or this link to to the flat csv file (http://ucdp.uu.se/downloads/dyadic/ucdp-dyadic-171.csv).\nPolity: The Polity data can be downloaded from their website (http://www.systemicpeace.org/inscrdata.html). Look for the newest version of the time series that has the widest coverage."
  },
  {
    "objectID": "15_project-dempeace.html#example-with-2-datasets",
    "href": "15_project-dempeace.html#example-with-2-datasets",
    "title": "11  Joins and Merges, Wide and Long1",
    "section": "11.4 Example with 2 Datasets",
    "text": "11.4 Example with 2 Datasets\nLet’s read in a sample dataset.\n\npolity <- read_csv(\"data/input/sample_polity.csv\")\nmid <- read_csv(\"data/input/sample_mid.csv\")\n\nWhat does polity look like?\n\nunique(polity$country)\n\n[1] \"France\"        \"Prussia\"       \"Germany\"       \"United States\"\n\nggplot(polity, aes(x = year, y = polity2)) +\n  facet_wrap(~ country) +\n  geom_line()\n\n\n\nhead(polity)\n\n# A tibble: 6 × 5\n  scode ccode country  year polity2\n  <chr> <dbl> <chr>   <dbl>   <dbl>\n1 FRN     220 France   1800      -8\n2 FRN     220 France   1801      -8\n3 FRN     220 France   1802      -8\n4 FRN     220 France   1803      -8\n5 FRN     220 France   1804      -8\n6 FRN     220 France   1805      -8\n\n\nMID is a dataset that captures a dispute for a given country and year.\n\nmid\n\n# A tibble: 6,132 × 5\n   ccode polity_code dispute StYear EndYear\n   <dbl> <chr>         <dbl>  <dbl>   <dbl>\n 1   200 UKG               1   1902    1903\n 2     2 USA               1   1902    1903\n 3   345 YGS               1   1913    1913\n 4   300 <NA>              1   1913    1913\n 5   339 ALB               1   1946    1946\n 6   200 UKG               1   1946    1946\n 7   200 UKG               1   1951    1952\n 8   651 EGY               1   1951    1952\n 9   630 IRN               1   1856    1857\n10   200 UKG               1   1856    1857\n# ℹ 6,122 more rows"
  },
  {
    "objectID": "15_project-dempeace.html#loops",
    "href": "15_project-dempeace.html#loops",
    "title": "11  Joins and Merges, Wide and Long1",
    "section": "11.5 Loops",
    "text": "11.5 Loops\nNotice that in the mid data, we have a start of a dispute vs. an end of a dispute.In order to combine this into the polity data, we want a way to give each of the interval years a row.\nThere are many ways to do this, but one is a loop. We go through one row at a time, and then for each we make a new dataset. that has year as a sequence of each year. A lengthy loop like this is typically slow, and you’d want to recast the task so you can do things with functions. But, a loop is a good place to start.\n\nmid_year_by_year <- data_frame(ccode = numeric(),\n                               year = numeric(),\n                               dispute = numeric())\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\nfor(i in 1:nrow(mid)) {\n  x <- data_frame(ccode = mid$ccode[i], ## row i's country\n             year = mid$StYear[i]:mid$EndYear[i],  ## sequence of years for dispute in row i\n             dispute = 1) \n  mid_year_by_year <- rbind(mid_year_by_year, x)\n}\n\nhead(mid_year_by_year)\n\n# A tibble: 6 × 3\n  ccode  year dispute\n  <dbl> <int>   <dbl>\n1   200  1902       1\n2   200  1903       1\n3     2  1902       1\n4     2  1903       1\n5   345  1913       1\n6   300  1913       1"
  },
  {
    "objectID": "15_project-dempeace.html#merging",
    "href": "15_project-dempeace.html#merging",
    "title": "11  Joins and Merges, Wide and Long1",
    "section": "11.6 Merging",
    "text": "11.6 Merging\nWe want to combine these two datasets by merging. Base-R has a function called merge. dplyr has several types of joins (the same thing). Those names are based on SQL syntax.\n\nHere we can do a left_join matching rows from mid to polity. We want to keep the rows in polity that do not match in mid, and label them as non-disputes.\n\np_m <- left_join(polity,\n                 distinct(mid_year_by_year),\n                 by = c(\"ccode\", \"year\"))\n\nhead(p_m)\n\n# A tibble: 6 × 6\n  scode ccode country  year polity2 dispute\n  <chr> <dbl> <chr>   <dbl>   <dbl>   <dbl>\n1 FRN     220 France   1800      -8      NA\n2 FRN     220 France   1801      -8      NA\n3 FRN     220 France   1802      -8      NA\n4 FRN     220 France   1803      -8      NA\n5 FRN     220 France   1804      -8      NA\n6 FRN     220 France   1805      -8      NA\n\n\nReplace dispute = NA rows with a zero.\n\np_m$dispute[is.na(p_m$dispute)] <- 0\n\nReshape the dataset long to wide\n\np_m_wide <- pivot_wider(p_m, \n                        id_cols = c(scode, ccode, country),\n                        names_from = year,\n                        values_from = polity2)\n\nselect(p_m_wide, 1:10)\n\n# A tibble: 4 × 10\n  scode ccode country       `1800` `1801` `1802` `1803` `1804` `1805` `1806`\n  <chr> <dbl> <chr>          <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 FRN     220 France            -8     -8     -8     -8     -8     -8     -8\n2 GMY     255 Prussia          -10    -10    -10    -10    -10    -10     NA\n3 GMY     255 Germany           NA     NA     NA     NA     NA     NA     NA\n4 USA       2 United States      4      4      4      4      4      4      4"
  },
  {
    "objectID": "15_project-dempeace.html#main-project",
    "href": "15_project-dempeace.html#main-project",
    "title": "11  Joins and Merges, Wide and Long1",
    "section": "11.7 Main Project",
    "text": "11.7 Main Project\nTry building a panel that would be useful in answering the Democratic Peace Question, perhaps in these steps.\n\nTask 1: Data Input and Standardization\nOften, files we need are saved in the .xls or xlsx format. It is possible to read these files directly into R, but experience suggests that this process is slower than converting them first to .csv format and reading them in as .csv files.\nreadxl/readr/haven packages(https://github.com/tidyverse/tidyverse) is constantly expanding to capture more file types. In day 1, we used the package readxl, using the read_excel() function.\n\n\n\n\n\nTask 2: Data Merging\nWe will use data to test a version of the Democratic Peace Thesis (DPS). Democracies are said to go to war less because the leaders who wage wars are accountable to voters who have to bear the costs of war. Are democracies less likely to engage in militarized interstate disputes?\nTo start, let’s download and merge some data.\n\nLoad in the Militarized Interstate Dispute (MID) files. Militarized interstate disputes are hostile action between two formally recognized states. Examples of this would be threats to use force, threats to declare war, beginning war, fortifying a border with troops, and so on.\nFind a way to merge the Polity IV dataset and the MID data. This process can be a bit tricky.\nAn advanced version of this task would be to download the dyadic form of the data and try merging that with polity.\n\n\n\n\n\n\nTask 3: Tabulations and Visualization\n\nCalculate the mean Polity2 score by year. Plot the result. Use graphical indicators of your choosing to show where key events fall in this timeline (such as 1914, 1929, 1939, 1989, 2008). Speculate on why the behavior from 1800 to 1920 seems to be qualitatively different than behavior afterwards.\nDo the same but only among state-years that were invovled in a MID. Plot this line together with your results from 1.\nDo the same but only among state years that were not involved in a MID.\nArrive at a tentative conclusion for how well the Democratic Peace argument seems to hold up in this dataset. Visualize this conclusion."
  },
  {
    "objectID": "17_vectorization_optimization.html",
    "href": "17_vectorization_optimization.html",
    "title": "12  Functionals and Optimization",
    "section": "",
    "text": "For our main example in this session, we’ll take a look at the the 2022 Cooperative Election Survey (CES). The ces22.dta file contains a subset of questions asked to respondents in the 2022 wave of this annual survey stored in Stata’s dta format.\n\nces <- haven::read_dta(\"data/input/ces22_subset.dta\")\n\nStata files often contain metadata for each column. In the CES, responses are stored as numeric values that represent one of the discrete choices available to the respondent. As a result, while many of the columns have numeric data types, they should not be treated as numbers. This is something to be careful of as you work with any dataset – always read the codebook.\nFor example, while we could take the “mean” of the race variable, this is a nonsense quantity.\n\nmean(ces$race)\n\n[1] 1.696817\n\n\nWe can take a closer look at how the race variable is stored:\n\nhead(ces$race)\n\n<labelled<double>[6]>: Race\n[1] 1 1 1 1 1 7\n\nLabels:\n value             label\n     1             White\n     2             Black\n     3          Hispanic\n     4             Asian\n     5   Native American\n     6 Two or more races\n     7             Other\n     8    Middle Eastern\n    98           skipped\n    99         not asked\n\n\nAnd take a closer look at its class\n\nclass(ces$race)\n\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\n\nhaven reads in the Stata metadata and creates a column that has the haven_labelled class, which stores both the numeric labels and the metadata which denotes the mapping between the numeric label and its meaning. We’d like to use this to convert the variables to factors that we can work with in R. To do this, we’ll be using the as_factor() function. This function is “generic” and has a specific implementation for haven_labelled objects in the haven package\n\nhead(as_factor(ces$race))\n\n[1] White White White White White Other\n10 Levels: White Black Hispanic Asian Native American ... not asked\n\n\nOr in “tidy” style\n\nces %>% mutate(race = as_factor(race))\n\n# A tibble: 60,000 × 28\n     caseid commonweight commonpostweight gender4 educ    race  hispanic pid3   \n      <dbl>        <dbl>            <dbl> <dbl+l> <dbl+l> <fct> <dbl+lb> <dbl+l>\n 1   1.98e9        3.65            3.53   1 [Man] 6 [Pos… White 2 [No]   1 [Dem…\n 2   1.98e9        0.780           0.819  1 [Man] 3 [Som… White 2 [No]   3 [Ind…\n 3   1.98e9        0.892           0.774  2 [Wom… 5 [4-y… White 2 [No]   1 [Dem…\n 4   1.98e9        1.10            1.21   3 [Non… 6 [Pos… White 2 [No]   4 [Oth…\n 5   1.98e9        0.543           0.328  1 [Man] 6 [Pos… White 2 [No]   3 [Ind…\n 6   1.98e9        0.114           0.0989 1 [Man] 5 [4-y… Other 2 [No]   3 [Ind…\n 7   1.98e9        0.899           1.21   2 [Wom… 2 [Hig… Black 2 [No]   1 [Dem…\n 8   1.98e9        0.633           0.431  1 [Man] 6 [Pos… White 2 [No]   1 [Dem…\n 9   1.98e9        0.900           0.854  2 [Wom… 5 [4-y… White 2 [No]   1 [Dem…\n10   1.98e9        0.725           0.586  1 [Man] 6 [Pos… White 2 [No]   1 [Dem…\n# ℹ 59,990 more rows\n# ℹ 20 more variables: pid7 <dbl+lbl>, inputstate <dbl+lbl>,\n#   CC22_306 <dbl+lbl>, CC22_320a <dbl+lbl>, CC22_320b <dbl+lbl>,\n#   CC22_320c <dbl+lbl>, CC22_320d <dbl+lbl>, CC22_320e <dbl+lbl>,\n#   CC22_320f <dbl+lbl>, CC22_320g <dbl+lbl>, CC22_320h <dbl+lbl>,\n#   page_CC22_320grid_timing <dbl>, CC22_333 <dbl+lbl>, CC22_333a <dbl+lbl>,\n#   CC22_333b <dbl+lbl>, CC22_333c <dbl+lbl>, CC22_333d <dbl+lbl>, …\n\n\nBut we’d like to do this for every column in our dataset (except for columns that are numeric). We could manually go through every single column and convert it to a factor, or we could use some tidyverse tools.\nThe across() function allows us to select columns using the same custom selection syntax as in select(). We can combine this with the mutate function to mutate every column that we want to transform. Here, we want to select the columns that are of a certain class – the “labelled” class.\n\nces <- ces %>% mutate(across(where(is.labelled), as_factor))\n\nacross(), like select() is very flexible in how columns can be selected. In addition to selecting by number, you can select by partial string match, numerical sequence, or even regular expressions!\nNow all of our columns operate correctly as factors.\n\nces\n\n# A tibble: 60,000 × 28\n   caseid commonweight commonpostweight gender4 educ  race  hispanic pid3  pid7 \n    <dbl>        <dbl>            <dbl> <fct>   <fct> <fct> <fct>    <fct> <fct>\n 1 1.98e9        3.65            3.53   Man     Post… White No       Demo… Stro…\n 2 1.98e9        0.780           0.819  Man     Some… White No       Inde… Lean…\n 3 1.98e9        0.892           0.774  Woman   4-ye… White No       Demo… Stro…\n 4 1.98e9        1.10            1.21   Non-bi… Post… White No       Other Inde…\n 5 1.98e9        0.543           0.328  Man     Post… White No       Inde… Inde…\n 6 1.98e9        0.114           0.0989 Man     4-ye… Other No       Inde… Inde…\n 7 1.98e9        0.899           1.21   Woman   High… Black No       Demo… Not …\n 8 1.98e9        0.633           0.431  Man     Post… White No       Demo… Not …\n 9 1.98e9        0.900           0.854  Woman   4-ye… White No       Demo… Stro…\n10 1.98e9        0.725           0.586  Man     Post… White No       Demo… Not …\n# ℹ 59,990 more rows\n# ℹ 19 more variables: inputstate <fct>, CC22_306 <fct>, CC22_320a <fct>,\n#   CC22_320b <fct>, CC22_320c <fct>, CC22_320d <fct>, CC22_320e <fct>,\n#   CC22_320f <fct>, CC22_320g <fct>, CC22_320h <fct>,\n#   page_CC22_320grid_timing <dbl>, CC22_333 <fct>, CC22_333a <fct>,\n#   CC22_333b <fct>, CC22_333c <fct>, CC22_333d <fct>, CC22_333e <fct>,\n#   page_CC22_333_timing <dbl>, page_CC22_333grid_timing <dbl>\n\n\nWe can do some more steps to make our data easier to work with. First, we can rename columns to be more informative. For example, CC22_306 is a question on vaccination status. Let’s rename() that column to make it easier to reference\n\nces <- ces %>% rename(vaccinated = CC22_306) #New aname = old name\n\nvaccinated has a number of different levels\n\ntable(ces$vaccinated)\n\n\n                              I am fully vaccinated and have received at least one booster shot \n                                                                                          33870 \n                                     I am fully vaccinated but have not received a booster shot \n                                                                                          10554 \nI am partially vaccinated (I have received the first of two shots for either Pfizer or Moderna) \n                                                                                           1956 \n                                                                     I am not vaccinated at all \n                                                                                          13467 \n                                                                                        skipped \n                                                                                              0 \n                                                                                      not asked \n                                                                                              0 \n\n\nLet’s recode this categorical variable to a binary indicator for whether a respondent is fully vaccinated. Note that two of these categories have “fully vaccinated”. We want to code these as a value of \\(1\\) and the rest as \\(0\\). To do this we’ll use the case_when() function inside of a mutate()\n\nces <- ces %>% mutate(fullyvax = case_when(str_detect(vaccinated, \"fully vaccinated\") ~ 1, \n                                           !str_detect(vaccinated, \"fully vaccinated\") ~ 0,\n                                           is.na(vaccinated) ~ NA_real_))\n\nHow many respondents are fully vaccinated?\n\ntable(ces$fullyvax)\n\n\n    0     1 \n15423 44424 \n\n\nCC22_320 contains approval ratings of various political institutions. Let’s pull the three big ones (President, Legislature, Supreme Court) and create indicators for whether the respondent approves or disapproves of that institution.\n\nces <- ces %>% mutate(approvePresident = case_when(str_detect(CC22_320a, \"disapprove\") ~ 0,\n                                                    str_detect(CC22_320a, \"approve\") ~ 1,\n                                                    str_detect(CC22_320a, \"Not sure\") ~ 0), \n                      approveCongress = case_when(str_detect(CC22_320b, \"disapprove\") ~ 0,\n                                                    str_detect(CC22_320b, \"approve\") ~ 1,\n                                                    str_detect(CC22_320b, \"Not sure\") ~ 0), \n                      approveCourt = case_when(str_detect(CC22_320c, \"disapprove\") ~ 0,\n                                                    str_detect(CC22_320c, \"approve\") ~ 1,\n                                                    str_detect(CC22_320c, \"Not sure\") ~ 0))\n\nLet’s pull those three columns, party ID and the survey weights and analyze them further.\n\nces_approval <- ces %>% select(commonweight, pid3, approvePresident, approveCongress, approveCourt)"
  },
  {
    "objectID": "17_vectorization_optimization.html#functionals",
    "href": "17_vectorization_optimization.html#functionals",
    "title": "12  Functionals and Optimization",
    "section": "Functionals",
    "text": "Functionals\nMany functions in R act on functions as input. You’ve already seen a lot of tidyverse functions that do this. For example, the summarize() function takes as input a dataset and a function or functions describing what operations should be done on the dataset. Above, we used mutate which took the functional arguments across and as_factor.\nR is a functional programming language in that functions are “first-class citizens” and can be treated as any other data type. They can be given a name, passed as inputs to other functions, and returned as output.\nThis allows programs to be written in terms of compositions of functions. In fact, this is one of the principles of the tidy project as outlined by Hadley Wickham, and many of the constructs provided by the tidyverse encourage you to work this way. See the tidy manifesto for more.\n\n12.1.1 Replacing for loops\nR programmers tend to favor functional replacements for for loops. Rather than iterate over elements of a vector or list and execute some operation for each iteration, you can use one of the apply (Base-R) or map (tidyverse) functions.\nTo illustrate, let’s take a look at our approval rating data from before. How would we write a for-loop to calculate the (weighted) mean for each column?\n\napproval_data <- ces_approval %>% select(approvePresident, approveCongress, approveCourt)\nfor_time <- proc.time() # Store current time -- used for timing speed of function\napproval <- rep(NA, 3)\nnames(approval) <- c(\"approvePresident\", \"approveCongress\", \"approveCourt\")\nfor (name in names(approval)){\n  approval[name] <- weighted.mean(approval_data[[name]], ces_approval$commonweight, na.rm=T)\n}\nprint(proc.time() - for_time)\n\n   user  system elapsed \n  0.012   0.000   0.013 \n\nprint(approval)\n\napprovePresident  approveCongress     approveCourt \n       0.4131196        0.2635909        0.3600206 \n\n\nThis is pretty terrible looking. We’ve already seen the summarize() function work for this task, but let’s illustrate another operation in base-R, the apply function. apply() operates on matrices and applies a function to each row or column.\n\nfor_time <- proc.time() # Store current time -- used for timing speed of function\napproval <- apply(approval_data, 2, function(x) weighted.mean(x, ces_approval$commonweight, na.rm=T))\nprint(proc.time() - for_time)\n\n   user  system elapsed \n  0.007   0.000   0.007 \n\nprint(approval)\n\napprovePresident  approveCongress     approveCourt \n       0.4131196        0.2635909        0.3600206 \n\n\nNote that there aren’t really substantial speed benefits. Rather, the code just looks cleaner. There are some common functions that are faster, but they have been specifically optimized for speed. Consider colMeans or rowMeans for simple (unweighted) means.\nAlso note how we defined an “in-line” function within the apply() call. This is sometimes called a “lambda” or “anonymous” function. We won’t be able to call it outside of the apply() call since it has no name, but there’s not ever a reason why we would want to.\nsapply() works on generic lists while tapply() can also apply a function based on the value of an index.\nHowever, if you are using tidyverse, you probably should prefer the equivalent “generic” version of apply() – the map family of functions. They vary primarily in how they return their output. map() always returns a list while other forms (like map_dbl() or map_chr()) will force this list to be a vector of the specified type.\n\napproval_data %>% map_dbl(function(x) weighted.mean(x, ces_approval$commonweight, na.rm=T))\n\napprovePresident  approveCongress     approveCourt \n       0.4131196        0.2635909        0.3600206 \n\n\nYou can use map() on a vector to iterate over a function repeatedly\n\nmap(1:10, function(x) sample(1:100, 1)) # Sample a number from 1:100 ten times.\n\n[[1]]\n[1] 64\n\n[[2]]\n[1] 62\n\n[[3]]\n[1] 46\n\n[[4]]\n[1] 6\n\n[[5]]\n[1] 23\n\n[[6]]\n[1] 72\n\n[[7]]\n[1] 95\n\n[[8]]\n[1] 48\n\n[[9]]\n[1] 39\n\n[[10]]\n[1] 68\n\n\nNote that when applying functions to grouped columns of a data frame, you should probably still be using summarize() for most cases. group_map() has more flexibility, but can be a bit more difficult to implement.\n\nces_approval %>% group_by(pid3) %>% summarize_at(vars(contains(\"approve\")), ~ weighted.mean(., commonweight, na.rm=T))\n\n# A tibble: 5 × 4\n  pid3        approvePresident approveCongress approveCourt\n  <fct>                  <dbl>           <dbl>        <dbl>\n1 Democrat              0.837            0.507        0.205\n2 Republican            0.0646           0.113        0.607\n3 Independent           0.338            0.185        0.346\n4 Other                 0.270            0.121        0.343\n5 Not sure              0.248            0.150        0.170"
  },
  {
    "objectID": "17_vectorization_optimization.html#excerise-optimization",
    "href": "17_vectorization_optimization.html#excerise-optimization",
    "title": "12  Functionals and Optimization",
    "section": "12.2 Excerise: Optimization",
    "text": "12.2 Excerise: Optimization\nIn this section, we’ll illustrate another valuable use of functional programming – optimization routines. Many tasks require finding maxima or minima of functions. Most functions of interest are very difficult to analyze analytically and rarely does a closed-form solution for the optima exist. However, a large number of methods exist that allow you to find a numerical solution to this problem.\nConsider the following function.\n\\[f(x_1, x_2)  = -x_1^2 + 2x_1 - 2x_2^2 + 3x_2 + x_1x_2 + 2\\] Let’s it’s maximum using numerical optimization\n\npolynom <- function(x){\n  return(-x[1]^2 + 2*x[1] - 2*x[2]^2 + 3*x[2] + x[1]*x[2] + 2)\n}\n\nWe will use an implementation of the BFGS algorithm. This is an extension of the classic Newton-Raphson method. To find the maximum of a function, this approach starts with an initial guess \\(x_n\\). Then, each subsequent step updates \\(x_n\\) by taking steps in the direction of the gradient, scaled by the Hessian. In a single dimension, the update step is:\n\\[x_{n+1} = x_{n} + \\frac{f^{\\prime}(x_{n})}{f^{\\prime\\prime}(x_n)}\\]\nFor minimization, we replace the plus with a minus (as we want to move away from the gradient).\nBFGS is implemented alongside many other algorithms in the optim function that is part of Base-R. optim takes as input a set of initial parameters, the name of the function to be optimized. A function that returns the gradient can also be specified, but this is optional. If not provided, optim routines that use the gradient will approximate it using a finite difference method (evaluating the function at small changes in the inputs).\n\nmax_1 <- optim(c(0,0), polynom, method = \"BFGS\", hessian=T, control=list(fnscale=-1))\n\nNote that we have set hessian=T to return an evaluation of the hessian matrix at the critical point. We have also set this as a maximization problem by using the fnscale argument in control. By default, optim() minimizes the function. fnscale flips the function so that minimization is maximization of the original function.\nLet’s see the output\n\nmax_1\n\n$par\n[1] 1.571269 1.142798\n\n$value\n[1] 5.285714\n\n$counts\nfunction gradient \n      10        6 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n     [,1] [,2]\n[1,]   -2    1\n[2,]    1   -4\n\n\nThe \\(x\\) solution is stored in $par, the value at the maximum is stored in $value. $convergence describes whether the optimization routine converged or not (0 = success). We can confirm that this is a maximum by showing that the Hessian is negative definite and that all the eigenvalues of the Hessian are negative.\n\neigen(max_1$hessian)\n\neigen() decomposition\n$values\n[1] -1.585786 -4.414214\n\n$vectors\n           [,1]       [,2]\n[1,] -0.9238795 -0.3826834\n[2,] -0.3826834  0.9238795\n\n\nNote that starting values matter. Choosing a starting point far away from the true solution means that more steps are required to reach convergence. Here, the function is well-behaved enough that even choosing \\(x_0 = \\{1000, -100\\}\\) doesn’t increase the number of steps too much, but some functions can be very poorly behaved (near-zero gradients) for some inputs.\n\nmax_2 <- optim(c(1000,-100), polynom, method = \"BFGS\", hessian=T, control=list(fnscale=-1))\nmax_2\n\n$par\n[1] 1.571428 1.142848\n\n$value\n[1] 5.285714\n\n$counts\nfunction gradient \n      25       10 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n     [,1] [,2]\n[1,]   -2    1\n[2,]    1   -4\n\n\nChallenge problem:\nConsider the following function.\n\nmyfun <- function(x, mu=5, sigma=10){\n  return(((x*sigma*sqrt(2*pi))^-1)*exp((-(log(x) - mu)^2)/(2*sigma^2)))\n}\n\nUsing optim, find the maximum of myfun for parameters mu=5 and sigma=10.\nHint: myfun is defined only over the positive reals, but optim assumes the inputs are unbounded (at least for BFGS). Try to transform the inputs such that optim will work (you can do this with a lambda function).\n\n\n\nNow, find the maximum for parameters mu=7 and sigma=5\n\n\n\nChallenge Problem 2:\nWrite a function that returns the gradient of polynom. Pass this function as as an argument to optim. Compare the speed of the optimizer when the gradient is known in closed form vs. when it is approximated."
  },
  {
    "objectID": "16_simulation.html",
    "href": "16_simulation.html",
    "title": "13  Simulation1",
    "section": "",
    "text": "An increasing amount of political science contributions now include a simulation.\n\nAxelrod (1977) demonstrated via simulation how atomized individuals evolve to be grouped in similar clusters or countries, a model of culture.2\nChen and Rodden (2013) argued in a 2013 article that the vote-seat inequality in U.S. elections that is often attributed to intentional partisan gerrymandering can actually attributed to simply the reality of “human geography” – Democratic voters tend to be concentrated in smaller area. Put another way, no feasible form of gerrymandering could spread out Democratic voters in such a way to equalize their vote-seat translation effectiveness. After demonstrating the empirical pattern of human geography, they advance their key claim by simulating thousands of redistricting plans and record the vote-seat ratio.3\nGary King, James Honaker, and multiple other authors propose a way to analyze missing data with a method of multiple imputation, which uses a lot of simulation from a researcher’s observed dataset.4 (Software: Amelia5)\n\nStatistical methods also incorporate simulation:\n\nThe bootstrap: a statistical method for estimating uncertainty around some parameter by re-sampling observations.\nBagging: a method for improving machine learning predictions by re-sampling observations, storing the estimate across many re-samples, and averaging these estimates to form the final estimate. A variance reduction technique.\nStatistical reasoning: if you are trying to understand a quantitative problem, a wonderful first-step to understand the problem better is to simulate it! The analytical solution is often very hard (or impossible), but the simulation is often much easier :-)"
  },
  {
    "objectID": "16_simulation.html#pick-a-sample-any-sample",
    "href": "16_simulation.html#pick-a-sample-any-sample",
    "title": "13  Simulation1",
    "section": "13.1 Pick a sample, any sample",
    "text": "13.1 Pick a sample, any sample"
  },
  {
    "objectID": "16_simulation.html#the-sample-function",
    "href": "16_simulation.html#the-sample-function",
    "title": "13  Simulation1",
    "section": "13.2 The sample() function",
    "text": "13.2 The sample() function\nThe core functions for coding up stochastic data revolves around several key functions, so we will simply review them here.\nSuppose you have a vector of values x and from it you want to randomly sample a sample of length size. For this, use the sample function\n\nsample(x = 1:10, size = 5)\n\n[1]  6  9  2  1 10\n\n\nThere are two subtypes of sampling – with and without replacement.\n\nSampling without replacement (replace = FALSE) means once an element of x is chosen, it will not be considered again:\n\n\nsample(x = 1:10, size = 10, replace = FALSE) ## no number appears more than once\n\n [1]  1  4  2  9  5  6  7  3 10  8\n\n\n\nSampling with replacement (replace = TRUE) means that even if an element of x is chosen, it is put back in the pool and may be chosen again.\n\n\nsample(x = 1:10, size = 10, replace = TRUE) ## any number can appear more than once\n\n [1] 10  1  7 10  4  1  7  5  2  2\n\n\nIt follows then that you cannot sample without replacement a sample that is larger than the pool.\n\nsample(x = 1:10, size = 10, replace = FALSE)\n\n [1]  9  6  1 10  8  5  4  7  3  2\n\n\nSo far, every element in x has had an equal probability of being chosen. In some application, we want a sampling scheme where some elements are more likely to be chosen than others. The argument prob handles this.\nFor example, this simulates 20 fair coin tosses (each outcome is equally likely to happen)\n\nsample(c(\"Head\", \"Tail\"), size = 20, prob = c(.5, .5), replace = TRUE)\n\n [1] \"Tail\" \"Head\" \"Head\" \"Tail\" \"Head\" \"Tail\" \"Head\" \"Head\" \"Tail\" \"Tail\"\n[11] \"Tail\" \"Tail\" \"Tail\" \"Head\" \"Head\" \"Head\" \"Tail\" \"Tail\" \"Tail\" \"Tail\"\n\n\nBut this simulates 20 biased coin tosses, where say the probability of Tails is 4 times more likely than the number of Heads\n\nsample(c(\"Head\", \"Tail\"), size = 20, prob = c(0.2, 0.8), replace = TRUE)\n\n [1] \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\" \"Tail\"\n[11] \"Tail\" \"Tail\" \"Tail\" \"Head\" \"Head\" \"Tail\" \"Tail\" \"Tail\" \"Head\" \"Tail\"\n\n\n\n13.2.1 Sampling rows from a dataframe\nIn tidyverse, there is a convenience function to sample rows randomly: slice_sample()\nFor example, load the dataset on cars, mtcars, which has 32 observations.\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nsample_n picks a user-specified number of rows from the dataset:\n\nmtcars[sample(1:ncol(mtcars), size=3, replace=F),]\n\n               mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nValiant       18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nMerc 240D     24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n\nslice_sample(mtcars, n=3)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat X1-9         27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 280C         17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n\n\nSometimes you want a X percent sample of your dataset. In this case use the prop argument to slice_sample\n\nmtcars %>% slice_sample(prop=.5)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n\n\nA common task in statistics is to generate “bootstrap” samples of your data. This involves sampling a new dataset of equivalent size (n rows) but with replacement (so some observations are dropped; others might appear 2 or 3 times). You can do this with slice_sample as well\n\nmtcars %>% slice_sample(prop=1, replace=T)\n\n                        mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet Sportabout      18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant                18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMazda RX4...3          21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMerc 280...4           19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nLincoln Continental    10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nFiat 128...6           32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nVolvo 142E             21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nPorsche 914-2...8      26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nAMC Javelin...9        15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nChrysler Imperial...10 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nHonda Civic            30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nMazda RX4 Wag          21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDodge Challenger       15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nDuster 360             14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nPorsche 914-2...15     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nFerrari Dino           19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 450SL...17        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nFiat 128...18          32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nMerc 450SL...19        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nPorsche 914-2...20     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nAMC Javelin...21       15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMazda RX4...22         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nChrysler Imperial...23 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDatsun 710             22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMazda RX4...25         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nChrysler Imperial...26 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nMerc 230               22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFord Pantera L         15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMerc 280...29          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nToyota Corona...30     21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nToyota Corona...31     21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nMaserati Bora          15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\n\nAs a side-note, these functions have very practical uses for any type of data analysis:\n\nInspecting your dataset: using head() all the same time and looking over the first few rows might lead you to ignore any issues that end up in the bottom for whatever reason.\nTesting your analysis with a small sample: If running analyses on a dataset takes more than a handful of seconds, change your dataset upstream to a fraction of the size so the rest of the code runs in less than a second. Once verifying your analysis code runs, then re-do it with your full dataset (by simply removing the sample_n / sample_frac line of code in the beginning). While three seconds may not sound like much, they accumulate and eat up time."
  },
  {
    "objectID": "16_simulation.html#random-numbers-from-specific-distributions",
    "href": "16_simulation.html#random-numbers-from-specific-distributions",
    "title": "13  Simulation1",
    "section": "13.3 Random numbers from specific distributions",
    "text": "13.3 Random numbers from specific distributions\n\nrbinom()\nrbinom builds upon sample as a tool to help you answer the question – what is the total number of successes I would get if I sampled a binary (Bernoulli) result from a test with size number of trials each, with a event-wise probability of prob. The first argument n asks me how many such numbers I want.\nFor example, I want to know how many Heads I would get if I flipped a fair coin 100 times.\n\nrbinom(n = 100, size = 1, prob = 0.1)\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n\n\nNow imagine this I wanted to do this experiment 10 times, which would require I flip the coin 10 x 100 = 1000 times! Helpfully, we can do this in one line\n\nrbinom(n = 10, size = 100, prob = 0.5)\n\n [1] 46 41 50 50 59 53 57 49 51 56\n\n\n\n\nrunif()\nrunif also simulates a stochastic scheme where each event has equal probability of getting chosen like sample, but is a continuous rather than discrete system. We will cover this more in the next math module.\nThe intuition to emphasize here is that one can generate potentially infinite amounts (size n) of noise that is a essentially random\n\nrunif(n = 5, min=0, max=1)\n\n[1] 0.01051054 0.57140770 0.35731914 0.36474572 0.28763365\n\n\n\n\nrnorm()\nrnorm is also a continuous distribution, but draws from a Normal distribution – perhaps the most important distribution in statistics. It runs the same way as runif\n\nrnorm(n=5, mean=0, sd=2)\n\n[1]  1.9375519 -0.9540334 -0.7534709 -1.4081971  3.5492358\n\n\nTo better visualize the difference between the output of runif and rnorm, let’s generate lots of each and plot a histogram.\n\nfrom_runif <- runif(n = 1000)\nfrom_rnorm <- rnorm(n = 1000)\n\npar(mfrow = c(1, 2)) ## base-R parameter for two plots at once\nhist(from_runif)\nhist(from_rnorm)"
  },
  {
    "objectID": "16_simulation.html#r-p-and-d",
    "href": "16_simulation.html#r-p-and-d",
    "title": "13  Simulation1",
    "section": "13.4 r, p, and d",
    "text": "13.4 r, p, and d\nEach distribution can do more than generate random numbers (the prefix r). We can compute the cumulative probability by the function pbinom(), punif(), and pnorm(). Also the density – the value of the PDF – by dbinom(), dunif() and dnorm().\n\nrandom_normals <- rnorm(n=1000000, mean=0, sd=1)\nhist(random_normals)\n\n\n\nrandom_binom_2 <- qbinom(runif(n=1000000), size=10, prob=.4)\nhist(random_binom_2)"
  },
  {
    "objectID": "16_simulation.html#set.seed",
    "href": "16_simulation.html#set.seed",
    "title": "13  Simulation1",
    "section": "13.5 set.seed()",
    "text": "13.5 set.seed()\nR doesn’t have the ability to generate truly random numbers! Random numbers are actually very hard to generate. (Think: flipping a coin –> can be perfectly predicted if I know wind speed, the angle the coin is flipped, etc.). Some people use random noise in the atmosphere or random behavior in quantum systems to generate “truly” (?) random numbers. Conversely, R uses deterministic algorithms which take as an input a “seed” and which then perform a series of operations to generate a sequence of random-seeming numbers (that is, numbers whose sequence is sufficiently hard to predict).\nLet’s think about this another way. Sampling is a stochastic process, so every time you run sample() or runif() you are bound to get a different output (because different random seeds are used). This is intentional in some cases but you might want to avoid it in others. For example, you might want to diagnose a coding discrepancy by setting the random number generator to give the same number each time. To do this, use the function set.seed().\nIn the function goes any number. When you run a sample function in the same command as a preceding set.seed(), the sampling function will always give you the same sequence of numbers. In a sense, the sampler is no longer random (in the sense of unpredictable to use; remember: it never was “truly” random in the first place)\n\nset.seed(02138)\nrunif(n = 10)\n\n [1] 0.51236144 0.61530551 0.37451441 0.43541258 0.21166530 0.17812129\n [7] 0.04420775 0.45567854 0.88718264 0.06970056\n\n\nThe random number generator should give you the exact same sequence of numbers if you precede the function by the same seed,\n\nset.seed(02138)\nrunif(n = 10)\n\n [1] 0.51236144 0.61530551 0.37451441 0.43541258 0.21166530 0.17812129\n [7] 0.04420775 0.45567854 0.88718264 0.06970056"
  },
  {
    "objectID": "16_simulation.html#calculating-an-expectation-using-monte-carlo",
    "href": "16_simulation.html#calculating-an-expectation-using-monte-carlo",
    "title": "13  Simulation1",
    "section": "13.6 Calculating an expectation using Monte Carlo",
    "text": "13.6 Calculating an expectation using Monte Carlo\nWe can use repeated, independent draws from a random process in order to approximate its mean (or variance, or higher-order moments). The intuition comes from the law of large numbers. As we obtain repeated independent and identically distributed realizations from a random variable and take their average, this average should get closer and closer to the true expected value. The longer we run the simulation, the better the approximation.\nCrucially, we can do this for essentially any random process that we can repeatedly obtain independent draws from. All we need to do is be able to implement the process in code. This can often be easier than trying to calculate an expectation analytically.\nConsider the problem of calculating the expectation of the maximum of two independent 20-sided die rolls. We could construct a function that took the maximum of two independent draws from a 1-20 vector. Doing this a large number of times and taking the average allows us to approximate the true expectation of this new random variable without having to do any math!\n\nradvantage <- function(sides=20, k=2){\n  return(max(sample.int(sides, k, replace = T)))\n}\n\n# Take the mean of 100,000 rolls with \"advantage\"\nset.seed(60639)\nmean(map_vec(1:1e5, radvantage))\n\n[1] 33379\n\n\nNext, consider the “birthday problem” or the probability that among \\(k\\) randomly chosen people, at least two people will have the same birthday. Recall that the expected value of an indicator random variable is equal to the probability that the indicator is equal to \\(1\\) (so we can calculate probabilities using Monte Carlo by getting draws from an indicator variable for the event of interest).\nWe could construct a function that simulates one hypothetical set of \\(k\\) people. Assume no leap years and uniformly distributed birthdays.\n\nbirthday <- function(k){\n  # For each person, sample a birthday\n  bdays <- map_vec(1:k, function(x) sample.int(365, 1))\n  # If any birthdays are duplicated, the length of unique(x) will be shorter than the length of x\n  return(as.numeric(length(bdays) != length(unique(bdays))))\n}\n\nset.seed(60639)\n# For a room of 23 people\nmean(map_vec(1:1e4, function(x) birthday(23)))\n\n[1] 0.5156\n\n# For a room of 90 people\nmean(map_vec(1:1e4, function(x) birthday(60)))\n\n[1] 0.9953\n\n\nNotably, it is still important that the expectation of the random variable exists. Consider the Cauchy distribution which has no finite mean. Suppose I take a large number of independent draws from this distribution and average them. I get wildly different results across different runs!\n\nset.seed(60639)\nmean(rcauchy(1e5)) # Average of 100,000 draws from a Cauchy(0, 1)\n\n[1] 7.696414\n\nmean(rcauchy(1e5)) # Another average of 100,000 draws from a Cauchy(0, 1) -- wildly different resutl!\n\n[1] 0.8749143"
  },
  {
    "objectID": "16_simulation.html#exercise",
    "href": "16_simulation.html#exercise",
    "title": "13  Simulation1",
    "section": "Exercise",
    "text": "Exercise\n\nBaccarat\nMonte Carlo simulations are frequently used to analyze expected outcomes in casino games and calculate the “house edge” or the percent of a player’s winnings that they can expect to lose on a particular bet. One casino game that is known to have a very low house edge is Baccarat. In this exercise, you will implement a function that simulates a game of Baccarat. Using this function, you will calculate the expected value for each of the three types of bets in the game. You will then simulate how these expected values might change if the casino set different payouts for some of the bets.\nFirst, a brief primer on baccarat. Baccarat is a table game in which two hands of playing cards – the “player” hand and the “banker” hand – are drawn and then compared against one another. The hand containing the highest score wins the round. The most common version played in modern casinos is called “punto banco” baccarat and essentially plays itself according to a fixed set of card drawing rules. The names “player” and “banker” are simply labels for each of the two hands.\nBefore each round of play, bettors can place one of three bets: that the “player” hand will win, that the “banker” hand will win, and that the hands will “tie” and have the same score.\nThen, the round begins. The game is played by drawing cards from a “shoe” of cards containing multiple decks of standard playing cards (typically six or eight). The shoe is shuffled to randomize the order. Two cards are dealt to the player hand and two cards are dealt to the banker hand We will ignore common casino practice of “burning” some cards from the top of the deck to discourage card counting as this does not impact the simulation.\nA third card may be then dealt to the player hand depending on the value of that hand. A third card may then also be dealt to the banker hand depending also on the value of the banker hand and whether the player was given a third card. The rules for determining whether either hand receives a third card are below. After all cards are dealt, the value of the hands is calculated.\nFace cards and 10s are worth zero points, aces are worth one point, and all other cards are worth their value (2-9). The value of a hand is the ones digit of the sum of the point values of the cards in that hand. So a hand consisting of a 2 and a 5 would be worth 7 points, while a hand consisting of a 4, 5 and 3 would be worth 2 points (\\(4 + 5 + 3 = 12, 12 \\text{ mod } 10 = 2\\)). In other words, hands are valued at their point sum modulo 10.\nThe hand with the highest value is declared the winner. If the hands are equally valued, the result is a “tie”\nThe rules for determining whether the player or banker hand receives a third card in a given round are somewhat complex:\nFirst, if either the player or the banker has a hand valued at 8 or 9, then no third cards are drawn, the round ends, and a winner is declared based on the value of the initial two-card hands.\nSecond, the game decides whether to give the “player hand” a third card. If the player’s initial hand value is 5 or less, then they are given a third card. If the player’s initial hand value is 6 or 7, they “stand”\nThird, the game decides whether to give the “banker hand” a third card. If the player did not receive a third card, the banker acts by the same rule as the player (draw if 5 or less, stand if 6 or 7). If the player did receive a third card, then the decision to draw depends on both the value of the banker’s current hand and the value of the third card drawn by the player.\n\nIf the banker’s hand is valued at 2 or less, they always draw a third card irrespective of what the player was dealt.\nIf the banker’s hand is valued at 3, they draw a third card unless the player’s third card is an 8.\nIf the banker’s hand is valued at 4, they draw a third card if the player’s third card is between 2 and 7 (inclusive)\nIf the banker’s hand is valued at 5, they draw a third card if the player’s third card is between 4 and 7 (inclusive)\nIf the banker’s hand is valued at 6, they draw a third card only if the player’s third card is a 6 or 7.\nIf the banker’s hand is valued at 7, they do not draw a card\n\nThe Wikipedia page for Baccarat has a nice table that summarizes the “hit/stand” decision for the banker.\nAfter the cards are dealt and the winner determined for the round, bets are paid.\n\nIf the bettor bet “player” and the “player” hand wins, they keep the amount wagered and receive an equal amount from the casino (pays 1-to-1)\nIf the bettor bet “banker” and the “banker” hand wins, they keep the amount wagered and receive 95% of their wager from the casino (pays 19-to-20).\nIf the bettor bet either “player” or “banker” and the result is a tie, they keep the amount wagered but receive no additional money from the casino (push).\nIf the bettor bet “tie” and the result is a tie, they keep the amount wagered and receive 8 times their wagered amount from the casino (pays 8-to-1)\nIf the bettor bet “player” and the “banker” wins or if they bet “banker” and the “player” wins, they lose the amount wagered.\nIf the bettor bet “tie” and either “banker” or “player” wins, they lose the amount wagered.\n\n\n\n13.6.1 Part 1 (-)\nImplement, in code, a function that simulates a single round of play (ignore the betting process for now). Assume the casino is playing with a shoe of six decks of standard playing cards and that this shoe is refreshed after every round.\nBelow is an outline for the sorts of functions you should implement to break this problem down into smaller component parts along with some hints.\n\n# This function should take as input some number of decks and generate a shoe of cards in the form of a vector\n# HINT: You don't need to store the actual cards, just their values\n# HINT 2: Read the documentation for the 'rep' function\ngen_deck <- function(){\n\n}\n\n\n# This function should take as input some vector that represents a hand of cards and return its value\n# HINT: In R, the modulo operator is %%\nvalue_hand <- function(){\n\n}\n\n\n# This function should take as input the banker's and the player's hands and return the outcome of the round\ndetermine_winner <- function(){\n\n}\n\n\n# This is your main function. It should return (at a minimum) the outcome of the round of play\n# HINT: Write out each phase of the round in plain language in the comments. Then try to implement that phase in code.\n# HINT 2: You'll likely need to use a lot of conditional statements to implement the drawing rules\nplay_round <- function(){\n \n}\n\n\n\n13.6.2 Part 2 (-)\nNow implement a function that takes as input a bettor’s chosen outcome and their wager. The function should then play a round of baccarat and return the amount that the bettor receives from the casino. There are a few equally valid ways to implement “winnings.” For the purposes of this problem, you should have the function return \\(0\\) if the player loses their entire wager.\n\n# This function should take as input a choice and a wager. You can choose to have it wrap `play_round()` or have the call to `play_round()` passed as an argument. Either way, it should return a \"payoff\" based on the choice the bettor selects and the (random) outcome of the round.\nstandard_payoff <- function(){\n \n}\n\n\n\n13.6.3 Part 3 (-)\nUsing a Monte Carlo simulation, calculate the expected return of a wager of 100 dollars on the “player” bet. In other words, if a bettor pays \\(100\\) to bet on “player”, what is the amount that they expect to win. Set your seed once to \\(60639\\) and run it for 300,000 iterations (try running for fewer iterations as you’re testing, but you’ll need a decent number of iterations to get the desired numerical precision)\nCalculate the “house edge” of the “player” bet. The house edge is the difference between a bettor’s amount wagered and their expected return, divided by the amount wagered. In other words, it’s the amount that a casino expects to keep of a player’s bet.\n\\[\\text{HouseEdge} = \\frac{\\text{AmountWagered} - \\text{ExpectedReturn}}{\\text{AmountWagered}}\\]\n\nset.seed(60639) # Set the seed\n# Simulation code here\n\n\n\n13.6.4 Part 4 (-)\nUse a Monte Carlo simulation to calculate the expected return of a wager of 100 dollars on the “banker” bet. Calculate the “house edge” of the “banker” bet. Set the seed to \\(60640\\) and run for 300,000 iterations.\n\nset.seed(60640) # Set the seed\n# Simulation code here\n\nWhich bet has the lower house edge?\n\n\n13.6.5 Part 5 (-)\nOne odd feature of Baccarat is that the “banker” bets pay 19-to-20 rather than the 1-to-1 for player bets. Using a Monte Carlo simulation, show why casinos don’t pay 1-to-1 on the banker (calculate the house edge). You’ll need to write a new payoff function to handle the alternative payout structure. Set the seed to \\(60641\\) and run for 300,000 iterations.\n\nset.seed(60641) # Set the seed\n# Simulation code here\n\n\n\n13.6.6 Challenge problem (-)\nA new version of Baccarat that has become popular in some areas called “EZ-Baccarat” purports to solve the problem you identified in Part 5 while still paying “banker” bets at 1-to-1. It does so by having any round where the banker wins with a hand of 3 cards that totals 7 points be a “push” (bettors keep their wagers but don’t win any additional money) rather than a “win” for the banker hand.\nShow, using a Monte Carlo simulation, how this payout structure retains the house edge on “banker” bets. You may have to modify your simulation functions to return more information about the outcome of the round in order to implement a new payoff function.\nSee the state regulatory documents for more info about this version of the game.\n\nset.seed(60642) # Set the seed"
  }
]